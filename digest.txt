Directory structure:
└── /./
    ├── custom_collector.py
    ├── custom_app_metrics_collector.py
    ├── discord_utils.py
    ├── uv.lock
    ├── sparkmeasure_collector.py
    ├── pyproject.toml
    ├── github_utils.py
    ├── external_metrics_collector.py
    ├── README.md
    ├── base_collector.py
    ├── analyze_failure.py
    ├── httpendpoints_collector.py
    ├── collectors.md
    ├── test_collectors.py
    ├── collectors_api.py
    └── os_metrics_collector.py

================================================
File: /custom_collector.py
================================================
# spark_debugger/collectors/custom_logging.py
import os
import re
import json
import time
import logging
from typing import Dict, Any, List, Optional, Union, TextIO
from datetime import datetime, timedelta

from base_collector import BaseCollector

class LogPattern:
    """Class representing a log pattern to match and extract information."""
    
    def __init__(self, name: str, pattern: str, is_error: bool = False, log_level: str = None):
        """Initialize a log pattern.
        
        Args:
            name: Name of the pattern (used for identification)
            pattern: Regular expression pattern to match
            is_error: Whether this pattern indicates an error
            log_level: Log level for this pattern (INFO, WARN, ERROR, etc.)
        """
        self.name = name
        self.pattern = re.compile(pattern)
        self.is_error = is_error
        self.log_level = log_level
    
    def match(self, line: str) -> Optional[Dict[str, str]]:
        """Match a log line against this pattern.
        
        Args:
            line: Log line to match
            
        Returns:
            Dictionary of captured groups or None if no match
        """
        match = self.pattern.search(line)
        if match:
            return {
                "pattern": self.name,
                "is_error": self.is_error,
                "log_level": self.log_level,
                "groups": match.groupdict()
            }
        return None

class CustomLoggingCollector(BaseCollector):
    """Collects and analyzes Spark logs to identify issues.
    
    This collector parses driver and executor logs for patterns indicating
    problems and extracts structured information from them.
    """
    
    def __init__(self, log_dir: Optional[str] = None):
        """Initialize the custom logging collector.
        
        Args:
            log_dir: Directory containing Spark logs. If None, will try common locations.
        """
        super().__init__(name="custom_logging")
        self.log_dir = log_dir
        self.log_patterns = self._build_patterns()
        self.last_scan_time = 0
        self.last_seen_position = {}  # Tracks position in each log file
        self.current_job_id = None
        
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        return [
            # Exception metrics
            "exceptions.count", "exceptions.types", "exceptions.trend",
            "exceptions.latest", "exceptions.patterns",
            
            # Data quality metrics
            "data_quality.nulls", "data_quality.schema_mismatch",
            "data_quality.type_conversions", "data_quality.constraints",
            
            # General log metrics
            "logs.error_count", "logs.warning_count", "logs.shuffle_failures",
            "logs.serialization_errors", "logs.gc_overhead",
            
            # Streaming specific metrics
            "streaming.dropped_batches", "streaming.processing_time",
            "streaming.trigger_delay", "streaming.batch_size"
        ]
    
    def setup(self, context: Dict[str, Any]) -> None:
        """Set up the collector with context information.
        
        Args:
            context: Dictionary that may contain:
                    - log_dir: Override for log directory
                    - app_id: Application ID to focus on
                    - job_id: Job ID to focus on
                    - spark_session: Active Spark session
        """
        # Override log_dir if provided
        if "log_dir" in context:
            self.log_dir = context["log_dir"]
            
        # Try to find log directory if not specified
        if not self.log_dir:
            self.log_dir = self._discover_log_dir(context)
            
        # Set job ID if provided
        if "job_id" in context:
            self.current_job_id = context["job_id"]
            
        self.logger.info(f"Custom logging collector initialized with log directory: {self.log_dir}")
        
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect and analyze Spark logs.
        
        Args:
            context: Dictionary containing collection context
                    May include log_dir to override the default
                    
        Returns:
            Dictionary of collected metrics
        """
        # Update setup if context contains relevant info
        if any(key in context for key in ["log_dir", "app_id", "job_id", "spark_session"]):
            self.setup(context)
            
        if not self.log_dir or not os.path.exists(self.log_dir):
            self.logger.warning(f"Log directory not found: {self.log_dir}")
            return {
                "timestamp": time.time(),
                "status": "error",
                "message": f"Log directory not found: {self.log_dir}"
            }
            
        metrics = {
            "timestamp": time.time(),
            "log_dir": self.log_dir,
            "scan_duration": 0,  # Will be updated at the end
            "exceptions": {
                "count": 0,
                "types": {},
                "latest": []
            },
            "data_quality": {
                "issues": []
            },
            "logs": {
                "error_count": 0,
                "warning_count": 0,
                "info_count": 0,
                "shuffle_failures": 0,
                "serialization_errors": 0,
                "gc_overhead": 0
            },
            "streaming": {
                "metrics": {}
            },
            "job_specific": {}
        }
        
        # Get current time before starting scan
        start_time = time.time()
        
        # Get all log files
        log_files = self._get_log_files()
        
        # Scan all log files
        for log_file in log_files:
            file_metrics = self._scan_log_file(log_file, only_new=True)
            
            # Merge file metrics into overall metrics
            metrics["exceptions"]["count"] += file_metrics["exceptions"]["count"]
            
            # Merge exception types
            for exc_type, count in file_metrics["exceptions"]["types"].items():
                if exc_type in metrics["exceptions"]["types"]:
                    metrics["exceptions"]["types"][exc_type] += count
                else:
                    metrics["exceptions"]["types"][exc_type] = count
            
            # Add latest exceptions
            metrics["exceptions"]["latest"].extend(file_metrics["exceptions"]["latest"])
            
            # Merge log counts
            metrics["logs"]["error_count"] += file_metrics["logs"]["error_count"]
            metrics["logs"]["warning_count"] += file_metrics["logs"]["warning_count"]
            metrics["logs"]["info_count"] += file_metrics["logs"]["info_count"]
            metrics["logs"]["shuffle_failures"] += file_metrics["logs"]["shuffle_failures"]
            metrics["logs"]["serialization_errors"] += file_metrics["logs"]["serialization_errors"]
            metrics["logs"]["gc_overhead"] += file_metrics["logs"]["gc_overhead"]
            
            # Merge data quality issues
            metrics["data_quality"]["issues"].extend(file_metrics["data_quality"]["issues"])
            
            # Merge streaming metrics if available
            if file_metrics["streaming"]["metrics"]:
                metrics["streaming"]["metrics"].update(file_metrics["streaming"]["metrics"])
        
        # Sort latest exceptions by timestamp (newest first)
        metrics["exceptions"]["latest"] = sorted(
            metrics["exceptions"]["latest"],
            key=lambda e: e.get("timestamp", 0),
            reverse=True
        )
        
        # Limit to top 10 exceptions
        metrics["exceptions"]["latest"] = metrics["exceptions"]["latest"][:10]
        
        # Calculate scan duration
        metrics["scan_duration"] = time.time() - start_time
        
        # Add job-specific information if available
        if self.current_job_id:
            metrics["job_specific"] = self._extract_job_specific_logs(self.current_job_id)
            
        # Update last scan time
        self.last_scan_time = start_time
        
        return metrics
    
    def _get_log_files(self) -> List[str]:
        """Get all log files in the log directory.
        
        Returns:
            List of log file paths
        """
        log_files = []
        
        if not self.log_dir or not os.path.exists(self.log_dir):
            return log_files
            
        # Walk through the directory to find all log files
        for root, _, files in os.walk(self.log_dir):
            for file in files:
                if file.endswith(".log") or "stderr" in file or "stdout" in file:
                    log_files.append(os.path.join(root, file))
        
        return log_files
    
    def _scan_log_file(self, log_file: str, only_new: bool = True) -> Dict[str, Any]:
        """Scan a log file for patterns.
        
        Args:
            log_file: Path to the log file
            only_new: If True, only scan new content since last scan
            
        Returns:
            Dictionary of metrics extracted from the log file
        """
        metrics = {
            "file": log_file,
            "exceptions": {
                "count": 0,
                "types": {},
                "latest": []
            },
            "data_quality": {
                "issues": []
            },
            "logs": {
                "error_count": 0,
                "warning_count": 0,
                "info_count": 0,
                "shuffle_failures": 0,
                "serialization_errors": 0,
                "gc_overhead": 0
            },
            "streaming": {
                "metrics": {}
            }
        }
        
        # Check if the file exists
        if not os.path.exists(log_file):
            self.logger.warning(f"Log file not found: {log_file}")
            return metrics
            
        # Determine where to start scanning
        start_position = 0
        if only_new and log_file in self.last_seen_position:
            start_position = self.last_seen_position[log_file]
            
        try:
            with open(log_file, "r", encoding="utf-8", errors="replace") as f:
                # Skip to the last position if needed
                if start_position > 0:
                    f.seek(start_position)
                
                # Process the file
                metrics = self._process_log_stream(f, metrics)
                
                # Update the last seen position
                self.last_seen_position[log_file] = f.tell()
                
        except Exception as e:
            self.logger.error(f"Error scanning log file {log_file}: {str(e)}")
            
        return metrics
    
    def _process_log_stream(self, log_stream: TextIO, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Process a log stream (file or buffer) for patterns.
        
        Args:
            log_stream: Text stream to process
            metrics: Metrics dictionary to update
            
        Returns:
            Updated metrics dictionary
        """
        # Variables to track multi-line exceptions
        in_exception = False
        current_exception = {
            "type": "Unknown",
            "message": "",
            "timestamp": 0,
            "stack_trace": []
        }
        
        # Process each line
        for line in log_stream:
            # Check for log levels
            if "ERROR" in line:
                metrics["logs"]["error_count"] += 1
            elif "WARN" in line:
                metrics["logs"]["warning_count"] += 1
            elif "INFO" in line:
                metrics["logs"]["info_count"] += 1
                
            # Try to parse timestamp from the line
            timestamp = self._extract_timestamp(line)
            
            # Check if this is the start of an exception
            exception_match = re.search(r"Exception|Error|Throwable|Failed", line)
            if exception_match and not in_exception:
                in_exception = True
                current_exception = {
                    "type": "Unknown",
                    "message": line.strip(),
                    "timestamp": timestamp or time.time(),
                    "stack_trace": []
                }
                
                # Try to extract the exception type
                exc_type_match = re.search(r"([\w\.]+Exception|[\w\.]+Error|[\w\.]+Throwable)", line)
                if exc_type_match:
                    current_exception["type"] = exc_type_match.group(1)
                    
                    # Update exception type counter
                    exc_type = current_exception["type"]
                    if exc_type in metrics["exceptions"]["types"]:
                        metrics["exceptions"]["types"][exc_type] += 1
                    else:
                        metrics["exceptions"]["types"][exc_type] = 1
            
            # If we're in an exception, collect the stack trace
            elif in_exception:
                current_exception["stack_trace"].append(line.strip())
                
                # Check if this is the end of the exception
                if not line.startswith(" ") and not line.startswith("\t") and line.strip() and not re.search(r"at [\w\.]+\(", line):
                    in_exception = False
                    
                    # Add the exception to the latest list
                    metrics["exceptions"]["count"] += 1
                    metrics["exceptions"]["latest"].append(current_exception)
            
            # Match specific patterns
            for pattern in self.log_patterns:
                match_result = pattern.match(line)
                if match_result:
                    # Handle different pattern types
                    pattern_name = match_result["pattern"]
                    
                    if pattern_name == "gc_overhead":
                        metrics["logs"]["gc_overhead"] += 1
                    elif pattern_name == "shuffle_failure":
                        metrics["logs"]["shuffle_failures"] += 1
                    elif pattern_name == "serialization_error":
                        metrics["logs"]["serialization_errors"] += 1
                    elif pattern_name.startswith("data_quality"):
                        # Add data quality issue
                        metrics["data_quality"]["issues"].append({
                            "type": pattern_name.replace("data_quality_", ""),
                            "message": line.strip(),
                            "timestamp": timestamp or time.time(),
                            "details": match_result["groups"]
                        })
                    elif pattern_name.startswith("streaming"):
                        # Extract streaming metrics
                        stream_metric = pattern_name.replace("streaming_", "")
                        
                        if stream_metric not in metrics["streaming"]["metrics"]:
                            metrics["streaming"]["metrics"][stream_metric] = []
                            
                        metrics["streaming"]["metrics"][stream_metric].append({
                            "timestamp": timestamp or time.time(),
                            "value": match_result["groups"].get("value"),
                            "details": match_result["groups"]
                        })
        
        # If we were in an exception when the file ended, add it
        if in_exception:
            metrics["exceptions"]["count"] += 1
            metrics["exceptions"]["latest"].append(current_exception)
            
        return metrics
    
    def _extract_job_specific_logs(self, job_id: str) -> Dict[str, Any]:
        """Extract logs specific to a job ID.
        
        Args:
            job_id: Job ID to focus on
            
        Returns:
            Dictionary of job-specific metrics
        """
        job_metrics = {
            "logs": [],
            "exceptions": [],
            "stages": {}
        }
        
        # Get all log files
        log_files = self._get_log_files()
        
        # Regular expression to match job IDs
        job_id_pattern = re.compile(r"job[_\s]+(?P<job_id>\d+)")
        stage_id_pattern = re.compile(r"stage[_\s]+(?P<stage_id>\d+)")
        
        # Scan all log files
        for log_file in log_files:
            try:
                with open(log_file, "r", encoding="utf-8", errors="replace") as f:
                    for line in f:
                        # Check if this line mentions the job ID
                        job_match = job_id_pattern.search(line)
                        if job_match and job_match.group("job_id") == job_id:
                            # Add to job-specific logs
                            timestamp = self._extract_timestamp(line)
                            job_metrics["logs"].append({
                                "timestamp": timestamp or 0,
                                "message": line.strip(),
                                "file": log_file
                            })
                            
                            # Check if this line mentions a stage
                            stage_match = stage_id_pattern.search(line)
                            if stage_match:
                                stage_id = stage_match.group("stage_id")
                                
                                if stage_id not in job_metrics["stages"]:
                                    job_metrics["stages"][stage_id] = {
                                        "logs": [],
                                        "exceptions": []
                                    }
                                
                                job_metrics["stages"][stage_id]["logs"].append({
                                    "timestamp": timestamp or 0,
                                    "message": line.strip(),
                                    "file": log_file
                                })
                                
                            # Check if this line mentions an exception
                            if "Exception" in line or "Error" in line:
                                job_metrics["exceptions"].append({
                                    "timestamp": timestamp or 0,
                                    "message": line.strip(),
                                    "file": log_file
                                })
                                
                                # If it also mentions a stage, add to stage exceptions
                                if stage_match:
                                    stage_id = stage_match.group("stage_id")
                                    job_metrics["stages"][stage_id]["exceptions"].append({
                                        "timestamp": timestamp or 0,
                                        "message": line.strip(),
                                        "file": log_file
                                    })
                        
            except Exception as e:
                self.logger.error(f"Error extracting job logs from {log_file}: {str(e)}")
        
        # Sort logs and exceptions by timestamp
        job_metrics["logs"] = sorted(job_metrics["logs"], key=lambda x: x["timestamp"])
        job_metrics["exceptions"] = sorted(job_metrics["exceptions"], key=lambda x: x["timestamp"])
        
        # Sort stage logs and exceptions by timestamp
        for stage_id in job_metrics["stages"]:
            job_metrics["stages"][stage_id]["logs"] = sorted(
                job_metrics["stages"][stage_id]["logs"],
                key=lambda x: x["timestamp"]
            )
            job_metrics["stages"][stage_id]["exceptions"] = sorted(
                job_metrics["stages"][stage_id]["exceptions"],
                key=lambda x: x["timestamp"]
            )
        
        return job_metrics
    
    def _extract_timestamp(self, line: str) -> Optional[float]:
        """Extract timestamp from a log line.
        
        Args:
            line: Log line to extract timestamp from
            
        Returns:
            Timestamp as a float or None if not found
        """
        # Try different timestamp patterns
        timestamp_patterns = [
            # ISO format: 2023-05-09T14:32:05.123Z
            r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?Z?)",
            # Common log format: [2023-05-09 14:32:05,123]
            r"\[(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}(,\d+)?)\]",
            # Simple format: 2023/05/09 14:32:05
            r"(\d{4}/\d{2}/\d{2}\s\d{2}:\d{2}:\d{2})"
        ]
        
        for pattern in timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                try:
                    # Try to parse the timestamp
                    timestamp_str = match.group(1)
                    
                    # Handle different formats
                    if "T" in timestamp_str:
                        # ISO format
                        if "Z" not in timestamp_str:
                            timestamp_str += "Z"
                        dt = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
                    elif "[" in line:
                        # Common log format
                        timestamp_str = timestamp_str.replace(",", ".")
                        dt = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S.%f" if "." in timestamp_str else "%Y-%m-%d %H:%M:%S")
                    else:
                        # Simple format
                        dt = datetime.strptime(timestamp_str, "%Y/%m/%d %H:%M:%S")
                        
                    return dt.timestamp()
                except (ValueError, TypeError):
                    # If parsing fails, try next pattern
                    continue
        
        return None
    
    def _discover_log_dir(self, context: Dict[str, Any]) -> Optional[str]:
        """Try to discover the Spark log directory.
        
        Args:
            context: Context information that might help locate the logs
            
        Returns:
            Log directory path or None if not found
        """
        # Try to get log directory from Spark session
        if "spark_session" in context:
            spark = context["spark_session"]
            
            try:
                # Get Spark configuration
                log_dir = spark.conf.get("spark.eventLog.dir")
                if log_dir and os.path.exists(log_dir):
                    return log_dir
            except:
                pass
        
        # Try common Spark log locations
        common_locations = [
            os.path.join(os.getcwd(), "spark-logs"),
            os.path.join(os.getcwd(), "logs"),
            "/tmp/spark-events",
            "/var/log/spark",
            os.path.expanduser("~/spark/logs"),
            os.path.expanduser("~/.spark/logs")
        ]
        
        # Check for SPARK_LOG_DIR environment variable
        if "SPARK_LOG_DIR" in os.environ:
            common_locations.insert(0, os.environ["SPARK_LOG_DIR"])
        
        # Check for SPARK_HOME environment variable
        if "SPARK_HOME" in os.environ:
            spark_home_logs = os.path.join(os.environ["SPARK_HOME"], "logs")
            common_locations.insert(0, spark_home_logs)
        
        # Try each location
        for location in common_locations:
            if os.path.exists(location) and os.path.isdir(location):
                return location
        
        return None
    
    def _build_patterns(self) -> List[LogPattern]:
        """Build a list of log patterns to match.
        
        Returns:
            List of LogPattern objects
        """
        patterns = []
        
        # Exception patterns
        patterns.append(LogPattern(
            "java_exception",
            r"Exception in thread .*?: ([\w\.]+Exception):(.*)",
            is_error=True,
            log_level="ERROR"
        ))
        
        patterns.append(LogPattern(
            "python_exception",
            r"Traceback \(most recent call last\):(.*?)([\w\.]+Error|[\w\.]+Exception):(.*)",
            is_error=True,
            log_level="ERROR"
        ))
        
        # GC overhead patterns
        patterns.append(LogPattern(
            "gc_overhead",
            r"(java\.lang\.OutOfMemoryError: GC overhead limit exceeded|Executor heartbeat timed out after \d+ ms)",
            is_error=True,
            log_level="ERROR"
        ))
        
        # Shuffle failure patterns
        patterns.append(LogPattern(
            "shuffle_failure",
            r"(Failed to fetch shuffle block|FetchFailedException|shuffle data of [^ ]+ not available|ShuffleMapStage \d+ failed|shuffle file not found)",
            is_error=True,
            log_level="ERROR"
        ))
        
        # Serialization error patterns
        patterns.append(LogPattern(
            "serialization_error",
            r"(Task not serializable|NotSerializableException|org\.apache\.spark\.SparkException: Task not serializable)",
            is_error=True,
            log_level="ERROR"
        ))
        
        # Data skew patterns
        patterns.append(LogPattern(
            "data_skew",
            r"Stage (?P<stage_id>\d+) contains a task of very large size \((?P<size>[\d\.]+) (?P<unit>\w+)\)",
            is_error=False,
            log_level="WARN"
        ))
        
        # Data quality patterns
        patterns.append(LogPattern(
            "data_quality_nulls",
            r"Nulls found in column '(?P<column>[^']+)'(?: \(count: (?P<count>\d+)\))?",
            is_error=False,
            log_level="WARN"
        ))
        
        patterns.append(LogPattern(
            "data_quality_schema_mismatch",
            r"Schema mismatch detected: expected '(?P<expected>[^']+)' but got '(?P<actual>[^']+)'",
            is_error=True,
            log_level="ERROR"
        ))
        
        patterns.append(LogPattern(
            "data_quality_type_conversion",
            r"Cannot convert value '(?P<value>[^']+)' to type (?P<type>\w+)",
            is_error=True,
            log_level="ERROR"
        ))
        
        # Streaming patterns
        patterns.append(LogPattern(
            "streaming_processing_time",
            r"Streaming query made progress:.*?batchDuration=(?P<value>[\d\.]+)(?P<unit>ms|s)",
            is_error=False,
            log_level="INFO"
        ))
        
        patterns.append(LogPattern(
            "streaming_input_rate",
            r"Streaming query made progress:.*?inputRowsPerSecond=(?P<value>[\d\.]+)",
            is_error=False,
            log_level="INFO"
        ))
        
        patterns.append(LogPattern(
            "streaming_processing_rate",
            r"Streaming query made progress:.*?processedRowsPerSecond=(?P<value>[\d\.]+)",
            is_error=False,
            log_level="INFO"
        ))
        
        patterns.append(LogPattern(
            "streaming_trigger_delay",
            r"Streaming query made progress:.*?triggerExecution=(?P<value>[\d\.]+)(?P<unit>ms|s)",
            is_error=False,
            log_level="INFO"
        ))
        
        patterns.append(LogPattern(
            "streaming_batch_size",
            r"Streaming query made progress:.*?numInputRows=(?P<value>\d+)",
            is_error=False,
            log_level="INFO"
        ))
        
        patterns.append(LogPattern(
            "streaming_watermark",
            r"Streaming query made progress:.*?watermark=(?P<value>[\d\-\s:\.]+)",
            is_error=False,
            log_level="INFO"
        ))
        
        patterns.append(LogPattern(
            "streaming_data_loss",
            r"(some data may have been lost|cannot find offset|no available offset|offsetOutOfRange)",
            is_error=True,
            log_level="ERROR"
        ))
        
        return patterns


================================================
File: /custom_app_metrics_collector.py
================================================
#!/usr/bin/env python3
# custom_app_metrics_collector.py - Collector for application-specific metrics

import time
import logging
import json
from typing import Dict, List, Any, Optional, Union, Callable
import importlib.util
import inspect
import os

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, count, avg, min as min_, max as max_, sum as sum_

from base_collector import BaseCollector

class MetricFunction:
    """Class representing a custom metric function."""
    
    def __init__(self, name: str, func: Callable, description: str = "", 
                 required_args: List[str] = None, optional_args: Dict[str, Any] = None):
        """Initialize a metric function.
        
        Args:
            name: Name of the metric function
            func: The function to call for calculating the metric
            description: Description of what the metric calculates
            required_args: List of required argument names
            optional_args: Dictionary of optional arguments with default values
        """
        self.name = name
        self.func = func
        self.description = description
        self.required_args = required_args or []
        self.optional_args = optional_args or {}

class CustomAppMetricsCollector(BaseCollector):
    """Collector for application-specific and domain-specific metrics.
    
    This collector enables custom metrics definitions targeting the specific
    application domain and business logic. It can be extended with user-defined
    metric functions and data quality rules.
    """
    
    def __init__(self, custom_metrics_path: Optional[str] = None):
        """Initialize the custom application metrics collector.
        
        Args:
            custom_metrics_path: Path to a Python file with custom metric definitions
        """
        super().__init__(name="custom_app_metrics")
        self.spark = None
        self.custom_metrics_path = custom_metrics_path
        self.metric_functions = {}
        self.quality_rules = {}
        self.last_metrics = {}
        
        # Register built-in metrics
        self._register_builtin_metrics()
        
        # Load custom metrics if provided
        if custom_metrics_path:
            self._load_custom_metrics(custom_metrics_path)
    
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        return [
            # Built-in data quality metrics
            "app.data_quality.null_percentage", 
            "app.data_quality.duplicate_percentage",
            "app.data_quality.invalid_values",
            "app.data_quality.domain_errors",
            
            # Business metrics (examples, actual metrics depend on custom definitions)
            "app.business.success_rate",
            "app.business.latency",
            "app.business.throughput",
            "app.business.error_rate",
            
            # Custom metrics (loaded from external files)
            *[f"app.custom.{name}" for name in self.metric_functions.keys()]
        ]
    
    def setup(self, context: Dict[str, Any]) -> None:
        """Set up the collector with context information.
        
        Args:
            context: Dictionary containing at least "spark_session"
                     May also include custom_metrics_path to override the default
        """
        if "spark_session" not in context:
            raise ValueError("spark_session is required for custom app metrics")
            
        self.spark = context["spark_session"]
        
        # Override custom metrics path if provided
        if "custom_metrics_path" in context:
            self.custom_metrics_path = context["custom_metrics_path"]
            # Reload custom metrics
            self._load_custom_metrics(self.custom_metrics_path)
            
        self.logger.info(f"Custom app metrics collector initialized with {len(self.metric_functions)} metric functions")
    
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect application-specific metrics.
        
        Args:
            context: Dictionary containing:
                     - spark_session: Spark session (if not already set)
                     - table_name: Name of the table to analyze
                     - dataset: DataFrame to analyze
                     - schema: Optionally provide schema information
                     - custom_args: Additional arguments for custom metrics
                     
        Returns:
            Dictionary of collected metrics
        """
        # Update spark session if provided
        if "spark_session" in context and not self.spark:
            self.spark = context["spark_session"]
            
        # Get the dataset to analyze
        dataset = None
        table_name = None
        
        if "dataset" in context:
            dataset = context["dataset"]
        elif "table_name" in context and self.spark:
            table_name = context["table_name"]
            try:
                dataset = self.spark.table(table_name)
            except Exception as e:
                self.logger.error(f"Error loading table {table_name}: {str(e)}")
                
        if dataset is None:
            self.logger.error("No dataset or table_name provided")
            return {
                "timestamp": time.time(),
                "status": "error",
                "message": "No dataset or table_name provided"
            }
            
        # Get schema info if provided
        schema_info = context.get("schema", None)
        if schema_info is None and dataset is not None:
            schema_info = self._extract_schema_info(dataset)
            
        # Get custom arguments
        custom_args = context.get("custom_args", {})
        
        # Prepare metrics result
        metrics = {
            "timestamp": time.time(),
            "source": table_name or "dataset",
            "data_quality": {},
            "business": {},
            "custom": {}
        }
        
        # Calculate data quality metrics
        metrics["data_quality"] = self._calculate_data_quality_metrics(dataset, schema_info)
        
        # Calculate domain-specific metrics
        metrics["business"] = self._calculate_business_metrics(dataset, custom_args)
        
        # Calculate custom metrics
        metrics["custom"] = self._calculate_custom_metrics(dataset, custom_args)
        
        # Update last metrics
        self.last_metrics = metrics
        
        return metrics
    
    def _register_builtin_metrics(self) -> None:
        """Register built-in metric functions."""
        # Register data quality metrics
        self.metric_functions["null_percentage"] = MetricFunction(
            name="null_percentage",
            func=self._calculate_null_percentage,
            description="Calculate percentage of null values in specified columns",
            required_args=["dataset"],
            optional_args={"columns": None}
        )
        
        self.metric_functions["duplicate_percentage"] = MetricFunction(
            name="duplicate_percentage",
            func=self._calculate_duplicate_percentage,
            description="Calculate percentage of duplicate rows based on specified columns",
            required_args=["dataset"],
            optional_args={"columns": None}
        )
        
        self.metric_functions["value_distribution"] = MetricFunction(
            name="value_distribution",
            func=self._calculate_value_distribution,
            description="Calculate value distribution for a column",
            required_args=["dataset", "column"],
            optional_args={"max_categories": 20}
        )
        
        self.metric_functions["numeric_stats"] = MetricFunction(
            name="numeric_stats",
            func=self._calculate_numeric_stats,
            description="Calculate statistics for numeric columns",
            required_args=["dataset"],
            optional_args={"columns": None}
        )
    
    def _load_custom_metrics(self, metrics_path: str) -> None:
        """Load custom metric functions from a Python file.
        
        Args:
            metrics_path: Path to a Python file with custom metric definitions
        """
        if not os.path.exists(metrics_path):
            self.logger.error(f"Custom metrics file not found: {metrics_path}")
            return
            
        try:
            # Load the module
            spec = importlib.util.spec_from_file_location("custom_metrics", metrics_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            
            # Find metric functions
            for name, obj in inspect.getmembers(module):
                # Look for functions decorated with @metric or @quality_rule
                if hasattr(obj, "_is_metric") and callable(obj):
                    metric_name = getattr(obj, "_metric_name", name)
                    description = getattr(obj, "_description", "")
                    required_args = getattr(obj, "_required_args", ["dataset"])
                    optional_args = getattr(obj, "_optional_args", {})
                    
                    self.metric_functions[metric_name] = MetricFunction(
                        name=metric_name,
                        func=obj,
                        description=description,
                        required_args=required_args,
                        optional_args=optional_args
                    )
                    
                    self.logger.info(f"Registered custom metric: {metric_name}")
                
                # Look for quality rules
                elif hasattr(obj, "_is_quality_rule") and callable(obj):
                    rule_name = getattr(obj, "_rule_name", name)
                    description = getattr(obj, "_description", "")
                    required_args = getattr(obj, "_required_args", ["dataset"])
                    optional_args = getattr(obj, "_optional_args", {})
                    
                    self.quality_rules[rule_name] = MetricFunction(
                        name=rule_name,
                        func=obj,
                        description=description,
                        required_args=required_args,
                        optional_args=optional_args
                    )
                    
                    self.logger.info(f"Registered quality rule: {rule_name}")
                    
        except Exception as e:
            self.logger.error(f"Error loading custom metrics: {str(e)}")
    
    def _extract_schema_info(self, dataset: DataFrame) -> Dict[str, Any]:
        """Extract schema information from a dataset.
        
        Args:
            dataset: DataFrame to extract schema from
            
        Returns:
            Dictionary with schema information
        """
        schema = dataset.schema
        schema_info = {
            "fields": [
                {
                    "name": field.name,
                    "type": str(field.dataType),
                    "nullable": field.nullable,
                    "metadata": dict(field.metadata) if field.metadata else {}
                }
                for field in schema.fields
            ],
            "column_names": [field.name for field in schema.fields],
            "numeric_columns": [
                field.name for field in schema.fields
                if str(field.dataType).startswith(("IntegerType", "LongType", "DoubleType", "FloatType", "DecimalType"))
            ],
            "string_columns": [
                field.name for field in schema.fields
                if str(field.dataType).startswith("StringType")
            ],
            "timestamp_columns": [
                field.name for field in schema.fields
                if str(field.dataType).startswith(("TimestampType", "DateType"))
            ]
        }
        
        return schema_info
    
    def _calculate_data_quality_metrics(self, dataset: DataFrame, schema_info: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate data quality metrics.
        
        Args:
            dataset: DataFrame to analyze
            schema_info: Schema information
            
        Returns:
            Dictionary with data quality metrics
        """
        metrics = {}
        
        try:
            # Get all column names
            columns = schema_info["column_names"]
            
            # Calculate null percentage for each column
            null_metrics = self._calculate_null_percentage(dataset, columns)
            metrics["null_percentage"] = null_metrics
            
            # Calculate overall null rate
            if null_metrics and "column_metrics" in null_metrics:
                total_nulls = sum(col_metric["null_count"] for col_metric in null_metrics["column_metrics"].values())
                total_cells = dataset.count() * len(columns)
                metrics["overall_null_rate"] = total_nulls / total_cells if total_cells > 0 else 0
            
            # Calculate duplicate percentage
            dup_metrics = self._calculate_duplicate_percentage(dataset)
            metrics["duplicate_percentage"] = dup_metrics["duplicate_percentage"]
            metrics["duplicate_count"] = dup_metrics["duplicate_count"]
            
            # Calculate numeric statistics for numeric columns
            if "numeric_columns" in schema_info and schema_info["numeric_columns"]:
                metrics["numeric_stats"] = self._calculate_numeric_stats(
                    dataset, schema_info["numeric_columns"]
                )
            
            # Run quality rules
            metrics["quality_rules"] = self._run_quality_rules(dataset)
            
        except Exception as e:
            self.logger.error(f"Error calculating data quality metrics: {str(e)}")
            metrics["error"] = str(e)
            
        return metrics
    
    def _calculate_business_metrics(self, dataset: DataFrame, custom_args: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate business-specific metrics.
        
        Args:
            dataset: DataFrame to analyze
            custom_args: Additional arguments for metrics
            
        Returns:
            Dictionary with business metrics
        """
        metrics = {}
        
        # This would typically be customized for the specific application
        # Here we provide a generic example
        
        try:
            # Example: Calculate success rate if status column exists
            if "status" in dataset.columns:
                success_count = dataset.filter(col("status") == "success").count()
                total_count = dataset.count()
                metrics["success_rate"] = success_count / total_count if total_count > 0 else 0
            
            # Example: Calculate average processing time if timestamp columns exist
            timestamp_cols = [c for c in dataset.columns if "time" in c.lower()]
            if len(timestamp_cols) >= 2:
                # Assuming start_time and end_time columns
                time_cols = [c for c in timestamp_cols if "start" in c.lower() or "end" in c.lower()]
                if len(time_cols) >= 2:
                    start_col = next((c for c in time_cols if "start" in c.lower()), None)
                    end_col = next((c for c in time_cols if "end" in c.lower()), None)
                    
                    if start_col and end_col:
                        # Calculate time difference
                        metrics["avg_processing_time"] = dataset.selectExpr(
                            f"avg(unix_timestamp({end_col}) - unix_timestamp({start_col})) as time_diff"
                        ).collect()[0][0]
            
            # Example: Calculate throughput if timestamp column exists
            if timestamp_cols:
                latest_col = timestamp_cols[0]  # Just use the first timestamp column
                min_time = dataset.agg(min_(col(latest_col))).collect()[0][0]
                max_time = dataset.agg(max_(col(latest_col))).collect()[0][0]
                
                if min_time and max_time:
                    # Calculate events per second
                    time_diff_seconds = (max_time.timestamp() - min_time.timestamp())
                    if time_diff_seconds > 0:
                        metrics["throughput"] = dataset.count() / time_diff_seconds
                        
        except Exception as e:
            self.logger.error(f"Error calculating business metrics: {str(e)}")
            metrics["error"] = str(e)
            
        return metrics
    
    def _calculate_custom_metrics(self, dataset: DataFrame, custom_args: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate custom metrics using registered functions.
        
        Args:
            dataset: DataFrame to analyze
            custom_args: Additional arguments for metrics
            
        Returns:
            Dictionary with custom metrics
        """
        metrics = {}
        
        # Prepare common arguments
        common_args = {"dataset": dataset}
        if self.spark:
            common_args["spark"] = self.spark
            
        # Update with custom arguments
        args = {**common_args, **custom_args}
        
        # Call each metric function
        for name, metric_func in self.metric_functions.items():
            try:
                # Check if we have all required arguments
                missing_args = [arg for arg in metric_func.required_args if arg not in args]
                if missing_args:
                    self.logger.warning(f"Skipping metric {name} due to missing arguments: {missing_args}")
                    continue
                
                # Prepare function arguments
                func_args = {
                    arg: args[arg]
                    for arg in metric_func.required_args
                    if arg in args
                }
                
                # Add optional arguments if available
                for arg, default_value in metric_func.optional_args.items():
                    func_args[arg] = args.get(arg, default_value)
                
                # Call the function
                result = metric_func.func(**func_args)
                metrics[name] = result
                
            except Exception as e:
                self.logger.error(f"Error calculating custom metric {name}: {str(e)}")
                metrics[f"{name}_error"] = str(e)
                
        return metrics
    
    def _run_quality_rules(self, dataset: DataFrame) -> Dict[str, Any]:
        """Run quality rules on the dataset.
        
        Args:
            dataset: DataFrame to analyze
            
        Returns:
            Dictionary with quality rule results
        """
        rule_results = {}
        
        for name, rule in self.quality_rules.items():
            try:
                # Call the rule function
                result = rule.func(dataset=dataset)
                rule_results[name] = result
                
            except Exception as e:
                self.logger.error(f"Error running quality rule {name}: {str(e)}")
                rule_results[f"{name}_error"] = str(e)
                
        return rule_results
    
    # Built-in metric functions
    
    def _calculate_null_percentage(self, dataset: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """Calculate percentage of null values in columns.
        
        Args:
            dataset: DataFrame to analyze
            columns: List of columns to check (if None, check all columns)
            
        Returns:
            Dictionary with null percentage metrics
        """
        if not columns:
            columns = dataset.columns
            
        # Count total rows
        total_rows = dataset.count()
        if total_rows == 0:
            return {
                "overall_null_rate": 0,
                "column_metrics": {}
            }
            
        # Create expressions for null counts
        exprs = []
        for col_name in columns:
            exprs.append(count(col(col_name).isNull().cast("int")).alias(f"{col_name}_null_count"))
            
        # Calculate null counts
        null_counts = dataset.select(*exprs).collect()[0].asDict()
        
        # Format results
        column_metrics = {}
        for col_name in columns:
            null_count = null_counts.get(f"{col_name}_null_count", 0)
            column_metrics[col_name] = {
                "null_count": null_count,
                "null_percentage": (null_count / total_rows) * 100 if total_rows > 0 else 0
            }
            
        return {
            "overall_null_rate": sum(m["null_count"] for m in column_metrics.values()) / (total_rows * len(columns)) if total_rows > 0 else 0,
            "column_metrics": column_metrics
        }
    
    def _calculate_duplicate_percentage(self, dataset: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """Calculate percentage of duplicate rows.
        
        Args:
            dataset: DataFrame to analyze
            columns: List of columns to consider for duplicates (if None, use all columns)
            
        Returns:
            Dictionary with duplicate percentage metrics
        """
        if not columns:
            columns = dataset.columns
            
        # Count total rows
        total_rows = dataset.count()
        if total_rows == 0:
            return {
                "duplicate_percentage": 0,
                "duplicate_count": 0,
                "unique_count": 0
            }
        
        # Count distinct rows
        distinct_rows = dataset.select(*columns).distinct().count()
        duplicate_rows = total_rows - distinct_rows
        
        return {
            "duplicate_percentage": (duplicate_rows / total_rows) * 100 if total_rows > 0 else 0,
            "duplicate_count": duplicate_rows,
            "unique_count": distinct_rows
        }
    
    def _calculate_value_distribution(self, dataset: DataFrame, column: str, max_categories: int = 20) -> Dict[str, Any]:
        """Calculate value distribution for a column.
        
        Args:
            dataset: DataFrame to analyze
            column: Column to analyze
            max_categories: Maximum number of categories to return
            
        Returns:
            Dictionary with value distribution metrics
        """
        if column not in dataset.columns:
            return {
                "error": f"Column {column} not found in dataset"
            }
            
        # Count total rows
        total_rows = dataset.count()
        if total_rows == 0:
            return {
                "total": 0,
                "distribution": {}
            }
            
        # Calculate value counts
        value_counts = dataset.groupBy(column).count().orderBy(col("count").desc()).limit(max_categories)
        distribution = {
            str(row[column]): {
                "count": row["count"],
                "percentage": (row["count"] / total_rows) * 100 if total_rows > 0 else 0
            }
            for row in value_counts.collect()
        }
        
        return {
            "total": total_rows,
            "distribution": distribution
        }
    
    def _calculate_numeric_stats(self, dataset: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """Calculate statistics for numeric columns.
        
        Args:
            dataset: DataFrame to analyze
            columns: List of numeric columns to analyze (if None, use all numeric columns)
            
        Returns:
            Dictionary with numeric statistics
        """
        if not columns:
            # Try to identify numeric columns
            schema = dataset.schema
            columns = [
                field.name for field in schema.fields
                if str(field.dataType).startswith(("IntegerType", "LongType", "DoubleType", "FloatType", "DecimalType"))
            ]
            
        if not columns:
            return {
                "error": "No numeric columns found"
            }
            
        # Create expressions for statistics
        exprs = []
        for col_name in columns:
            exprs.extend([
                min_(col(col_name)).alias(f"{col_name}_min"),
                max_(col(col_name)).alias(f"{col_name}_max"),
                avg(col(col_name)).alias(f"{col_name}_avg"),
                expr(f"percentile({col_name}, 0.5)").alias(f"{col_name}_median")
            ])
            
        # Calculate statistics
        stats = dataset.select(*exprs).collect()[0].asDict()
        
        # Format results
        column_stats = {}
        for col_name in columns:
            column_stats[col_name] = {
                "min": stats.get(f"{col_name}_min"),
                "max": stats.get(f"{col_name}_max"),
                "avg": stats.get(f"{col_name}_avg"),
                "median": stats.get(f"{col_name}_median")
            }
            
        return column_stats

# Decorator for custom metrics
def metric(name: Optional[str] = None, description: str = "", 
           required_args: List[str] = None, optional_args: Dict[str, Any] = None):
    """Decorator to register a function as a custom metric.
    
    Args:
        name: Custom name for the metric (default: function name)
        description: Description of the metric
        required_args: List of required argument names
        optional_args: Dictionary of optional arguments with default values
        
    Returns:
        Decorated function
    """
    def decorator(func):
        func._is_metric = True
        func._metric_name = name or func.__name__
        func._description = description
        func._required_args = required_args or ["dataset"]
        func._optional_args = optional_args or {}
        return func
    return decorator

# Decorator for quality rules
def quality_rule(name: Optional[str] = None, description: str = ""):
    """Decorator to register a function as a quality rule.
    
    Args:
        name: Custom name for the rule (default: function name)
        description: Description of the rule
        
    Returns:
        Decorated function
    """
    def decorator(func):
        func._is_quality_rule = True
        func._rule_name = name or func.__name__
        func._description = description
        func._required_args = ["dataset"]
        func._optional_args = {}
        return func
    return decorator

# Example of custom metrics file structure:
"""
# my_custom_metrics.py
from custom_app_metrics_collector import metric, quality_rule
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, count, when

@metric(name="order_success_rate", description="Calculate order success rate")
def calculate_order_success_rate(dataset: DataFrame):
    successful_orders = dataset.filter(col("status") == "COMPLETED").count()
    total_orders = dataset.count()
    return successful_orders / total_orders if total_orders > 0 else 0

@quality_rule(name="valid_email_format", description="Check if emails are valid format")
def check_valid_email_format(dataset: DataFrame):
    if "email" not in dataset.columns:
        return {"pass": True, "message": "No email column found"}
        
    invalid_emails = dataset.filter(~col("email").rlike("^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$")).count()
    return {
        "pass": invalid_emails == 0,
        "invalid_count": invalid_emails,
        "message": f"Found {invalid_emails} invalid email formats"
    }
"""

================================================
File: /discord_utils.py
================================================


async def send_discord_message(message: str, channel_id: int):
    """
    Send a message to a specific Discord channel.
    
    Args:
        message (str): The message to send
        channel_id (int): The ID of the channel to send the message to
    """



================================================
File: /uv.lock
================================================
version = 1
requires-python = ">=3.13"

[[package]]
name = "aiohappyeyeballs"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265 },
]

[[package]]
name = "aiohttp"
version = "3.11.18"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/e7/fa1a8c00e2c54b05dc8cb5d1439f627f7c267874e3f7bb047146116020f9/aiohttp-3.11.18.tar.gz", hash = "sha256:ae856e1138612b7e412db63b7708735cff4d38d0399f6a5435d3dac2669f558a", size = 7678653 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0a/18/be8b5dd6b9cf1b2172301dbed28e8e5e878ee687c21947a6c81d6ceaa15d/aiohttp-3.11.18-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:474215ec618974054cf5dc465497ae9708543cbfc312c65212325d4212525811", size = 699833 },
    { url = "https://files.pythonhosted.org/packages/0d/84/ecdc68e293110e6f6f6d7b57786a77555a85f70edd2b180fb1fafaff361a/aiohttp-3.11.18-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6ced70adf03920d4e67c373fd692123e34d3ac81dfa1c27e45904a628567d804", size = 462774 },
    { url = "https://files.pythonhosted.org/packages/d7/85/f07718cca55884dad83cc2433746384d267ee970e91f0dcc75c6d5544079/aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:2d9f6c0152f8d71361905aaf9ed979259537981f47ad099c8b3d81e0319814bd", size = 454429 },
    { url = "https://files.pythonhosted.org/packages/82/02/7f669c3d4d39810db8842c4e572ce4fe3b3a9b82945fdd64affea4c6947e/aiohttp-3.11.18-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a35197013ed929c0aed5c9096de1fc5a9d336914d73ab3f9df14741668c0616c", size = 1670283 },
    { url = "https://files.pythonhosted.org/packages/ec/79/b82a12f67009b377b6c07a26bdd1b81dab7409fc2902d669dbfa79e5ac02/aiohttp-3.11.18-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:540b8a1f3a424f1af63e0af2d2853a759242a1769f9f1ab053996a392bd70118", size = 1717231 },
    { url = "https://files.pythonhosted.org/packages/a6/38/d5a1f28c3904a840642b9a12c286ff41fc66dfa28b87e204b1f242dbd5e6/aiohttp-3.11.18-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f9e6710ebebfce2ba21cee6d91e7452d1125100f41b906fb5af3da8c78b764c1", size = 1769621 },
    { url = "https://files.pythonhosted.org/packages/53/2d/deb3749ba293e716b5714dda06e257f123c5b8679072346b1eb28b766a0b/aiohttp-3.11.18-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f8af2ef3b4b652ff109f98087242e2ab974b2b2b496304063585e3d78de0b000", size = 1678667 },
    { url = "https://files.pythonhosted.org/packages/b8/a8/04b6e11683a54e104b984bd19a9790eb1ae5f50968b601bb202d0406f0ff/aiohttp-3.11.18-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:28c3f975e5ae3dbcbe95b7e3dcd30e51da561a0a0f2cfbcdea30fc1308d72137", size = 1601592 },
    { url = "https://files.pythonhosted.org/packages/5e/9d/c33305ae8370b789423623f0e073d09ac775cd9c831ac0f11338b81c16e0/aiohttp-3.11.18-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:c28875e316c7b4c3e745172d882d8a5c835b11018e33432d281211af35794a93", size = 1621679 },
    { url = "https://files.pythonhosted.org/packages/56/45/8e9a27fff0538173d47ba60362823358f7a5f1653c6c30c613469f94150e/aiohttp-3.11.18-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:13cd38515568ae230e1ef6919e2e33da5d0f46862943fcda74e7e915096815f3", size = 1656878 },
    { url = "https://files.pythonhosted.org/packages/84/5b/8c5378f10d7a5a46b10cb9161a3aac3eeae6dba54ec0f627fc4ddc4f2e72/aiohttp-3.11.18-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:0e2a92101efb9f4c2942252c69c63ddb26d20f46f540c239ccfa5af865197bb8", size = 1620509 },
    { url = "https://files.pythonhosted.org/packages/9e/2f/99dee7bd91c62c5ff0aa3c55f4ae7e1bc99c6affef780d7777c60c5b3735/aiohttp-3.11.18-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:e6d3e32b8753c8d45ac550b11a1090dd66d110d4ef805ffe60fa61495360b3b2", size = 1680263 },
    { url = "https://files.pythonhosted.org/packages/03/0a/378745e4ff88acb83e2d5c884a4fe993a6e9f04600a4560ce0e9b19936e3/aiohttp-3.11.18-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:ea4cf2488156e0f281f93cc2fd365025efcba3e2d217cbe3df2840f8c73db261", size = 1715014 },
    { url = "https://files.pythonhosted.org/packages/f6/0b/b5524b3bb4b01e91bc4323aad0c2fcaebdf2f1b4d2eb22743948ba364958/aiohttp-3.11.18-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9d4df95ad522c53f2b9ebc07f12ccd2cb15550941e11a5bbc5ddca2ca56316d7", size = 1666614 },
    { url = "https://files.pythonhosted.org/packages/c7/b7/3d7b036d5a4ed5a4c704e0754afe2eef24a824dfab08e6efbffb0f6dd36a/aiohttp-3.11.18-cp313-cp313-win32.whl", hash = "sha256:cdd1bbaf1e61f0d94aced116d6e95fe25942f7a5f42382195fd9501089db5d78", size = 411358 },
    { url = "https://files.pythonhosted.org/packages/1e/3c/143831b32cd23b5263a995b2a1794e10aa42f8a895aae5074c20fda36c07/aiohttp-3.11.18-cp313-cp313-win_amd64.whl", hash = "sha256:bdd619c27e44382cf642223f11cfd4d795161362a5a1fc1fa3940397bc89db01", size = 437658 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "attrs"
version = "25.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5a/b0/1367933a8532ee6ff8d63537de4f1177af4bff9f3e829baf7331f595bb24/attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b", size = 812032 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3", size = 63815 },
]

[[package]]
name = "audioop-lts"
version = "0.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/dd/3b/69ff8a885e4c1c42014c2765275c4bd91fe7bc9847e9d8543dbcbb09f820/audioop_lts-0.2.1.tar.gz", hash = "sha256:e81268da0baa880431b68b1308ab7257eb33f356e57a5f9b1f915dfb13dd1387", size = 30204 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/01/91/a219253cc6e92db2ebeaf5cf8197f71d995df6f6b16091d1f3ce62cb169d/audioop_lts-0.2.1-cp313-abi3-macosx_10_13_universal2.whl", hash = "sha256:fd1345ae99e17e6910f47ce7d52673c6a1a70820d78b67de1b7abb3af29c426a", size = 46252 },
    { url = "https://files.pythonhosted.org/packages/ec/f6/3cb21e0accd9e112d27cee3b1477cd04dafe88675c54ad8b0d56226c1e0b/audioop_lts-0.2.1-cp313-abi3-macosx_10_13_x86_64.whl", hash = "sha256:e175350da05d2087e12cea8e72a70a1a8b14a17e92ed2022952a4419689ede5e", size = 27183 },
    { url = "https://files.pythonhosted.org/packages/ea/7e/f94c8a6a8b2571694375b4cf94d3e5e0f529e8e6ba280fad4d8c70621f27/audioop_lts-0.2.1-cp313-abi3-macosx_11_0_arm64.whl", hash = "sha256:4a8dd6a81770f6ecf019c4b6d659e000dc26571b273953cef7cd1d5ce2ff3ae6", size = 26726 },
    { url = "https://files.pythonhosted.org/packages/ef/f8/a0e8e7a033b03fae2b16bc5aa48100b461c4f3a8a38af56d5ad579924a3a/audioop_lts-0.2.1-cp313-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1cd3c0b6f2ca25c7d2b1c3adeecbe23e65689839ba73331ebc7d893fcda7ffe", size = 80718 },
    { url = "https://files.pythonhosted.org/packages/8f/ea/a98ebd4ed631c93b8b8f2368862cd8084d75c77a697248c24437c36a6f7e/audioop_lts-0.2.1-cp313-abi3-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ff3f97b3372c97782e9c6d3d7fdbe83bce8f70de719605bd7ee1839cd1ab360a", size = 88326 },
    { url = "https://files.pythonhosted.org/packages/33/79/e97a9f9daac0982aa92db1199339bd393594d9a4196ad95ae088635a105f/audioop_lts-0.2.1-cp313-abi3-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a351af79edefc2a1bd2234bfd8b339935f389209943043913a919df4b0f13300", size = 80539 },
    { url = "https://files.pythonhosted.org/packages/b2/d3/1051d80e6f2d6f4773f90c07e73743a1e19fcd31af58ff4e8ef0375d3a80/audioop_lts-0.2.1-cp313-abi3-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2aeb6f96f7f6da80354330470b9134d81b4cf544cdd1c549f2f45fe964d28059", size = 78577 },
    { url = "https://files.pythonhosted.org/packages/7a/1d/54f4c58bae8dc8c64a75071c7e98e105ddaca35449376fcb0180f6e3c9df/audioop_lts-0.2.1-cp313-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c589f06407e8340e81962575fcffbba1e92671879a221186c3d4662de9fe804e", size = 82074 },
    { url = "https://files.pythonhosted.org/packages/36/89/2e78daa7cebbea57e72c0e1927413be4db675548a537cfba6a19040d52fa/audioop_lts-0.2.1-cp313-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:fbae5d6925d7c26e712f0beda5ed69ebb40e14212c185d129b8dfbfcc335eb48", size = 84210 },
    { url = "https://files.pythonhosted.org/packages/a5/57/3ff8a74df2ec2fa6d2ae06ac86e4a27d6412dbb7d0e0d41024222744c7e0/audioop_lts-0.2.1-cp313-abi3-musllinux_1_2_i686.whl", hash = "sha256:d2d5434717f33117f29b5691fbdf142d36573d751716249a288fbb96ba26a281", size = 85664 },
    { url = "https://files.pythonhosted.org/packages/16/01/21cc4e5878f6edbc8e54be4c108d7cb9cb6202313cfe98e4ece6064580dd/audioop_lts-0.2.1-cp313-abi3-musllinux_1_2_ppc64le.whl", hash = "sha256:f626a01c0a186b08f7ff61431c01c055961ee28769591efa8800beadd27a2959", size = 93255 },
    { url = "https://files.pythonhosted.org/packages/3e/28/7f7418c362a899ac3b0bf13b1fde2d4ffccfdeb6a859abd26f2d142a1d58/audioop_lts-0.2.1-cp313-abi3-musllinux_1_2_s390x.whl", hash = "sha256:05da64e73837f88ee5c6217d732d2584cf638003ac72df124740460531e95e47", size = 87760 },
    { url = "https://files.pythonhosted.org/packages/6d/d8/577a8be87dc7dd2ba568895045cee7d32e81d85a7e44a29000fe02c4d9d4/audioop_lts-0.2.1-cp313-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:56b7a0a4dba8e353436f31a932f3045d108a67b5943b30f85a5563f4d8488d77", size = 84992 },
    { url = "https://files.pythonhosted.org/packages/ef/9a/4699b0c4fcf89936d2bfb5425f55f1a8b86dff4237cfcc104946c9cd9858/audioop_lts-0.2.1-cp313-abi3-win32.whl", hash = "sha256:6e899eb8874dc2413b11926b5fb3857ec0ab55222840e38016a6ba2ea9b7d5e3", size = 26059 },
    { url = "https://files.pythonhosted.org/packages/3a/1c/1f88e9c5dd4785a547ce5fd1eb83fff832c00cc0e15c04c1119b02582d06/audioop_lts-0.2.1-cp313-abi3-win_amd64.whl", hash = "sha256:64562c5c771fb0a8b6262829b9b4f37a7b886c01b4d3ecdbae1d629717db08b4", size = 30412 },
    { url = "https://files.pythonhosted.org/packages/c4/e9/c123fd29d89a6402ad261516f848437472ccc602abb59bba522af45e281b/audioop_lts-0.2.1-cp313-abi3-win_arm64.whl", hash = "sha256:c45317debeb64002e980077642afbd977773a25fa3dfd7ed0c84dccfc1fafcb0", size = 23578 },
    { url = "https://files.pythonhosted.org/packages/7a/99/bb664a99561fd4266687e5cb8965e6ec31ba4ff7002c3fce3dc5ef2709db/audioop_lts-0.2.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:3827e3fce6fee4d69d96a3d00cd2ab07f3c0d844cb1e44e26f719b34a5b15455", size = 46827 },
    { url = "https://files.pythonhosted.org/packages/c4/e3/f664171e867e0768ab982715e744430cf323f1282eb2e11ebfb6ee4c4551/audioop_lts-0.2.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:161249db9343b3c9780ca92c0be0d1ccbfecdbccac6844f3d0d44b9c4a00a17f", size = 27479 },
    { url = "https://files.pythonhosted.org/packages/a6/0d/2a79231ff54eb20e83b47e7610462ad6a2bea4e113fae5aa91c6547e7764/audioop_lts-0.2.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:5b7b4ff9de7a44e0ad2618afdc2ac920b91f4a6d3509520ee65339d4acde5abf", size = 27056 },
    { url = "https://files.pythonhosted.org/packages/86/46/342471398283bb0634f5a6df947806a423ba74b2e29e250c7ec0e3720e4f/audioop_lts-0.2.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:72e37f416adb43b0ced93419de0122b42753ee74e87070777b53c5d2241e7fab", size = 87802 },
    { url = "https://files.pythonhosted.org/packages/56/44/7a85b08d4ed55517634ff19ddfbd0af05bf8bfd39a204e4445cd0e6f0cc9/audioop_lts-0.2.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:534ce808e6bab6adb65548723c8cbe189a3379245db89b9d555c4210b4aaa9b6", size = 95016 },
    { url = "https://files.pythonhosted.org/packages/a8/2a/45edbca97ea9ee9e6bbbdb8d25613a36e16a4d1e14ae01557392f15cc8d3/audioop_lts-0.2.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d2de9b6fb8b1cf9f03990b299a9112bfdf8b86b6987003ca9e8a6c4f56d39543", size = 87394 },
    { url = "https://files.pythonhosted.org/packages/14/ae/832bcbbef2c510629593bf46739374174606e25ac7d106b08d396b74c964/audioop_lts-0.2.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f24865991b5ed4b038add5edbf424639d1358144f4e2a3e7a84bc6ba23e35074", size = 84874 },
    { url = "https://files.pythonhosted.org/packages/26/1c/8023c3490798ed2f90dfe58ec3b26d7520a243ae9c0fc751ed3c9d8dbb69/audioop_lts-0.2.1-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2bdb3b7912ccd57ea53197943f1bbc67262dcf29802c4a6df79ec1c715d45a78", size = 88698 },
    { url = "https://files.pythonhosted.org/packages/2c/db/5379d953d4918278b1f04a5a64b2c112bd7aae8f81021009da0dcb77173c/audioop_lts-0.2.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:120678b208cca1158f0a12d667af592e067f7a50df9adc4dc8f6ad8d065a93fb", size = 90401 },
    { url = "https://files.pythonhosted.org/packages/99/6e/3c45d316705ab1aec2e69543a5b5e458d0d112a93d08994347fafef03d50/audioop_lts-0.2.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:54cd4520fc830b23c7d223693ed3e1b4d464997dd3abc7c15dce9a1f9bd76ab2", size = 91864 },
    { url = "https://files.pythonhosted.org/packages/08/58/6a371d8fed4f34debdb532c0b00942a84ebf3e7ad368e5edc26931d0e251/audioop_lts-0.2.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:d6bd20c7a10abcb0fb3d8aaa7508c0bf3d40dfad7515c572014da4b979d3310a", size = 98796 },
    { url = "https://files.pythonhosted.org/packages/ee/77/d637aa35497e0034ff846fd3330d1db26bc6fd9dd79c406e1341188b06a2/audioop_lts-0.2.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:f0ed1ad9bd862539ea875fb339ecb18fcc4148f8d9908f4502df28f94d23491a", size = 94116 },
    { url = "https://files.pythonhosted.org/packages/1a/60/7afc2abf46bbcf525a6ebc0305d85ab08dc2d1e2da72c48dbb35eee5b62c/audioop_lts-0.2.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:e1af3ff32b8c38a7d900382646e91f2fc515fd19dea37e9392275a5cbfdbff63", size = 91520 },
    { url = "https://files.pythonhosted.org/packages/65/6d/42d40da100be1afb661fd77c2b1c0dfab08af1540df57533621aea3db52a/audioop_lts-0.2.1-cp313-cp313t-win32.whl", hash = "sha256:f51bb55122a89f7a0817d7ac2319744b4640b5b446c4c3efcea5764ea99ae509", size = 26482 },
    { url = "https://files.pythonhosted.org/packages/01/09/f08494dca79f65212f5b273aecc5a2f96691bf3307cac29acfcf84300c01/audioop_lts-0.2.1-cp313-cp313t-win_amd64.whl", hash = "sha256:f0f2f336aa2aee2bce0b0dcc32bbba9178995454c7b979cf6ce086a8801e14c7", size = 30780 },
    { url = "https://files.pythonhosted.org/packages/5d/35/be73b6015511aa0173ec595fc579133b797ad532996f2998fd6b8d1bbe6b/audioop_lts-0.2.1-cp313-cp313t-win_arm64.whl", hash = "sha256:78bfb3703388c780edf900be66e07de5a3d4105ca8e8720c5c4d67927e0b15d0", size = 23918 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "cffi"
version = "1.17.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pycparser" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fc/97/c783634659c2920c3fc70419e3af40972dbaf758daa229a7d6ea6135c90d/cffi-1.17.1.tar.gz", hash = "sha256:1c39c6016c32bc48dd54561950ebd6836e1670f2ae46128f67cf49e789c52824", size = 516621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8d/f8/dd6c246b148639254dad4d6803eb6a54e8c85c6e11ec9df2cffa87571dbe/cffi-1.17.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:f3a2b4222ce6b60e2e8b337bb9596923045681d71e5a082783484d845390938e", size = 182989 },
    { url = "https://files.pythonhosted.org/packages/8b/f1/672d303ddf17c24fc83afd712316fda78dc6fce1cd53011b839483e1ecc8/cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:0984a4925a435b1da406122d4d7968dd861c1385afe3b45ba82b750f229811e2", size = 178802 },
    { url = "https://files.pythonhosted.org/packages/0e/2d/eab2e858a91fdff70533cab61dcff4a1f55ec60425832ddfdc9cd36bc8af/cffi-1.17.1-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d01b12eeeb4427d3110de311e1774046ad344f5b1a7403101878976ecd7a10f3", size = 454792 },
    { url = "https://files.pythonhosted.org/packages/75/b2/fbaec7c4455c604e29388d55599b99ebcc250a60050610fadde58932b7ee/cffi-1.17.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:706510fe141c86a69c8ddc029c7910003a17353970cff3b904ff0686a5927683", size = 478893 },
    { url = "https://files.pythonhosted.org/packages/4f/b7/6e4a2162178bf1935c336d4da8a9352cccab4d3a5d7914065490f08c0690/cffi-1.17.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:de55b766c7aa2e2a3092c51e0483d700341182f08e67c63630d5b6f200bb28e5", size = 485810 },
    { url = "https://files.pythonhosted.org/packages/c7/8a/1d0e4a9c26e54746dc08c2c6c037889124d4f59dffd853a659fa545f1b40/cffi-1.17.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c59d6e989d07460165cc5ad3c61f9fd8f1b4796eacbd81cee78957842b834af4", size = 471200 },
    { url = "https://files.pythonhosted.org/packages/26/9f/1aab65a6c0db35f43c4d1b4f580e8df53914310afc10ae0397d29d697af4/cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dd398dbc6773384a17fe0d3e7eeb8d1a21c2200473ee6806bb5e6a8e62bb73dd", size = 479447 },
    { url = "https://files.pythonhosted.org/packages/5f/e4/fb8b3dd8dc0e98edf1135ff067ae070bb32ef9d509d6cb0f538cd6f7483f/cffi-1.17.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:3edc8d958eb099c634dace3c7e16560ae474aa3803a5df240542b305d14e14ed", size = 484358 },
    { url = "https://files.pythonhosted.org/packages/f1/47/d7145bf2dc04684935d57d67dff9d6d795b2ba2796806bb109864be3a151/cffi-1.17.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:72e72408cad3d5419375fc87d289076ee319835bdfa2caad331e377589aebba9", size = 488469 },
    { url = "https://files.pythonhosted.org/packages/bf/ee/f94057fa6426481d663b88637a9a10e859e492c73d0384514a17d78ee205/cffi-1.17.1-cp313-cp313-win32.whl", hash = "sha256:e03eab0a8677fa80d646b5ddece1cbeaf556c313dcfac435ba11f107ba117b5d", size = 172475 },
    { url = "https://files.pythonhosted.org/packages/7c/fc/6a8cb64e5f0324877d503c854da15d76c1e50eb722e320b15345c4d0c6de/cffi-1.17.1-cp313-cp313-win_amd64.whl", hash = "sha256:f6a16c31041f09ead72d69f583767292f750d24913dadacf5756b966aacb3f1a", size = 182009 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622 },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435 },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653 },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231 },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243 },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442 },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147 },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057 },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454 },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174 },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166 },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064 },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641 },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626 },
]

[[package]]
name = "cryptography"
version = "44.0.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cffi", marker = "platform_python_implementation != 'PyPy'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/53/d6/1411ab4d6108ab167d06254c5be517681f1e331f90edf1379895bcb87020/cryptography-44.0.3.tar.gz", hash = "sha256:fe19d8bc5536a91a24a8133328880a41831b6c5df54599a8417b62fe015d3053", size = 711096 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/08/53/c776d80e9d26441bb3868457909b4e74dd9ccabd182e10b2b0ae7a07e265/cryptography-44.0.3-cp37-abi3-macosx_10_9_universal2.whl", hash = "sha256:962bc30480a08d133e631e8dfd4783ab71cc9e33d5d7c1e192f0b7c06397bb88", size = 6670281 },
    { url = "https://files.pythonhosted.org/packages/6a/06/af2cf8d56ef87c77319e9086601bef621bedf40f6f59069e1b6d1ec498c5/cryptography-44.0.3-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4ffc61e8f3bf5b60346d89cd3d37231019c17a081208dfbbd6e1605ba03fa137", size = 3959305 },
    { url = "https://files.pythonhosted.org/packages/ae/01/80de3bec64627207d030f47bf3536889efee8913cd363e78ca9a09b13c8e/cryptography-44.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:58968d331425a6f9eedcee087f77fd3c927c88f55368f43ff7e0a19891f2642c", size = 4171040 },
    { url = "https://files.pythonhosted.org/packages/bd/48/bb16b7541d207a19d9ae8b541c70037a05e473ddc72ccb1386524d4f023c/cryptography-44.0.3-cp37-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:e28d62e59a4dbd1d22e747f57d4f00c459af22181f0b2f787ea83f5a876d7c76", size = 3963411 },
    { url = "https://files.pythonhosted.org/packages/42/b2/7d31f2af5591d217d71d37d044ef5412945a8a8e98d5a2a8ae4fd9cd4489/cryptography-44.0.3-cp37-abi3-manylinux_2_28_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:af653022a0c25ef2e3ffb2c673a50e5a0d02fecc41608f4954176f1933b12359", size = 3689263 },
    { url = "https://files.pythonhosted.org/packages/25/50/c0dfb9d87ae88ccc01aad8eb93e23cfbcea6a6a106a9b63a7b14c1f93c75/cryptography-44.0.3-cp37-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:157f1f3b8d941c2bd8f3ffee0af9b049c9665c39d3da9db2dc338feca5e98a43", size = 4196198 },
    { url = "https://files.pythonhosted.org/packages/66/c9/55c6b8794a74da652690c898cb43906310a3e4e4f6ee0b5f8b3b3e70c441/cryptography-44.0.3-cp37-abi3-manylinux_2_34_aarch64.whl", hash = "sha256:c6cd67722619e4d55fdb42ead64ed8843d64638e9c07f4011163e46bc512cf01", size = 3966502 },
    { url = "https://files.pythonhosted.org/packages/b6/f7/7cb5488c682ca59a02a32ec5f975074084db4c983f849d47b7b67cc8697a/cryptography-44.0.3-cp37-abi3-manylinux_2_34_x86_64.whl", hash = "sha256:b424563394c369a804ecbee9b06dfb34997f19d00b3518e39f83a5642618397d", size = 4196173 },
    { url = "https://files.pythonhosted.org/packages/d2/0b/2f789a8403ae089b0b121f8f54f4a3e5228df756e2146efdf4a09a3d5083/cryptography-44.0.3-cp37-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:c91fc8e8fd78af553f98bc7f2a1d8db977334e4eea302a4bfd75b9461c2d8904", size = 4087713 },
    { url = "https://files.pythonhosted.org/packages/1d/aa/330c13655f1af398fc154089295cf259252f0ba5df93b4bc9d9c7d7f843e/cryptography-44.0.3-cp37-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:25cd194c39fa5a0aa4169125ee27d1172097857b27109a45fadc59653ec06f44", size = 4299064 },
    { url = "https://files.pythonhosted.org/packages/10/a8/8c540a421b44fd267a7d58a1fd5f072a552d72204a3f08194f98889de76d/cryptography-44.0.3-cp37-abi3-win32.whl", hash = "sha256:3be3f649d91cb182c3a6bd336de8b61a0a71965bd13d1a04a0e15b39c3d5809d", size = 2773887 },
    { url = "https://files.pythonhosted.org/packages/b9/0d/c4b1657c39ead18d76bbd122da86bd95bdc4095413460d09544000a17d56/cryptography-44.0.3-cp37-abi3-win_amd64.whl", hash = "sha256:3883076d5c4cc56dbef0b898a74eb6992fdac29a7b9013870b34efe4ddb39a0d", size = 3209737 },
    { url = "https://files.pythonhosted.org/packages/34/a3/ad08e0bcc34ad436013458d7528e83ac29910943cea42ad7dd4141a27bbb/cryptography-44.0.3-cp39-abi3-macosx_10_9_universal2.whl", hash = "sha256:5639c2b16764c6f76eedf722dbad9a0914960d3489c0cc38694ddf9464f1bb2f", size = 6673501 },
    { url = "https://files.pythonhosted.org/packages/b1/f0/7491d44bba8d28b464a5bc8cc709f25a51e3eac54c0a4444cf2473a57c37/cryptography-44.0.3-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f3ffef566ac88f75967d7abd852ed5f182da252d23fac11b4766da3957766759", size = 3960307 },
    { url = "https://files.pythonhosted.org/packages/f7/c8/e5c5d0e1364d3346a5747cdcd7ecbb23ca87e6dea4f942a44e88be349f06/cryptography-44.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:192ed30fac1728f7587c6f4613c29c584abdc565d7417c13904708db10206645", size = 4170876 },
    { url = "https://files.pythonhosted.org/packages/73/96/025cb26fc351d8c7d3a1c44e20cf9a01e9f7cf740353c9c7a17072e4b264/cryptography-44.0.3-cp39-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:7d5fe7195c27c32a64955740b949070f21cba664604291c298518d2e255931d2", size = 3964127 },
    { url = "https://files.pythonhosted.org/packages/01/44/eb6522db7d9f84e8833ba3bf63313f8e257729cf3a8917379473fcfd6601/cryptography-44.0.3-cp39-abi3-manylinux_2_28_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3f07943aa4d7dad689e3bb1638ddc4944cc5e0921e3c227486daae0e31a05e54", size = 3689164 },
    { url = "https://files.pythonhosted.org/packages/68/fb/d61a4defd0d6cee20b1b8a1ea8f5e25007e26aeb413ca53835f0cae2bcd1/cryptography-44.0.3-cp39-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:cb90f60e03d563ca2445099edf605c16ed1d5b15182d21831f58460c48bffb93", size = 4198081 },
    { url = "https://files.pythonhosted.org/packages/1b/50/457f6911d36432a8811c3ab8bd5a6090e8d18ce655c22820994913dd06ea/cryptography-44.0.3-cp39-abi3-manylinux_2_34_aarch64.whl", hash = "sha256:ab0b005721cc0039e885ac3503825661bd9810b15d4f374e473f8c89b7d5460c", size = 3967716 },
    { url = "https://files.pythonhosted.org/packages/35/6e/dca39d553075980ccb631955c47b93d87d27f3596da8d48b1ae81463d915/cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl", hash = "sha256:3bb0847e6363c037df8f6ede57d88eaf3410ca2267fb12275370a76f85786a6f", size = 4197398 },
    { url = "https://files.pythonhosted.org/packages/9b/9d/d1f2fe681eabc682067c66a74addd46c887ebacf39038ba01f8860338d3d/cryptography-44.0.3-cp39-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:b0cc66c74c797e1db750aaa842ad5b8b78e14805a9b5d1348dc603612d3e3ff5", size = 4087900 },
    { url = "https://files.pythonhosted.org/packages/c4/f5/3599e48c5464580b73b236aafb20973b953cd2e7b44c7c2533de1d888446/cryptography-44.0.3-cp39-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:6866df152b581f9429020320e5eb9794c8780e90f7ccb021940d7f50ee00ae0b", size = 4301067 },
    { url = "https://files.pythonhosted.org/packages/a7/6c/d2c48c8137eb39d0c193274db5c04a75dab20d2f7c3f81a7dcc3a8897701/cryptography-44.0.3-cp39-abi3-win32.whl", hash = "sha256:c138abae3a12a94c75c10499f1cbae81294a6f983b3af066390adee73f433028", size = 2775467 },
    { url = "https://files.pythonhosted.org/packages/c9/ad/51f212198681ea7b0deaaf8846ee10af99fba4e894f67b353524eab2bbe5/cryptography-44.0.3-cp39-abi3-win_amd64.whl", hash = "sha256:5d186f32e52e66994dce4f766884bcb9c68b8da62d61d9d215bfe5fb56d21334", size = 3210375 },
]

[[package]]
name = "deprecated"
version = "1.2.18"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wrapt" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/97/06afe62762c9a8a86af0cfb7bfdab22a43ad17138b07af5b1a58442690a2/deprecated-1.2.18.tar.gz", hash = "sha256:422b6f6d859da6f2ef57857761bfb392480502a64c3028ca9bbe86085d72115d", size = 2928744 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl", hash = "sha256:bd5011788200372a32418f888e326a09ff80d0214bd961147cfed01b5c018eec", size = 9998 },
]

[[package]]
name = "discord-py"
version = "2.5.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohttp" },
    { name = "audioop-lts" },
]
sdist = { url = "https://files.pythonhosted.org/packages/7f/dd/5817c7af5e614e45cdf38cbf6c3f4597590c442822a648121a34dee7fa0f/discord_py-2.5.2.tar.gz", hash = "sha256:01cd362023bfea1a4a1d43f5280b5ef00cad2c7eba80098909f98bf28e578524", size = 1054879 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/57/a8/dc908a0fe4cd7e3950c9fa6906f7bf2e5d92d36b432f84897185e1b77138/discord_py-2.5.2-py3-none-any.whl", hash = "sha256:81f23a17c50509ffebe0668441cb80c139e74da5115305f70e27ce821361295a", size = 1155105 },
]

[[package]]
name = "frozenlist"
version = "1.6.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/f4/d744cba2da59b5c1d88823cf9e8a6c74e4659e2b27604ed973be2a0bf5ab/frozenlist-1.6.0.tar.gz", hash = "sha256:b99655c32c1c8e06d111e7f41c06c29a5318cb1835df23a45518e02a47c63b68", size = 42831 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6f/e5/04c7090c514d96ca00887932417f04343ab94904a56ab7f57861bf63652d/frozenlist-1.6.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:1d7fb014fe0fbfee3efd6a94fc635aeaa68e5e1720fe9e57357f2e2c6e1a647e", size = 158182 },
    { url = "https://files.pythonhosted.org/packages/e9/8f/60d0555c61eec855783a6356268314d204137f5e0c53b59ae2fc28938c99/frozenlist-1.6.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:01bcaa305a0fdad12745502bfd16a1c75b14558dabae226852f9159364573117", size = 122838 },
    { url = "https://files.pythonhosted.org/packages/5a/a7/d0ec890e3665b4b3b7c05dc80e477ed8dc2e2e77719368e78e2cd9fec9c8/frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:8b314faa3051a6d45da196a2c495e922f987dc848e967d8cfeaee8a0328b1cd4", size = 120980 },
    { url = "https://files.pythonhosted.org/packages/cc/19/9b355a5e7a8eba903a008579964192c3e427444752f20b2144b10bb336df/frozenlist-1.6.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:da62fecac21a3ee10463d153549d8db87549a5e77eefb8c91ac84bb42bb1e4e3", size = 305463 },
    { url = "https://files.pythonhosted.org/packages/9c/8d/5b4c758c2550131d66935ef2fa700ada2461c08866aef4229ae1554b93ca/frozenlist-1.6.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:d1eb89bf3454e2132e046f9599fbcf0a4483ed43b40f545551a39316d0201cd1", size = 297985 },
    { url = "https://files.pythonhosted.org/packages/48/2c/537ec09e032b5865715726b2d1d9813e6589b571d34d01550c7aeaad7e53/frozenlist-1.6.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d18689b40cb3936acd971f663ccb8e2589c45db5e2c5f07e0ec6207664029a9c", size = 311188 },
    { url = "https://files.pythonhosted.org/packages/31/2f/1aa74b33f74d54817055de9a4961eff798f066cdc6f67591905d4fc82a84/frozenlist-1.6.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:e67ddb0749ed066b1a03fba812e2dcae791dd50e5da03be50b6a14d0c1a9ee45", size = 311874 },
    { url = "https://files.pythonhosted.org/packages/bf/f0/cfec18838f13ebf4b37cfebc8649db5ea71a1b25dacd691444a10729776c/frozenlist-1.6.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fc5e64626e6682638d6e44398c9baf1d6ce6bc236d40b4b57255c9d3f9761f1f", size = 291897 },
    { url = "https://files.pythonhosted.org/packages/ea/a5/deb39325cbbea6cd0a46db8ccd76150ae2fcbe60d63243d9df4a0b8c3205/frozenlist-1.6.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:437cfd39564744ae32ad5929e55b18ebd88817f9180e4cc05e7d53b75f79ce85", size = 305799 },
    { url = "https://files.pythonhosted.org/packages/78/22/6ddec55c5243a59f605e4280f10cee8c95a449f81e40117163383829c241/frozenlist-1.6.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:62dd7df78e74d924952e2feb7357d826af8d2f307557a779d14ddf94d7311be8", size = 302804 },
    { url = "https://files.pythonhosted.org/packages/5d/b7/d9ca9bab87f28855063c4d202936800219e39db9e46f9fb004d521152623/frozenlist-1.6.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:a66781d7e4cddcbbcfd64de3d41a61d6bdde370fc2e38623f30b2bd539e84a9f", size = 316404 },
    { url = "https://files.pythonhosted.org/packages/a6/3a/1255305db7874d0b9eddb4fe4a27469e1fb63720f1fc6d325a5118492d18/frozenlist-1.6.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:482fe06e9a3fffbcd41950f9d890034b4a54395c60b5e61fae875d37a699813f", size = 295572 },
    { url = "https://files.pythonhosted.org/packages/2a/f2/8d38eeee39a0e3a91b75867cc102159ecccf441deb6ddf67be96d3410b84/frozenlist-1.6.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:e4f9373c500dfc02feea39f7a56e4f543e670212102cc2eeb51d3a99c7ffbde6", size = 307601 },
    { url = "https://files.pythonhosted.org/packages/38/04/80ec8e6b92f61ef085422d7b196822820404f940950dde5b2e367bede8bc/frozenlist-1.6.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:e69bb81de06827147b7bfbaeb284d85219fa92d9f097e32cc73675f279d70188", size = 314232 },
    { url = "https://files.pythonhosted.org/packages/3a/58/93b41fb23e75f38f453ae92a2f987274c64637c450285577bd81c599b715/frozenlist-1.6.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:7613d9977d2ab4a9141dde4a149f4357e4065949674c5649f920fec86ecb393e", size = 308187 },
    { url = "https://files.pythonhosted.org/packages/6a/a2/e64df5c5aa36ab3dee5a40d254f3e471bb0603c225f81664267281c46a2d/frozenlist-1.6.0-cp313-cp313-win32.whl", hash = "sha256:4def87ef6d90429f777c9d9de3961679abf938cb6b7b63d4a7eb8a268babfce4", size = 114772 },
    { url = "https://files.pythonhosted.org/packages/a0/77/fead27441e749b2d574bb73d693530d59d520d4b9e9679b8e3cb779d37f2/frozenlist-1.6.0-cp313-cp313-win_amd64.whl", hash = "sha256:37a8a52c3dfff01515e9bbbee0e6063181362f9de3db2ccf9bc96189b557cbfd", size = 119847 },
    { url = "https://files.pythonhosted.org/packages/df/bd/cc6d934991c1e5d9cafda83dfdc52f987c7b28343686aef2e58a9cf89f20/frozenlist-1.6.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:46138f5a0773d064ff663d273b309b696293d7a7c00a0994c5c13a5078134b64", size = 174937 },
    { url = "https://files.pythonhosted.org/packages/f2/a2/daf945f335abdbfdd5993e9dc348ef4507436936ab3c26d7cfe72f4843bf/frozenlist-1.6.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:f88bc0a2b9c2a835cb888b32246c27cdab5740059fb3688852bf91e915399b91", size = 136029 },
    { url = "https://files.pythonhosted.org/packages/51/65/4c3145f237a31247c3429e1c94c384d053f69b52110a0d04bfc8afc55fb2/frozenlist-1.6.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:777704c1d7655b802c7850255639672e90e81ad6fa42b99ce5ed3fbf45e338dd", size = 134831 },
    { url = "https://files.pythonhosted.org/packages/77/38/03d316507d8dea84dfb99bdd515ea245628af964b2bf57759e3c9205cc5e/frozenlist-1.6.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:85ef8d41764c7de0dcdaf64f733a27352248493a85a80661f3c678acd27e31f2", size = 392981 },
    { url = "https://files.pythonhosted.org/packages/37/02/46285ef9828f318ba400a51d5bb616ded38db8466836a9cfa39f3903260b/frozenlist-1.6.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:da5cb36623f2b846fb25009d9d9215322318ff1c63403075f812b3b2876c8506", size = 371999 },
    { url = "https://files.pythonhosted.org/packages/0d/64/1212fea37a112c3c5c05bfb5f0a81af4836ce349e69be75af93f99644da9/frozenlist-1.6.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:cbb56587a16cf0fb8acd19e90ff9924979ac1431baea8681712716a8337577b0", size = 392200 },
    { url = "https://files.pythonhosted.org/packages/81/ce/9a6ea1763e3366e44a5208f76bf37c76c5da570772375e4d0be85180e588/frozenlist-1.6.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c6154c3ba59cda3f954c6333025369e42c3acd0c6e8b6ce31eb5c5b8116c07e0", size = 390134 },
    { url = "https://files.pythonhosted.org/packages/bc/36/939738b0b495b2c6d0c39ba51563e453232813042a8d908b8f9544296c29/frozenlist-1.6.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2e8246877afa3f1ae5c979fe85f567d220f86a50dc6c493b9b7d8191181ae01e", size = 365208 },
    { url = "https://files.pythonhosted.org/packages/b4/8b/939e62e93c63409949c25220d1ba8e88e3960f8ef6a8d9ede8f94b459d27/frozenlist-1.6.0-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7b0f6cce16306d2e117cf9db71ab3a9e8878a28176aeaf0dbe35248d97b28d0c", size = 385548 },
    { url = "https://files.pythonhosted.org/packages/62/38/22d2873c90102e06a7c5a3a5b82ca47e393c6079413e8a75c72bff067fa8/frozenlist-1.6.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:1b8e8cd8032ba266f91136d7105706ad57770f3522eac4a111d77ac126a25a9b", size = 391123 },
    { url = "https://files.pythonhosted.org/packages/44/78/63aaaf533ee0701549500f6d819be092c6065cb5c577edb70c09df74d5d0/frozenlist-1.6.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:e2ada1d8515d3ea5378c018a5f6d14b4994d4036591a52ceaf1a1549dec8e1ad", size = 394199 },
    { url = "https://files.pythonhosted.org/packages/54/45/71a6b48981d429e8fbcc08454dc99c4c2639865a646d549812883e9c9dd3/frozenlist-1.6.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:cdb2c7f071e4026c19a3e32b93a09e59b12000751fc9b0b7758da899e657d215", size = 373854 },
    { url = "https://files.pythonhosted.org/packages/3f/f3/dbf2a5e11736ea81a66e37288bf9f881143a7822b288a992579ba1b4204d/frozenlist-1.6.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:03572933a1969a6d6ab509d509e5af82ef80d4a5d4e1e9f2e1cdd22c77a3f4d2", size = 395412 },
    { url = "https://files.pythonhosted.org/packages/b3/f1/c63166806b331f05104d8ea385c4acd511598568b1f3e4e8297ca54f2676/frozenlist-1.6.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:77effc978947548b676c54bbd6a08992759ea6f410d4987d69feea9cd0919911", size = 394936 },
    { url = "https://files.pythonhosted.org/packages/ef/ea/4f3e69e179a430473eaa1a75ff986526571215fefc6b9281cdc1f09a4eb8/frozenlist-1.6.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:a2bda8be77660ad4089caf2223fdbd6db1858462c4b85b67fbfa22102021e497", size = 391459 },
    { url = "https://files.pythonhosted.org/packages/d3/c3/0fc2c97dea550df9afd072a37c1e95421652e3206bbeaa02378b24c2b480/frozenlist-1.6.0-cp313-cp313t-win32.whl", hash = "sha256:a4d96dc5bcdbd834ec6b0f91027817214216b5b30316494d2b1aebffb87c534f", size = 128797 },
    { url = "https://files.pythonhosted.org/packages/ae/f5/79c9320c5656b1965634fe4be9c82b12a3305bdbc58ad9cb941131107b20/frozenlist-1.6.0-cp313-cp313t-win_amd64.whl", hash = "sha256:e18036cb4caa17ea151fd5f3d70be9d354c99eb8cf817a3ccde8a7873b074348", size = 134709 },
    { url = "https://files.pythonhosted.org/packages/71/3e/b04a0adda73bd52b390d730071c0d577073d3d26740ee1bad25c3ad0f37b/frozenlist-1.6.0-py3-none-any.whl", hash = "sha256:535eec9987adb04701266b92745d6cdcef2e77669299359c3009c3404dd5d191", size = 12404 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "multidict"
version = "6.4.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/da/2c/e367dfb4c6538614a0c9453e510d75d66099edf1c4e69da1b5ce691a1931/multidict-6.4.3.tar.gz", hash = "sha256:3ada0b058c9f213c5f95ba301f922d402ac234f1111a7d8fd70f1b99f3c281ec", size = 89372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6c/4b/86fd786d03915c6f49998cf10cd5fe6b6ac9e9a071cb40885d2e080fb90d/multidict-6.4.3-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:7a76534263d03ae0cfa721fea40fd2b5b9d17a6f85e98025931d41dc49504474", size = 63831 },
    { url = "https://files.pythonhosted.org/packages/45/05/9b51fdf7aef2563340a93be0a663acba2c428c4daeaf3960d92d53a4a930/multidict-6.4.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:805031c2f599eee62ac579843555ed1ce389ae00c7e9f74c2a1b45e0564a88dd", size = 37888 },
    { url = "https://files.pythonhosted.org/packages/0b/43/53fc25394386c911822419b522181227ca450cf57fea76e6188772a1bd91/multidict-6.4.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:c56c179839d5dcf51d565132185409d1d5dd8e614ba501eb79023a6cab25576b", size = 36852 },
    { url = "https://files.pythonhosted.org/packages/8a/68/7b99c751e822467c94a235b810a2fd4047d4ecb91caef6b5c60116991c4b/multidict-6.4.3-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9c64f4ddb3886dd8ab71b68a7431ad4aa01a8fa5be5b11543b29674f29ca0ba3", size = 223644 },
    { url = "https://files.pythonhosted.org/packages/80/1b/d458d791e4dd0f7e92596667784fbf99e5c8ba040affe1ca04f06b93ae92/multidict-6.4.3-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3002a856367c0b41cad6784f5b8d3ab008eda194ed7864aaa58f65312e2abcac", size = 230446 },
    { url = "https://files.pythonhosted.org/packages/e2/46/9793378d988905491a7806d8987862dc5a0bae8a622dd896c4008c7b226b/multidict-6.4.3-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3d75e621e7d887d539d6e1d789f0c64271c250276c333480a9e1de089611f790", size = 231070 },
    { url = "https://files.pythonhosted.org/packages/a7/b8/b127d3e1f8dd2a5bf286b47b24567ae6363017292dc6dec44656e6246498/multidict-6.4.3-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:995015cf4a3c0d72cbf453b10a999b92c5629eaf3a0c3e1efb4b5c1f602253bb", size = 229956 },
    { url = "https://files.pythonhosted.org/packages/0c/93/f70a4c35b103fcfe1443059a2bb7f66e5c35f2aea7804105ff214f566009/multidict-6.4.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a2b0fabae7939d09d7d16a711468c385272fa1b9b7fb0d37e51143585d8e72e0", size = 222599 },
    { url = "https://files.pythonhosted.org/packages/63/8c/e28e0eb2fe34921d6aa32bfc4ac75b09570b4d6818cc95d25499fe08dc1d/multidict-6.4.3-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:61ed4d82f8a1e67eb9eb04f8587970d78fe7cddb4e4d6230b77eda23d27938f9", size = 216136 },
    { url = "https://files.pythonhosted.org/packages/72/f5/fbc81f866585b05f89f99d108be5d6ad170e3b6c4d0723d1a2f6ba5fa918/multidict-6.4.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:062428944a8dc69df9fdc5d5fc6279421e5f9c75a9ee3f586f274ba7b05ab3c8", size = 228139 },
    { url = "https://files.pythonhosted.org/packages/bb/ba/7d196bad6b85af2307d81f6979c36ed9665f49626f66d883d6c64d156f78/multidict-6.4.3-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:b90e27b4674e6c405ad6c64e515a505c6d113b832df52fdacb6b1ffd1fa9a1d1", size = 226251 },
    { url = "https://files.pythonhosted.org/packages/cc/e2/fae46a370dce79d08b672422a33df721ec8b80105e0ea8d87215ff6b090d/multidict-6.4.3-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7d50d4abf6729921e9613d98344b74241572b751c6b37feed75fb0c37bd5a817", size = 221868 },
    { url = "https://files.pythonhosted.org/packages/26/20/bbc9a3dec19d5492f54a167f08546656e7aef75d181d3d82541463450e88/multidict-6.4.3-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:43fe10524fb0a0514be3954be53258e61d87341008ce4914f8e8b92bee6f875d", size = 233106 },
    { url = "https://files.pythonhosted.org/packages/ee/8d/f30ae8f5ff7a2461177f4d8eb0d8f69f27fb6cfe276b54ec4fd5a282d918/multidict-6.4.3-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:236966ca6c472ea4e2d3f02f6673ebfd36ba3f23159c323f5a496869bc8e47c9", size = 230163 },
    { url = "https://files.pythonhosted.org/packages/15/e9/2833f3c218d3c2179f3093f766940ded6b81a49d2e2f9c46ab240d23dfec/multidict-6.4.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:422a5ec315018e606473ba1f5431e064cf8b2a7468019233dcf8082fabad64c8", size = 225906 },
    { url = "https://files.pythonhosted.org/packages/f1/31/6edab296ac369fd286b845fa5dd4c409e63bc4655ed8c9510fcb477e9ae9/multidict-6.4.3-cp313-cp313-win32.whl", hash = "sha256:f901a5aace8e8c25d78960dcc24c870c8d356660d3b49b93a78bf38eb682aac3", size = 35238 },
    { url = "https://files.pythonhosted.org/packages/23/57/2c0167a1bffa30d9a1383c3dab99d8caae985defc8636934b5668830d2ef/multidict-6.4.3-cp313-cp313-win_amd64.whl", hash = "sha256:1c152c49e42277bc9a2f7b78bd5fa10b13e88d1b0328221e7aef89d5c60a99a5", size = 38799 },
    { url = "https://files.pythonhosted.org/packages/c9/13/2ead63b9ab0d2b3080819268acb297bd66e238070aa8d42af12b08cbee1c/multidict-6.4.3-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:be8751869e28b9c0d368d94f5afcb4234db66fe8496144547b4b6d6a0645cfc6", size = 68642 },
    { url = "https://files.pythonhosted.org/packages/85/45/f1a751e1eede30c23951e2ae274ce8fad738e8a3d5714be73e0a41b27b16/multidict-6.4.3-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0d4b31f8a68dccbcd2c0ea04f0e014f1defc6b78f0eb8b35f2265e8716a6df0c", size = 40028 },
    { url = "https://files.pythonhosted.org/packages/a7/29/fcc53e886a2cc5595cc4560df333cb9630257bda65003a7eb4e4e0d8f9c1/multidict-6.4.3-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:032efeab3049e37eef2ff91271884303becc9e54d740b492a93b7e7266e23756", size = 39424 },
    { url = "https://files.pythonhosted.org/packages/f6/f0/056c81119d8b88703971f937b371795cab1407cd3c751482de5bfe1a04a9/multidict-6.4.3-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9e78006af1a7c8a8007e4f56629d7252668344442f66982368ac06522445e375", size = 226178 },
    { url = "https://files.pythonhosted.org/packages/a3/79/3b7e5fea0aa80583d3a69c9d98b7913dfd4fbc341fb10bb2fb48d35a9c21/multidict-6.4.3-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:daeac9dd30cda8703c417e4fddccd7c4dc0c73421a0b54a7da2713be125846be", size = 222617 },
    { url = "https://files.pythonhosted.org/packages/06/db/3ed012b163e376fc461e1d6a67de69b408339bc31dc83d39ae9ec3bf9578/multidict-6.4.3-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1f6f90700881438953eae443a9c6f8a509808bc3b185246992c4233ccee37fea", size = 227919 },
    { url = "https://files.pythonhosted.org/packages/b1/db/0433c104bca380989bc04d3b841fc83e95ce0c89f680e9ea4251118b52b6/multidict-6.4.3-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f84627997008390dd15762128dcf73c3365f4ec0106739cde6c20a07ed198ec8", size = 226097 },
    { url = "https://files.pythonhosted.org/packages/c2/95/910db2618175724dd254b7ae635b6cd8d2947a8b76b0376de7b96d814dab/multidict-6.4.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3307b48cd156153b117c0ea54890a3bdbf858a5b296ddd40dc3852e5f16e9b02", size = 220706 },
    { url = "https://files.pythonhosted.org/packages/d1/af/aa176c6f5f1d901aac957d5258d5e22897fe13948d1e69063ae3d5d0ca01/multidict-6.4.3-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ead46b0fa1dcf5af503a46e9f1c2e80b5d95c6011526352fa5f42ea201526124", size = 211728 },
    { url = "https://files.pythonhosted.org/packages/e7/42/d51cc5fc1527c3717d7f85137d6c79bb7a93cd214c26f1fc57523774dbb5/multidict-6.4.3-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:1748cb2743bedc339d63eb1bca314061568793acd603a6e37b09a326334c9f44", size = 226276 },
    { url = "https://files.pythonhosted.org/packages/28/6b/d836dea45e0b8432343ba4acf9a8ecaa245da4c0960fb7ab45088a5e568a/multidict-6.4.3-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:acc9fa606f76fc111b4569348cc23a771cb52c61516dcc6bcef46d612edb483b", size = 212069 },
    { url = "https://files.pythonhosted.org/packages/55/34/0ee1a7adb3560e18ee9289c6e5f7db54edc312b13e5c8263e88ea373d12c/multidict-6.4.3-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:31469d5832b5885adeb70982e531ce86f8c992334edd2f2254a10fa3182ac504", size = 217858 },
    { url = "https://files.pythonhosted.org/packages/04/08/586d652c2f5acefe0cf4e658eedb4d71d4ba6dfd4f189bd81b400fc1bc6b/multidict-6.4.3-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:ba46b51b6e51b4ef7bfb84b82f5db0dc5e300fb222a8a13b8cd4111898a869cf", size = 226988 },
    { url = "https://files.pythonhosted.org/packages/82/e3/cc59c7e2bc49d7f906fb4ffb6d9c3a3cf21b9f2dd9c96d05bef89c2b1fd1/multidict-6.4.3-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:389cfefb599edf3fcfd5f64c0410da686f90f5f5e2c4d84e14f6797a5a337af4", size = 220435 },
    { url = "https://files.pythonhosted.org/packages/e0/32/5c3a556118aca9981d883f38c4b1bfae646f3627157f70f4068e5a648955/multidict-6.4.3-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:64bc2bbc5fba7b9db5c2c8d750824f41c6994e3882e6d73c903c2afa78d091e4", size = 221494 },
    { url = "https://files.pythonhosted.org/packages/b9/3b/1599631f59024b75c4d6e3069f4502409970a336647502aaf6b62fb7ac98/multidict-6.4.3-cp313-cp313t-win32.whl", hash = "sha256:0ecdc12ea44bab2807d6b4a7e5eef25109ab1c82a8240d86d3c1fc9f3b72efd5", size = 41775 },
    { url = "https://files.pythonhosted.org/packages/e8/4e/09301668d675d02ca8e8e1a3e6be046619e30403f5ada2ed5b080ae28d02/multidict-6.4.3-cp313-cp313t-win_amd64.whl", hash = "sha256:7146a8742ea71b5d7d955bffcef58a9e6e04efba704b52a460134fefd10a8208", size = 45946 },
    { url = "https://files.pythonhosted.org/packages/96/10/7d526c8974f017f1e7ca584c71ee62a638e9334d8d33f27d7cdfc9ae79e4/multidict-6.4.3-py3-none-any.whl", hash = "sha256:59fe01ee8e2a1e8ceb3f6dbb216b09c8d9f4ef1c22c4fc825d045a147fa2ebc9", size = 10400 },
]

[[package]]
name = "propcache"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz", hash = "sha256:40d980c33765359098837527e18eddefc9a24cea5b45e078a7f3bb5b032c6ecf", size = 43651 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/60/f645cc8b570f99be3cf46714170c2de4b4c9d6b827b912811eff1eb8a412/propcache-0.3.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:f1528ec4374617a7a753f90f20e2f551121bb558fcb35926f99e3c42367164b8", size = 77865 },
    { url = "https://files.pythonhosted.org/packages/6f/d4/c1adbf3901537582e65cf90fd9c26fde1298fde5a2c593f987112c0d0798/propcache-0.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dc1915ec523b3b494933b5424980831b636fe483d7d543f7afb7b3bf00f0c10f", size = 45452 },
    { url = "https://files.pythonhosted.org/packages/d1/b5/fe752b2e63f49f727c6c1c224175d21b7d1727ce1d4873ef1c24c9216830/propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a110205022d077da24e60b3df8bcee73971be9575dec5573dd17ae5d81751111", size = 44800 },
    { url = "https://files.pythonhosted.org/packages/62/37/fc357e345bc1971e21f76597028b059c3d795c5ca7690d7a8d9a03c9708a/propcache-0.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d249609e547c04d190e820d0d4c8ca03ed4582bcf8e4e160a6969ddfb57b62e5", size = 225804 },
    { url = "https://files.pythonhosted.org/packages/0d/f1/16e12c33e3dbe7f8b737809bad05719cff1dccb8df4dafbcff5575002c0e/propcache-0.3.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5ced33d827625d0a589e831126ccb4f5c29dfdf6766cac441d23995a65825dcb", size = 230650 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/018b9f2ed876bf5091e60153f727e8f9073d97573f790ff7cdf6bc1d1fb8/propcache-0.3.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4114c4ada8f3181af20808bedb250da6bae56660e4b8dfd9cd95d4549c0962f7", size = 234235 },
    { url = "https://files.pythonhosted.org/packages/45/5f/3faee66fc930dfb5da509e34c6ac7128870631c0e3582987fad161fcb4b1/propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:975af16f406ce48f1333ec5e912fe11064605d5c5b3f6746969077cc3adeb120", size = 228249 },
    { url = "https://files.pythonhosted.org/packages/62/1e/a0d5ebda5da7ff34d2f5259a3e171a94be83c41eb1e7cd21a2105a84a02e/propcache-0.3.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a34aa3a1abc50740be6ac0ab9d594e274f59960d3ad253cd318af76b996dd654", size = 214964 },
    { url = "https://files.pythonhosted.org/packages/db/a0/d72da3f61ceab126e9be1f3bc7844b4e98c6e61c985097474668e7e52152/propcache-0.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9cec3239c85ed15bfaded997773fdad9fb5662b0a7cbc854a43f291eb183179e", size = 222501 },
    { url = "https://files.pythonhosted.org/packages/18/6d/a008e07ad7b905011253adbbd97e5b5375c33f0b961355ca0a30377504ac/propcache-0.3.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:05543250deac8e61084234d5fc54f8ebd254e8f2b39a16b1dce48904f45b744b", size = 217917 },
    { url = "https://files.pythonhosted.org/packages/98/37/02c9343ffe59e590e0e56dc5c97d0da2b8b19fa747ebacf158310f97a79a/propcache-0.3.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5cb5918253912e088edbf023788de539219718d3b10aef334476b62d2b53de53", size = 217089 },
    { url = "https://files.pythonhosted.org/packages/53/1b/d3406629a2c8a5666d4674c50f757a77be119b113eedd47b0375afdf1b42/propcache-0.3.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f3bbecd2f34d0e6d3c543fdb3b15d6b60dd69970c2b4c822379e5ec8f6f621d5", size = 228102 },
    { url = "https://files.pythonhosted.org/packages/cd/a7/3664756cf50ce739e5f3abd48febc0be1a713b1f389a502ca819791a6b69/propcache-0.3.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aca63103895c7d960a5b9b044a83f544b233c95e0dcff114389d64d762017af7", size = 230122 },
    { url = "https://files.pythonhosted.org/packages/35/36/0bbabaacdcc26dac4f8139625e930f4311864251276033a52fd52ff2a274/propcache-0.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5a0a9898fdb99bf11786265468571e628ba60af80dc3f6eb89a3545540c6b0ef", size = 226818 },
    { url = "https://files.pythonhosted.org/packages/cc/27/4e0ef21084b53bd35d4dae1634b6d0bad35e9c58ed4f032511acca9d4d26/propcache-0.3.1-cp313-cp313-win32.whl", hash = "sha256:3a02a28095b5e63128bcae98eb59025924f121f048a62393db682f049bf4ac24", size = 40112 },
    { url = "https://files.pythonhosted.org/packages/a6/2c/a54614d61895ba6dd7ac8f107e2b2a0347259ab29cbf2ecc7b94fa38c4dc/propcache-0.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:813fbb8b6aea2fc9659815e585e548fe706d6f663fa73dff59a1677d4595a037", size = 44034 },
    { url = "https://files.pythonhosted.org/packages/5a/a8/0a4fd2f664fc6acc66438370905124ce62e84e2e860f2557015ee4a61c7e/propcache-0.3.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:a444192f20f5ce8a5e52761a031b90f5ea6288b1eef42ad4c7e64fef33540b8f", size = 82613 },
    { url = "https://files.pythonhosted.org/packages/4d/e5/5ef30eb2cd81576256d7b6caaa0ce33cd1d2c2c92c8903cccb1af1a4ff2f/propcache-0.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0fbe94666e62ebe36cd652f5fc012abfbc2342de99b523f8267a678e4dfdee3c", size = 47763 },
    { url = "https://files.pythonhosted.org/packages/87/9a/87091ceb048efeba4d28e903c0b15bcc84b7c0bf27dc0261e62335d9b7b8/propcache-0.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f011f104db880f4e2166bcdcf7f58250f7a465bc6b068dc84c824a3d4a5c94dc", size = 47175 },
    { url = "https://files.pythonhosted.org/packages/3e/2f/854e653c96ad1161f96194c6678a41bbb38c7947d17768e8811a77635a08/propcache-0.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e584b6d388aeb0001d6d5c2bd86b26304adde6d9bb9bfa9c4889805021b96de", size = 292265 },
    { url = "https://files.pythonhosted.org/packages/40/8d/090955e13ed06bc3496ba4a9fb26c62e209ac41973cb0d6222de20c6868f/propcache-0.3.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8a17583515a04358b034e241f952f1715243482fc2c2945fd99a1b03a0bd77d6", size = 294412 },
    { url = "https://files.pythonhosted.org/packages/39/e6/d51601342e53cc7582449e6a3c14a0479fab2f0750c1f4d22302e34219c6/propcache-0.3.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5aed8d8308215089c0734a2af4f2e95eeb360660184ad3912686c181e500b2e7", size = 294290 },
    { url = "https://files.pythonhosted.org/packages/3b/4d/be5f1a90abc1881884aa5878989a1acdafd379a91d9c7e5e12cef37ec0d7/propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d8e309ff9a0503ef70dc9a0ebd3e69cf7b3894c9ae2ae81fc10943c37762458", size = 282926 },
    { url = "https://files.pythonhosted.org/packages/57/2b/8f61b998c7ea93a2b7eca79e53f3e903db1787fca9373af9e2cf8dc22f9d/propcache-0.3.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b655032b202028a582d27aeedc2e813299f82cb232f969f87a4fde491a233f11", size = 267808 },
    { url = "https://files.pythonhosted.org/packages/11/1c/311326c3dfce59c58a6098388ba984b0e5fb0381ef2279ec458ef99bd547/propcache-0.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9f64d91b751df77931336b5ff7bafbe8845c5770b06630e27acd5dbb71e1931c", size = 290916 },
    { url = "https://files.pythonhosted.org/packages/4b/74/91939924b0385e54dc48eb2e4edd1e4903ffd053cf1916ebc5347ac227f7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:19a06db789a4bd896ee91ebc50d059e23b3639c25d58eb35be3ca1cbe967c3bf", size = 262661 },
    { url = "https://files.pythonhosted.org/packages/c2/d7/e6079af45136ad325c5337f5dd9ef97ab5dc349e0ff362fe5c5db95e2454/propcache-0.3.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:bef100c88d8692864651b5f98e871fb090bd65c8a41a1cb0ff2322db39c96c27", size = 264384 },
    { url = "https://files.pythonhosted.org/packages/b7/d5/ba91702207ac61ae6f1c2da81c5d0d6bf6ce89e08a2b4d44e411c0bbe867/propcache-0.3.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:87380fb1f3089d2a0b8b00f006ed12bd41bd858fabfa7330c954c70f50ed8757", size = 291420 },
    { url = "https://files.pythonhosted.org/packages/58/70/2117780ed7edcd7ba6b8134cb7802aada90b894a9810ec56b7bb6018bee7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e474fc718e73ba5ec5180358aa07f6aded0ff5f2abe700e3115c37d75c947e18", size = 290880 },
    { url = "https://files.pythonhosted.org/packages/4a/1f/ecd9ce27710021ae623631c0146719280a929d895a095f6d85efb6a0be2e/propcache-0.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:17d1c688a443355234f3c031349da69444be052613483f3e4158eef751abcd8a", size = 287407 },
    { url = "https://files.pythonhosted.org/packages/3e/66/2e90547d6b60180fb29e23dc87bd8c116517d4255240ec6d3f7dc23d1926/propcache-0.3.1-cp313-cp313t-win32.whl", hash = "sha256:359e81a949a7619802eb601d66d37072b79b79c2505e6d3fd8b945538411400d", size = 42573 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/50ad8599399d1861b4d2b6b45271f0ef6af1b09b0a2386a46dbaf19c9535/propcache-0.3.1-cp313-cp313t-win_amd64.whl", hash = "sha256:e7fb9a84c9abbf2b2683fa3e7b0d7da4d8ecf139a1c635732a8bda29c5214b0e", size = 46757 },
    { url = "https://files.pythonhosted.org/packages/b8/d3/c3cb8f1d6ae3b37f83e1de806713a9b3642c5895f0215a62e1a4bd6e5e34/propcache-0.3.1-py3-none-any.whl", hash = "sha256:9a8ecf38de50a7f518c21568c80f985e776397b902f1ce0b01f799aba1608b40", size = 12376 },
]

[[package]]
name = "pycparser"
version = "2.22"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1d/b2/31537cf4b1ca988837256c910a668b553fceb8f069bedc4b1c826024b52c/pycparser-2.22.tar.gz", hash = "sha256:491c8be9c040f5390f5bf44a5b07752bd07f56edf992381b05c701439eec10f6", size = 172736 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl", hash = "sha256:c3702b6d3dd8c7abc1afa565d7e63d53a1d0bd86cdc24edd75470f4de499cfcc", size = 117552 },
]

[[package]]
name = "pygithub"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "deprecated" },
    { name = "pyjwt", extra = ["crypto"] },
    { name = "pynacl" },
    { name = "requests" },
    { name = "typing-extensions" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/c0/88/e08ab18dc74b2916f48703ed1a797d57cb64eca0e23b0a9254e13cfe3911/pygithub-2.6.1.tar.gz", hash = "sha256:b5c035392991cca63959e9453286b41b54d83bf2de2daa7d7ff7e4312cebf3bf", size = 3659473 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ac/fc/a444cd19ccc8c4946a512f3827ed0b3565c88488719d800d54a75d541c0b/PyGithub-2.6.1-py3-none-any.whl", hash = "sha256:6f2fa6d076ccae475f9fc392cc6cdbd54db985d4f69b8833a28397de75ed6ca3", size = 410451 },
]

[[package]]
name = "pyjwt"
version = "2.10.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/46/bd74733ff231675599650d3e47f361794b22ef3e3770998dda30d3b63726/pyjwt-2.10.1.tar.gz", hash = "sha256:3cc5772eb20009233caf06e9d8a0577824723b44e6648ee0a2aedb6cf9381953", size = 87785 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/61/ad/689f02752eeec26aed679477e80e632ef1b682313be70793d798c1d5fc8f/PyJWT-2.10.1-py3-none-any.whl", hash = "sha256:dcdd193e30abefd5debf142f9adfcdd2b58004e644f25406ffaebd50bd98dacb", size = 22997 },
]

[package.optional-dependencies]
crypto = [
    { name = "cryptography" },
]

[[package]]
name = "pynacl"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cffi" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a7/22/27582568be639dfe22ddb3902225f91f2f17ceff88ce80e4db396c8986da/PyNaCl-1.5.0.tar.gz", hash = "sha256:8ac7448f09ab85811607bdd21ec2464495ac8b7c66d146bf545b0f08fb9220ba", size = 3392854 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/75/0b8ede18506041c0bf23ac4d8e2971b4161cd6ce630b177d0a08eb0d8857/PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl", hash = "sha256:401002a4aaa07c9414132aaed7f6836ff98f59277a234704ff66878c2ee4a0d1", size = 349920 },
    { url = "https://files.pythonhosted.org/packages/59/bb/fddf10acd09637327a97ef89d2a9d621328850a72f1fdc8c08bdf72e385f/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl", hash = "sha256:52cb72a79269189d4e0dc537556f4740f7f0a9ec41c1322598799b0bdad4ef92", size = 601722 },
    { url = "https://files.pythonhosted.org/packages/5d/70/87a065c37cca41a75f2ce113a5a2c2aa7533be648b184ade58971b5f7ccc/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a36d4a9dda1f19ce6e03c9a784a2921a4b726b02e1c736600ca9c22029474394", size = 680087 },
    { url = "https://files.pythonhosted.org/packages/ee/87/f1bb6a595f14a327e8285b9eb54d41fef76c585a0edef0a45f6fc95de125/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:0c84947a22519e013607c9be43706dd42513f9e6ae5d39d3613ca1e142fba44d", size = 856678 },
    { url = "https://files.pythonhosted.org/packages/66/28/ca86676b69bf9f90e710571b67450508484388bfce09acf8a46f0b8c785f/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:06b8f6fa7f5de8d5d2f7573fe8c863c051225a27b61e6860fd047b1775807858", size = 1133660 },
    { url = "https://files.pythonhosted.org/packages/3d/85/c262db650e86812585e2bc59e497a8f59948a005325a11bbbc9ecd3fe26b/PyNaCl-1.5.0-cp36-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:a422368fc821589c228f4c49438a368831cb5bbc0eab5ebe1d7fac9dded6567b", size = 663824 },
    { url = "https://files.pythonhosted.org/packages/fd/1a/cc308a884bd299b651f1633acb978e8596c71c33ca85e9dc9fa33a5399b9/PyNaCl-1.5.0-cp36-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:61f642bf2378713e2c2e1de73444a3778e5f0a38be6fee0fe532fe30060282ff", size = 1117912 },
    { url = "https://files.pythonhosted.org/packages/25/2d/b7df6ddb0c2a33afdb358f8af6ea3b8c4d1196ca45497dd37a56f0c122be/PyNaCl-1.5.0-cp36-abi3-win32.whl", hash = "sha256:e46dae94e34b085175f8abb3b0aaa7da40767865ac82c928eeb9e57e1ea8a543", size = 204624 },
    { url = "https://files.pythonhosted.org/packages/5e/22/d3db169895faaf3e2eda892f005f433a62db2decbcfbc2f61e6517adfa87/PyNaCl-1.5.0-cp36-abi3-win_amd64.whl", hash = "sha256:20f42270d27e1b6a29f54032090b972d97f0a1b0948cc52392041ef7831fee93", size = 212141 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "spark-debugger"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "discord-py" },
    { name = "pygithub" },
    { name = "python-dotenv" },
]

[package.metadata]
requires-dist = [
    { name = "discord-py", specifier = ">=2.3.2" },
    { name = "pygithub", specifier = ">=2.6.1" },
    { name = "python-dotenv", specifier = ">=1.0.0" },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "wrapt"
version = "1.17.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c3/fc/e91cc220803d7bc4db93fb02facd8461c37364151b8494762cc88b0fbcef/wrapt-1.17.2.tar.gz", hash = "sha256:41388e9d4d1522446fe79d3213196bd9e3b301a336965b9e27ca2788ebd122f3", size = 55531 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/b9/0ffd557a92f3b11d4c5d5e0c5e4ad057bd9eb8586615cdaf901409920b14/wrapt-1.17.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:6ed6ffac43aecfe6d86ec5b74b06a5be33d5bb9243d055141e8cabb12aa08125", size = 53800 },
    { url = "https://files.pythonhosted.org/packages/c0/ef/8be90a0b7e73c32e550c73cfb2fa09db62234227ece47b0e80a05073b375/wrapt-1.17.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:35621ae4c00e056adb0009f8e86e28eb4a41a4bfa8f9bfa9fca7d343fe94f998", size = 38824 },
    { url = "https://files.pythonhosted.org/packages/36/89/0aae34c10fe524cce30fe5fc433210376bce94cf74d05b0d68344c8ba46e/wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a604bf7a053f8362d27eb9fefd2097f82600b856d5abe996d623babd067b1ab5", size = 38920 },
    { url = "https://files.pythonhosted.org/packages/3b/24/11c4510de906d77e0cfb5197f1b1445d4fec42c9a39ea853d482698ac681/wrapt-1.17.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5cbabee4f083b6b4cd282f5b817a867cf0b1028c54d445b7ec7cfe6505057cf8", size = 88690 },
    { url = "https://files.pythonhosted.org/packages/71/d7/cfcf842291267bf455b3e266c0c29dcb675b5540ee8b50ba1699abf3af45/wrapt-1.17.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:49703ce2ddc220df165bd2962f8e03b84c89fee2d65e1c24a7defff6f988f4d6", size = 80861 },
    { url = "https://files.pythonhosted.org/packages/d5/66/5d973e9f3e7370fd686fb47a9af3319418ed925c27d72ce16b791231576d/wrapt-1.17.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8112e52c5822fc4253f3901b676c55ddf288614dc7011634e2719718eaa187dc", size = 89174 },
    { url = "https://files.pythonhosted.org/packages/a7/d3/8e17bb70f6ae25dabc1aaf990f86824e4fd98ee9cadf197054e068500d27/wrapt-1.17.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9fee687dce376205d9a494e9c121e27183b2a3df18037f89d69bd7b35bcf59e2", size = 86721 },
    { url = "https://files.pythonhosted.org/packages/6f/54/f170dfb278fe1c30d0ff864513cff526d624ab8de3254b20abb9cffedc24/wrapt-1.17.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:18983c537e04d11cf027fbb60a1e8dfd5190e2b60cc27bc0808e653e7b218d1b", size = 79763 },
    { url = "https://files.pythonhosted.org/packages/4a/98/de07243751f1c4a9b15c76019250210dd3486ce098c3d80d5f729cba029c/wrapt-1.17.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:703919b1633412ab54bcf920ab388735832fdcb9f9a00ae49387f0fe67dad504", size = 87585 },
    { url = "https://files.pythonhosted.org/packages/f9/f0/13925f4bd6548013038cdeb11ee2cbd4e37c30f8bfd5db9e5a2a370d6e20/wrapt-1.17.2-cp313-cp313-win32.whl", hash = "sha256:abbb9e76177c35d4e8568e58650aa6926040d6a9f6f03435b7a522bf1c487f9a", size = 36676 },
    { url = "https://files.pythonhosted.org/packages/bf/ae/743f16ef8c2e3628df3ddfd652b7d4c555d12c84b53f3d8218498f4ade9b/wrapt-1.17.2-cp313-cp313-win_amd64.whl", hash = "sha256:69606d7bb691b50a4240ce6b22ebb319c1cfb164e5f6569835058196e0f3a845", size = 38871 },
    { url = "https://files.pythonhosted.org/packages/3d/bc/30f903f891a82d402ffb5fda27ec1d621cc97cb74c16fea0b6141f1d4e87/wrapt-1.17.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:4a721d3c943dae44f8e243b380cb645a709ba5bd35d3ad27bc2ed947e9c68192", size = 56312 },
    { url = "https://files.pythonhosted.org/packages/8a/04/c97273eb491b5f1c918857cd26f314b74fc9b29224521f5b83f872253725/wrapt-1.17.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:766d8bbefcb9e00c3ac3b000d9acc51f1b399513f44d77dfe0eb026ad7c9a19b", size = 40062 },
    { url = "https://files.pythonhosted.org/packages/4e/ca/3b7afa1eae3a9e7fefe499db9b96813f41828b9fdb016ee836c4c379dadb/wrapt-1.17.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:e496a8ce2c256da1eb98bd15803a79bee00fc351f5dfb9ea82594a3f058309e0", size = 40155 },
    { url = "https://files.pythonhosted.org/packages/89/be/7c1baed43290775cb9030c774bc53c860db140397047cc49aedaf0a15477/wrapt-1.17.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d615e4fe22f4ad3528448c193b218e077656ca9ccb22ce2cb20db730f8d306", size = 113471 },
    { url = "https://files.pythonhosted.org/packages/32/98/4ed894cf012b6d6aae5f5cc974006bdeb92f0241775addad3f8cd6ab71c8/wrapt-1.17.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a5aaeff38654462bc4b09023918b7f21790efb807f54c000a39d41d69cf552cb", size = 101208 },
    { url = "https://files.pythonhosted.org/packages/ea/fd/0c30f2301ca94e655e5e057012e83284ce8c545df7661a78d8bfca2fac7a/wrapt-1.17.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9a7d15bbd2bc99e92e39f49a04653062ee6085c0e18b3b7512a4f2fe91f2d681", size = 109339 },
    { url = "https://files.pythonhosted.org/packages/75/56/05d000de894c4cfcb84bcd6b1df6214297b8089a7bd324c21a4765e49b14/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:e3890b508a23299083e065f435a492b5435eba6e304a7114d2f919d400888cc6", size = 110232 },
    { url = "https://files.pythonhosted.org/packages/53/f8/c3f6b2cf9b9277fb0813418e1503e68414cd036b3b099c823379c9575e6d/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:8c8b293cd65ad716d13d8dd3624e42e5a19cc2a2f1acc74b30c2c13f15cb61a6", size = 100476 },
    { url = "https://files.pythonhosted.org/packages/a7/b1/0bb11e29aa5139d90b770ebbfa167267b1fc548d2302c30c8f7572851738/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:4c82b8785d98cdd9fed4cac84d765d234ed3251bd6afe34cb7ac523cb93e8b4f", size = 106377 },
    { url = "https://files.pythonhosted.org/packages/6a/e1/0122853035b40b3f333bbb25f1939fc1045e21dd518f7f0922b60c156f7c/wrapt-1.17.2-cp313-cp313t-win32.whl", hash = "sha256:13e6afb7fe71fe7485a4550a8844cc9ffbe263c0f1a1eea569bc7091d4898555", size = 37986 },
    { url = "https://files.pythonhosted.org/packages/09/5e/1655cf481e079c1f22d0cabdd4e51733679932718dc23bf2db175f329b76/wrapt-1.17.2-cp313-cp313t-win_amd64.whl", hash = "sha256:eaf675418ed6b3b31c7a989fd007fa7c3be66ce14e5c3b27336383604c9da85c", size = 40750 },
    { url = "https://files.pythonhosted.org/packages/2d/82/f56956041adef78f849db6b289b282e72b55ab8045a75abad81898c28d19/wrapt-1.17.2-py3-none-any.whl", hash = "sha256:b18f2d1533a71f069c7f82d524a52599053d4c7166e9dd374ae2136b7f40f7c8", size = 23594 },
]

[[package]]
name = "yarl"
version = "1.20.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/62/51/c0edba5219027f6eab262e139f73e2417b0f4efffa23bf562f6e18f76ca5/yarl-1.20.0.tar.gz", hash = "sha256:686d51e51ee5dfe62dec86e4866ee0e9ed66df700d55c828a615640adc885307", size = 185258 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/6f/514c9bff2900c22a4f10e06297714dbaf98707143b37ff0bcba65a956221/yarl-1.20.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2137810a20b933b1b1b7e5cf06a64c3ed3b4747b0e5d79c9447c00db0e2f752f", size = 145030 },
    { url = "https://files.pythonhosted.org/packages/4e/9d/f88da3fa319b8c9c813389bfb3463e8d777c62654c7168e580a13fadff05/yarl-1.20.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:447c5eadd750db8389804030d15f43d30435ed47af1313303ed82a62388176d3", size = 96894 },
    { url = "https://files.pythonhosted.org/packages/cd/57/92e83538580a6968b2451d6c89c5579938a7309d4785748e8ad42ddafdce/yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:42fbe577272c203528d402eec8bf4b2d14fd49ecfec92272334270b850e9cd7d", size = 94457 },
    { url = "https://files.pythonhosted.org/packages/e9/ee/7ee43bd4cf82dddd5da97fcaddb6fa541ab81f3ed564c42f146c83ae17ce/yarl-1.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:18e321617de4ab170226cd15006a565d0fa0d908f11f724a2c9142d6b2812ab0", size = 343070 },
    { url = "https://files.pythonhosted.org/packages/4a/12/b5eccd1109e2097bcc494ba7dc5de156e41cf8309fab437ebb7c2b296ce3/yarl-1.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:4345f58719825bba29895011e8e3b545e6e00257abb984f9f27fe923afca2501", size = 337739 },
    { url = "https://files.pythonhosted.org/packages/7d/6b/0eade8e49af9fc2585552f63c76fa59ef469c724cc05b29519b19aa3a6d5/yarl-1.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5d9b980d7234614bc4674468ab173ed77d678349c860c3af83b1fffb6a837ddc", size = 351338 },
    { url = "https://files.pythonhosted.org/packages/45/cb/aaaa75d30087b5183c7b8a07b4fb16ae0682dd149a1719b3a28f54061754/yarl-1.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af4baa8a445977831cbaa91a9a84cc09debb10bc8391f128da2f7bd070fc351d", size = 353636 },
    { url = "https://files.pythonhosted.org/packages/98/9d/d9cb39ec68a91ba6e66fa86d97003f58570327d6713833edf7ad6ce9dde5/yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:123393db7420e71d6ce40d24885a9e65eb1edefc7a5228db2d62bcab3386a5c0", size = 348061 },
    { url = "https://files.pythonhosted.org/packages/72/6b/103940aae893d0cc770b4c36ce80e2ed86fcb863d48ea80a752b8bda9303/yarl-1.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab47acc9332f3de1b39e9b702d9c916af7f02656b2a86a474d9db4e53ef8fd7a", size = 334150 },
    { url = "https://files.pythonhosted.org/packages/ef/b2/986bd82aa222c3e6b211a69c9081ba46484cffa9fab2a5235e8d18ca7a27/yarl-1.20.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4a34c52ed158f89876cba9c600b2c964dfc1ca52ba7b3ab6deb722d1d8be6df2", size = 362207 },
    { url = "https://files.pythonhosted.org/packages/14/7c/63f5922437b873795d9422cbe7eb2509d4b540c37ae5548a4bb68fd2c546/yarl-1.20.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:04d8cfb12714158abf2618f792c77bc5c3d8c5f37353e79509608be4f18705c9", size = 361277 },
    { url = "https://files.pythonhosted.org/packages/81/83/450938cccf732466953406570bdb42c62b5ffb0ac7ac75a1f267773ab5c8/yarl-1.20.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7dc63ad0d541c38b6ae2255aaa794434293964677d5c1ec5d0116b0e308031f5", size = 364990 },
    { url = "https://files.pythonhosted.org/packages/b4/de/af47d3a47e4a833693b9ec8e87debb20f09d9fdc9139b207b09a3e6cbd5a/yarl-1.20.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d02b591a64e4e6ca18c5e3d925f11b559c763b950184a64cf47d74d7e41877", size = 374684 },
    { url = "https://files.pythonhosted.org/packages/62/0b/078bcc2d539f1faffdc7d32cb29a2d7caa65f1a6f7e40795d8485db21851/yarl-1.20.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:95fc9876f917cac7f757df80a5dda9de59d423568460fe75d128c813b9af558e", size = 382599 },
    { url = "https://files.pythonhosted.org/packages/74/a9/4fdb1a7899f1fb47fd1371e7ba9e94bff73439ce87099d5dd26d285fffe0/yarl-1.20.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bb769ae5760cd1c6a712135ee7915f9d43f11d9ef769cb3f75a23e398a92d384", size = 378573 },
    { url = "https://files.pythonhosted.org/packages/fd/be/29f5156b7a319e4d2e5b51ce622b4dfb3aa8d8204cd2a8a339340fbfad40/yarl-1.20.0-cp313-cp313-win32.whl", hash = "sha256:70e0c580a0292c7414a1cead1e076c9786f685c1fc4757573d2967689b370e62", size = 86051 },
    { url = "https://files.pythonhosted.org/packages/52/56/05fa52c32c301da77ec0b5f63d2d9605946fe29defacb2a7ebd473c23b81/yarl-1.20.0-cp313-cp313-win_amd64.whl", hash = "sha256:4c43030e4b0af775a85be1fa0433119b1565673266a70bf87ef68a9d5ba3174c", size = 92742 },
    { url = "https://files.pythonhosted.org/packages/d4/2f/422546794196519152fc2e2f475f0e1d4d094a11995c81a465faf5673ffd/yarl-1.20.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b6c4c3d0d6a0ae9b281e492b1465c72de433b782e6b5001c8e7249e085b69051", size = 163575 },
    { url = "https://files.pythonhosted.org/packages/90/fc/67c64ddab6c0b4a169d03c637fb2d2a212b536e1989dec8e7e2c92211b7f/yarl-1.20.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8681700f4e4df891eafa4f69a439a6e7d480d64e52bf460918f58e443bd3da7d", size = 106121 },
    { url = "https://files.pythonhosted.org/packages/6d/00/29366b9eba7b6f6baed7d749f12add209b987c4cfbfa418404dbadc0f97c/yarl-1.20.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:84aeb556cb06c00652dbf87c17838eb6d92cfd317799a8092cee0e570ee11229", size = 103815 },
    { url = "https://files.pythonhosted.org/packages/28/f4/a2a4c967c8323c03689383dff73396281ced3b35d0ed140580825c826af7/yarl-1.20.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f166eafa78810ddb383e930d62e623d288fb04ec566d1b4790099ae0f31485f1", size = 408231 },
    { url = "https://files.pythonhosted.org/packages/0f/a1/66f7ffc0915877d726b70cc7a896ac30b6ac5d1d2760613603b022173635/yarl-1.20.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5d3d6d14754aefc7a458261027a562f024d4f6b8a798adb472277f675857b1eb", size = 390221 },
    { url = "https://files.pythonhosted.org/packages/41/15/cc248f0504610283271615e85bf38bc014224122498c2016d13a3a1b8426/yarl-1.20.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2a8f64df8ed5d04c51260dbae3cc82e5649834eebea9eadfd829837b8093eb00", size = 411400 },
    { url = "https://files.pythonhosted.org/packages/5c/af/f0823d7e092bfb97d24fce6c7269d67fcd1aefade97d0a8189c4452e4d5e/yarl-1.20.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d9949eaf05b4d30e93e4034a7790634bbb41b8be2d07edd26754f2e38e491de", size = 411714 },
    { url = "https://files.pythonhosted.org/packages/83/70/be418329eae64b9f1b20ecdaac75d53aef098797d4c2299d82ae6f8e4663/yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c366b254082d21cc4f08f522ac201d0d83a8b8447ab562732931d31d80eb2a5", size = 404279 },
    { url = "https://files.pythonhosted.org/packages/19/f5/52e02f0075f65b4914eb890eea1ba97e6fd91dd821cc33a623aa707b2f67/yarl-1.20.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:91bc450c80a2e9685b10e34e41aef3d44ddf99b3a498717938926d05ca493f6a", size = 384044 },
    { url = "https://files.pythonhosted.org/packages/6a/36/b0fa25226b03d3f769c68d46170b3e92b00ab3853d73127273ba22474697/yarl-1.20.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9c2aa4387de4bc3a5fe158080757748d16567119bef215bec643716b4fbf53f9", size = 416236 },
    { url = "https://files.pythonhosted.org/packages/cb/3a/54c828dd35f6831dfdd5a79e6c6b4302ae2c5feca24232a83cb75132b205/yarl-1.20.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d2cbca6760a541189cf87ee54ff891e1d9ea6406079c66341008f7ef6ab61145", size = 402034 },
    { url = "https://files.pythonhosted.org/packages/10/97/c7bf5fba488f7e049f9ad69c1b8fdfe3daa2e8916b3d321aa049e361a55a/yarl-1.20.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:798a5074e656f06b9fad1a162be5a32da45237ce19d07884d0b67a0aa9d5fdda", size = 407943 },
    { url = "https://files.pythonhosted.org/packages/fd/a4/022d2555c1e8fcff08ad7f0f43e4df3aba34f135bff04dd35d5526ce54ab/yarl-1.20.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f106e75c454288472dbe615accef8248c686958c2e7dd3b8d8ee2669770d020f", size = 423058 },
    { url = "https://files.pythonhosted.org/packages/4c/f6/0873a05563e5df29ccf35345a6ae0ac9e66588b41fdb7043a65848f03139/yarl-1.20.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:3b60a86551669c23dc5445010534d2c5d8a4e012163218fc9114e857c0586fdd", size = 423792 },
    { url = "https://files.pythonhosted.org/packages/9e/35/43fbbd082708fa42e923f314c24f8277a28483d219e049552e5007a9aaca/yarl-1.20.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:3e429857e341d5e8e15806118e0294f8073ba9c4580637e59ab7b238afca836f", size = 422242 },
    { url = "https://files.pythonhosted.org/packages/ed/f7/f0f2500cf0c469beb2050b522c7815c575811627e6d3eb9ec7550ddd0bfe/yarl-1.20.0-cp313-cp313t-win32.whl", hash = "sha256:65a4053580fe88a63e8e4056b427224cd01edfb5f951498bfefca4052f0ce0ac", size = 93816 },
    { url = "https://files.pythonhosted.org/packages/3f/93/f73b61353b2a699d489e782c3f5998b59f974ec3156a2050a52dfd7e8946/yarl-1.20.0-cp313-cp313t-win_amd64.whl", hash = "sha256:53b2da3a6ca0a541c1ae799c349788d480e5144cac47dba0266c7cb6c76151fe", size = 101093 },
    { url = "https://files.pythonhosted.org/packages/ea/1f/70c57b3d7278e94ed22d85e09685d3f0a38ebdd8c5c73b65ba4c0d0fe002/yarl-1.20.0-py3-none-any.whl", hash = "sha256:5d0fe6af927a47a230f31e6004621fd0959eaa915fc62acfafa67ff7229a3124", size = 46124 },
]


================================================
File: /sparkmeasure_collector.py
================================================
# spark_debugger/collectors/spark_measure.py
import json
import time
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np

from base_collector import BaseCollector

class SparkMeasureCollector(BaseCollector):
    """Collects metrics from Spark using the SparkMeasure library.
    
    This collector provides detailed task, stage, and executor metrics
    by leveraging SparkMeasure's instrumentation capabilities.
    """
    
    def __init__(self, mode: str = "stage"):
        """Initialize the SparkMeasure collector.
        
        Args:
            mode: Collection mode, either "stage" or "task" (default: "stage")
                 "stage" mode is more lightweight, "task" gives more detailed metrics
        """
        super().__init__(name="spark_measure")
        self.mode = mode
        self.stage_metrics = None
        self.task_metrics = None
        self.current_job_id = None
        self.metrics_dfs = {}
        
    def setup(self, context: Dict[str, Any]) -> None:
        """Set up the SparkMeasure collector with a Spark session.
        
        Args:
            context: Dictionary containing at least "spark_session"
        """
        if "spark_session" not in context:
            raise ValueError("spark_session is required in context")
            
        spark = context["spark_session"]
        
        try:
            # Import SparkMeasure and initialize collectors
            from sparkmeasure import StageMetrics, TaskMetrics
            
            if self.mode == "stage" or self.mode == "both":
                self.stage_metrics = StageMetrics(spark)
                
            if self.mode == "task" or self.mode == "both":
                self.task_metrics = TaskMetrics(spark)
                
            self.logger.info(f"SparkMeasure {self.mode} metrics collector initialized")
            
        except ImportError:
            self.logger.error("Failed to import sparkmeasure. Make sure it's installed: pip install sparkmeasure")
            raise
    
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        return [
            # Stage-level metrics
            "stage.count", "stage.duration", "stage.executorRunTime", "stage.executorCpuTime",
            "stage.executorDeserializeTime", "stage.executorDeserializeCpuTime",
            "stage.resultSerializationTime", "stage.jvmGCTime", "stage.shuffleFetchWaitTime",
            "stage.shuffleWriteTime", "stage.resultSize", "stage.diskBytesSpilled",
            "stage.memoryBytesSpilled", "stage.peakExecutionMemory", "stage.recordsRead",
            "stage.bytesRead", "stage.recordsWritten", "stage.bytesWritten",
            "stage.shuffleRecordsRead", "stage.shuffleTotalBytesRead", "stage.shuffleBytesWritten",
            "stage.shuffleRecordsWritten",
            
            # Task-level metrics
            "task.count", "task.duration", "task.gcTime", "task.executorRunTime",
            "task.executorCpuTime", "task.bytesRead", "task.recordsRead",
            "task.bytesWritten", "task.recordsWritten", "task.shuffleBytesRead",
            "task.shuffleRecordsRead", "task.shuffleBytesWritten", "task.shuffleRecordsWritten",
            "task.memoryBytesSpilled", "task.diskBytesSpilled",
            
            # Derived metrics
            "derived.dataSkew", "derived.gcPressure", "derived.shuffleIntensity",
            "derived.taskFailureRate"
        ]
    
    def begin_collection(self, job_id: Optional[str] = None) -> None:
        """Begin collecting metrics for a specific job.
        
        Args:
            job_id: Optional identifier for the job
        """
        self.current_job_id = job_id or f"job_{int(time.time())}"
        
        if self.stage_metrics:
            self.stage_metrics.begin()
            
        if self.task_metrics:
            self.task_metrics.begin()
            
        self.logger.info(f"Started metrics collection for job {self.current_job_id}")
    
    def end_collection(self) -> Dict[str, Any]:
        """End collection and return the collected metrics.
        
        Returns:
            Dictionary of collected metrics
        """
        if not self.current_job_id:
            self.logger.warning("No active collection to end")
            return {}
            
        metrics = {"job_id": self.current_job_id}
        
        if self.stage_metrics:
            self.stage_metrics.end()
            metrics["stage"] = self._process_stage_metrics()
            
        if self.task_metrics:
            self.task_metrics.end()
            metrics["task"] = self._process_task_metrics()
            
        # Add derived metrics
        metrics["derived"] = self._calculate_derived_metrics(metrics)
        
        self.logger.info(f"Ended metrics collection for job {self.current_job_id}")
        self.current_job_id = None
        
        return metrics
    
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect current metrics without begin/end cycle.
        
        This can be used for spot checks during job execution.
        
        Args:
            context: Dictionary containing collection context
            
        Returns:
            Dictionary of current metrics
        """
        if "spark_session" in context and (self.stage_metrics is None and self.task_metrics is None):
            self.setup(context)
            
        metrics = {}
        
        if self.stage_metrics:
            metrics["stage"] = self._get_current_stage_metrics()
            
        if self.task_metrics:
            metrics["task"] = self._get_current_task_metrics()
            
        # Add derived metrics
        metrics["derived"] = self._calculate_derived_metrics(metrics)
            
        return metrics
    
    def _process_stage_metrics(self) -> Dict[str, Any]:
        """Process the collected stage metrics.
        
        Returns:
            Dictionary of processed stage metrics
        """
        metrics_data = self.stage_metrics.aggregate_metrics()
        
        # Convert to a more accessible dictionary
        metrics = {}
        for key, value in metrics_data.items():
            metrics[key] = value
            
        # Create DataFrames for more detailed analysis
        try:
            stage_metrics_df = self.stage_metrics.create_stagemetrics_DF()
            self.metrics_dfs["stage"] = stage_metrics_df
            
            # Calculate statistics on stage durations
            if len(stage_metrics_df) > 0:
                metrics["duration_stats"] = {
                    "min": stage_metrics_df.select("duration").agg({"duration": "min"}).collect()[0][0],
                    "max": stage_metrics_df.select("duration").agg({"duration": "max"}).collect()[0][0],
                    "mean": stage_metrics_df.select("duration").agg({"duration": "avg"}).collect()[0][0],
                    "count": stage_metrics_df.count()
                }
                
                # Identify stages with high memory spill
                memory_spill = stage_metrics_df.select("memoryBytesSpilled", "diskBytesSpilled")
                metrics["memory_spill"] = {
                    "total_memory_spilled": memory_spill.agg({"memoryBytesSpilled": "sum"}).collect()[0][0],
                    "total_disk_spilled": memory_spill.agg({"diskBytesSpilled": "sum"}).collect()[0][0]
                }
                
                # Get the memory report
                mem_metrics = self.stage_metrics.print_memory_report()
                metrics["memory_report"] = mem_metrics
                
        except Exception as e:
            self.logger.error(f"Error processing stage metrics: {str(e)}")
            
        return metrics
    
    def _process_task_metrics(self) -> Dict[str, Any]:
        """Process the collected task metrics.
        
        Returns:
            Dictionary of processed task metrics
        """
        metrics_data = self.task_metrics.aggregate_metrics()
        
        # Convert to a more accessible dictionary
        metrics = {}
        for key, value in metrics_data.items():
            metrics[key] = value
            
        # Create DataFrames for more detailed analysis
        try:
            task_metrics_df = self.task_metrics.create_taskmetrics_DF()
            self.metrics_dfs["task"] = task_metrics_df
            
            # Calculate task duration statistics to detect skew
            if len(task_metrics_df) > 0:
                # Convert to pandas for more advanced statistics
                pdf = task_metrics_df.select("duration", "executorId", "host", "stageid").toPandas()
                
                # Calculate skew metrics
                metrics["duration_stats"] = {
                    "min": pdf["duration"].min(),
                    "max": pdf["duration"].max(),
                    "mean": pdf["duration"].mean(),
                    "median": pdf["duration"].median(),
                    "stddev": pdf["duration"].std(),
                    "count": len(pdf),
                    "skew_ratio": pdf["duration"].max() / pdf["duration"].mean() if pdf["duration"].mean() > 0 else 0
                }
                
                # Group by executor to see per-executor performance
                per_executor = pdf.groupby("executorId").agg({
                    "duration": ["sum", "mean", "count"]
                }).reset_index()
                
                # Convert to native Python types for JSON serialization
                metrics["per_executor"] = json.loads(per_executor.to_json(orient="records"))
                
                # Identify straggler tasks (those taking significantly longer than average)
                threshold = pdf["duration"].mean() + 2 * pdf["duration"].std()
                stragglers = pdf[pdf["duration"] > threshold]
                
                if len(stragglers) > 0:
                    metrics["stragglers"] = {
                        "count": len(stragglers),
                        "percentage": 100 * len(stragglers) / len(pdf),
                        "details": json.loads(stragglers.head(10).to_json(orient="records"))
                    }
                    
        except Exception as e:
            self.logger.error(f"Error processing task metrics: {str(e)}")
            
        return metrics
    
    def _get_current_stage_metrics(self) -> Dict[str, Any]:
        """Get current stage metrics without ending collection.
        
        Returns:
            Dictionary of current stage metrics
        """
        metrics = {}
        
        try:
            # Get the current metrics data
            metrics_data = self.stage_metrics.create_metrics_data()
            for key, value in metrics_data.items():
                metrics[key] = value
                
        except Exception as e:
            self.logger.error(f"Error getting current stage metrics: {str(e)}")
            
        return metrics
    
    def _get_current_task_metrics(self) -> Dict[str, Any]:
        """Get current task metrics without ending collection.
        
        Returns:
            Dictionary of current task metrics
        """
        metrics = {}
        
        try:
            # Get the current metrics data
            metrics_data = self.task_metrics.create_metrics_data()
            for key, value in metrics_data.items():
                metrics[key] = value
                
        except Exception as e:
            self.logger.error(f"Error getting current task metrics: {str(e)}")
            
        return metrics
    
    def _calculate_derived_metrics(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate derived metrics from the raw metrics.
        
        Args:
            metrics: Dictionary of raw metrics
            
        Returns:
            Dictionary of derived metrics
        """
        derived = {}
        
        try:
            # Calculate data skew metric if we have task duration stats
            if "task" in metrics and "duration_stats" in metrics["task"]:
                stats = metrics["task"]["duration_stats"]
                derived["dataSkew"] = stats["skew_ratio"]
                
                # Classify skew level
                if derived["dataSkew"] > 10:
                    derived["skewLevel"] = "severe"
                elif derived["dataSkew"] > 5:
                    derived["skewLevel"] = "high"
                elif derived["dataSkew"] > 3:
                    derived["skewLevel"] = "moderate"
                else:
                    derived["skewLevel"] = "low"
            
            # Calculate GC pressure metric
            if "stage" in metrics and "jvmGCTime" in metrics["stage"] and "executorRunTime" in metrics["stage"]:
                gc_time = metrics["stage"]["jvmGCTime"]
                run_time = metrics["stage"]["executorRunTime"]
                
                if run_time > 0:
                    derived["gcPressure"] = gc_time / run_time
                    
                    # Classify GC pressure level
                    if derived["gcPressure"] > 0.3:
                        derived["gcLevel"] = "severe"
                    elif derived["gcPressure"] > 0.1:
                        derived["gcLevel"] = "high"
                    elif derived["gcPressure"] > 0.05:
                        derived["gcLevel"] = "moderate"
                    else:
                        derived["gcLevel"] = "low"
            
            # Calculate shuffle intensity metric
            if "stage" in metrics and "shuffleTotalBytesRead" in metrics["stage"] and "bytesRead" in metrics["stage"]:
                shuffle_bytes = metrics["stage"].get("shuffleTotalBytesRead", 0)
                input_bytes = metrics["stage"].get("bytesRead", 0)
                
                total_bytes = shuffle_bytes + input_bytes
                if total_bytes > 0:
                    derived["shuffleIntensity"] = shuffle_bytes / total_bytes
                    
                    # Classify shuffle intensity level
                    if derived["shuffleIntensity"] > 0.8:
                        derived["shuffleLevel"] = "very_high"
                    elif derived["shuffleIntensity"] > 0.5:
                        derived["shuffleLevel"] = "high"
                    elif derived["shuffleIntensity"] > 0.2:
                        derived["shuffleLevel"] = "moderate"
                    else:
                        derived["shuffleLevel"] = "low"
                        
            # Add memory spill ratio if available
            if "stage" in metrics and "memory_spill" in metrics["stage"]:
                memory_spilled = metrics["stage"]["memory_spill"]["total_memory_spilled"]
                disk_spilled = metrics["stage"]["memory_spill"]["total_disk_spilled"]
                
                derived["memorySpill"] = {
                    "total": memory_spilled + disk_spilled,
                    "disk_ratio": disk_spilled / (memory_spilled + disk_spilled) if memory_spilled + disk_spilled > 0 else 0
                }
                
                # Classify memory spill level
                if derived["memorySpill"]["total"] > 1e9:  # > 1GB
                    derived["spillLevel"] = "severe"
                elif derived["memorySpill"]["total"] > 1e8:  # > 100MB
                    derived["spillLevel"] = "high"
                elif derived["memorySpill"]["total"] > 1e7:  # > 10MB
                    derived["spillLevel"] = "moderate"
                else:
                    derived["spillLevel"] = "low"
                
        except Exception as e:
            self.logger.error(f"Error calculating derived metrics: {str(e)}")
            
        return derived


================================================
File: /pyproject.toml
================================================
[project]
name = "spark-debugger"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "discord.py>=2.3.2",
    "pygithub>=2.6.1",
    "python-dotenv>=1.0.0",
]


================================================
File: /github_utils.py
================================================
from typing import List, Union, Optional
from github import Github
from github.ContentFile import ContentFile
from github.Repository import Repository
import os
from dotenv import load_dotenv

load_dotenv()

def get_github_client(token: Optional[str] = None) -> Github:
    """
    Creates and returns a GitHub client instance.
    
    Args:
        token: GitHub Personal Access Token. If None, uses GH_APP_TOKEN env var.
        
    Returns:
        An authenticated Github client instance.
    """
    token = token or os.getenv("GH_APP_TOKEN")
    if not token:
        raise ValueError("GitHub token must be provided either directly or via GH_APP_TOKEN environment variable")
    return Github(token)

def get_repo_contents(owner: str, repo: str, path: str = "", token: Optional[str] = None) -> List[ContentFile]:
    """
    Gets the contents of a directory or a specific file in a repository.

    Args:
        owner: The owner of the repository.
        repo: The name of the repository.
        path: The path to the directory or file. If empty, gets root contents.
        token: Optional GitHub Personal Access Token. If None, uses GH_APP_TOKEN env var.

    Returns:
        A list of ContentFile objects representing the items in the directory.
        For files, returns a list with a single ContentFile.
    """
    gh = get_github_client(token)
    repository = gh.get_repo(f"{owner}/{repo}")
    
    try:
        contents = repository.get_contents(path.lstrip('/') if path else '')
        # If it's a single file, wrap it in a list for consistent return type
        if isinstance(contents, ContentFile):
            return [contents]
        return contents
    except Exception as e:
        raise Exception(f"Failed to get contents for {owner}/{repo}/{path}: {str(e)}")

def get_file_contents(owner: str, repo: str, path: str, token: Optional[str] = None) -> ContentFile:
    """
    Gets the contents of a specific file in a repository.

    Args:
        owner: The owner of the repository.
        repo: The name of the repository.
        path: The path to the file. Must not be empty.
        token: Optional GitHub Personal Access Token. If None, uses GH_APP_TOKEN env var.

    Returns:
        A ContentFile object representing the file contents.
        You can access the decoded content using .decoded_content
        and other metadata like .name, .path, .sha, etc.
    """
    if not path:
        raise ValueError("Path cannot be empty for get_file_contents.")

    gh = get_github_client(token)
    repository = gh.get_repo(f"{owner}/{repo}")
    
    try:
        return repository.get_contents(path.lstrip('/'))
    except Exception as e:
        raise Exception(f"Failed to get file contents for {owner}/{repo}/{path}: {str(e)}")


def make_code_change(owner: str, repo: str, new_content: dict, branch_name: str = None, commit_message: str = None, pr_title: str = None, pr_body: str = None):
    """
    Makes a code change to a file in a repository on a Lychee-specific feature branch, and makes a PR.

    Args:
        owner: The owner of the repository.
        repo: The name of the repository.
        new_content: A dictionary containing the new content for the file.
                    The keys should be the path to the file, and the values should be the new content.
        branch_name: Optional name for the feature branch. If None, generates one based on timestamp.
        commit_message: Optional commit message. If None, uses a default message.
        pr_title: Optional PR title. If None, uses a default title.
        pr_body: Optional PR body. If None, uses a default body.

    Returns:
        The URL of the created pull request.
    """
    gh = get_github_client()
    repository = gh.get_repo(f"{owner}/{repo}")

    # Get the default branch (usually main or master)
    default_branch = repository.default_branch
    base_branch = repository.get_branch(default_branch)

    # Generate branch name if not provided
    if not branch_name:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        branch_name = f"lychee/fix_{timestamp}"

    # Create new branch from default branch
    try:
        ref = f"refs/heads/{branch_name}"
        repository.create_git_ref(ref=ref, sha=base_branch.commit.sha)
    except Exception as e:
        raise Exception(f"Failed to create branch {branch_name}: {str(e)}")

    # Make changes to files
    try:
        for file_path, content in new_content.items():
            try:
                # Try to get existing file
                file = repository.get_contents(file_path, ref=branch_name)
                repository.update_file(
                    path=file_path,
                    message=commit_message or f"Update {file_path}",
                    content=content,
                    sha=file.sha,
                    branch=branch_name
                )
            except Exception:
                # File doesn't exist, create it
                repository.create_file(
                    path=file_path,
                    message=commit_message or f"Create {file_path}",
                    content=content,
                    branch=branch_name
                )
    except Exception as e:
        # Clean up branch if file operations fail
        try:
            ref = repository.get_git_ref(f"heads/{branch_name}")
            ref.delete()
        except:
            pass
        raise Exception(f"Failed to update files: {str(e)}")

    # Create pull request
    try:
        pr = repository.create_pull(
            title=pr_title or f"Lychee: Updates from {branch_name}",
            body=pr_body or "Automated pull request created by Lychee",
            head=branch_name,
            base=default_branch
        )
        return pr.html_url
    except Exception as e:
        # Clean up branch if PR creation fails
        try:
            ref = repository.get_git_ref(f"heads/{branch_name}")
            ref.delete()
        except:
            pass
        raise Exception(f"Failed to create pull request: {str(e)}")

================================================
File: /external_metrics_collector.py
================================================
#!/usr/bin/env python3
# external_metrics_collector.py - Collector for external metrics systems

import time
import logging
import json
import re
import requests
from typing import Dict, List, Any, Optional, Union, Tuple
import boto3
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta

from base_collector import BaseCollector

class ExternalMetricsCollector(BaseCollector):
    """Collector for external metrics systems like Ganglia, CloudWatch, etc.
    
    This collector interfaces with third-party monitoring systems to collect
    additional metrics about the cluster and Spark application.
    """
    
    def __init__(self, system_type: str = "cloudwatch"):
        """Initialize the external metrics collector.
        
        Args:
            system_type: Type of external system to collect from
                         Supported: cloudwatch, ganglia, grafana, prometheus
        """
        super().__init__(name="external_metrics")
        self.system_type = system_type
        self.configs = {}
        self.clients = {}
        
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        common_metrics = [
            "external.system_type",
            "external.collection_time"
        ]
        
        # CloudWatch specific metrics
        cloudwatch_metrics = [
            "external.cloudwatch.cpu_utilization",
            "external.cloudwatch.memory_utilization",
            "external.cloudwatch.disk_utilization",
            "external.cloudwatch.network_in",
            "external.cloudwatch.network_out",
            "external.cloudwatch.emr_metrics",
            "external.cloudwatch.step_metrics"
        ]
        
        # Ganglia specific metrics
        ganglia_metrics = [
            "external.ganglia.load_one",
            "external.ganglia.load_five",
            "external.ganglia.load_fifteen",
            "external.ganglia.cpu_user",
            "external.ganglia.cpu_system",
            "external.ganglia.cpu_idle",
            "external.ganglia.mem_free",
            "external.ganglia.mem_used",
            "external.ganglia.network_in",
            "external.ganglia.network_out"
        ]
        
        # Grafana specific metrics
        grafana_metrics = [
            "external.grafana.dashboard_metrics",
            "external.grafana.panel_metrics"
        ]
        
        # Prometheus specific metrics (for completeness, though we have a separate collector for it)
        prometheus_metrics = [
            "external.prometheus.query_metrics"
        ]
        
        # Return appropriate metrics based on system type
        if self.system_type == "cloudwatch":
            return common_metrics + cloudwatch_metrics
        elif self.system_type == "ganglia":
            return common_metrics + ganglia_metrics
        elif self.system_type == "grafana":
            return common_metrics + grafana_metrics
        elif self.system_type == "prometheus":
            return common_metrics + prometheus_metrics
        else:
            return common_metrics
    
    def setup(self, context: Dict[str, Any]) -> None:
        """Set up the collector with context information.
        
        Args:
            context: Dictionary containing configuration for the external system
                     For CloudWatch: aws_region, cluster_id, etc.
                     For Ganglia: ganglia_host, ganglia_port, etc.
                     For Grafana: grafana_url, api_key, etc.
        """
        # Update system type if provided
        if "system_type" in context:
            self.system_type = context["system_type"]
            
        # Store configuration
        self.configs = context.copy()
        
        # Set up clients based on system type
        if self.system_type == "cloudwatch":
            self._setup_cloudwatch_client(context)
        elif self.system_type == "ganglia":
            self._setup_ganglia_client(context)
        elif self.system_type == "grafana":
            self._setup_grafana_client(context)
        elif self.system_type == "prometheus":
            self._setup_prometheus_client(context)
            
        self.logger.info(f"External metrics collector initialized for {self.system_type}")
    
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from the external system.
        
        Args:
            context: Dictionary containing collection context.
                     May include specific metrics to collect or time ranges.
                     
        Returns:
            Dictionary of collected metrics
        """
        # Update configuration if needed
        if "system_type" in context:
            self.system_type = context["system_type"]
            
            # Reinitialize client if system type changed
            self.setup(context)
        elif any(key in context for key in self.configs.keys()):
            # Update configs
            self.configs.update({k: v for k, v in context.items() if k in self.configs})
            
        # Collect metrics based on system type
        metrics = {
            "timestamp": time.time(),
            "system_type": self.system_type,
            "collection_time": 0
        }
        
        start_time = time.time()
        
        try:
            if self.system_type == "cloudwatch":
                metrics.update(self._collect_cloudwatch_metrics(context))
            elif self.system_type == "ganglia":
                metrics.update(self._collect_ganglia_metrics(context))
            elif self.system_type == "grafana":
                metrics.update(self._collect_grafana_metrics(context))
            elif self.system_type == "prometheus":
                metrics.update(self._collect_prometheus_metrics(context))
            else:
                metrics["error"] = f"Unsupported system type: {self.system_type}"
        except Exception as e:
            self.logger.error(f"Error collecting {self.system_type} metrics: {str(e)}")
            metrics["error"] = str(e)
            
        # Calculate collection time
        metrics["collection_time"] = time.time() - start_time
        
        return metrics
    
    def _setup_cloudwatch_client(self, context: Dict[str, Any]) -> None:
        """Set up AWS CloudWatch client.
        
        Args:
            context: Configuration for CloudWatch
                     Required: aws_region
                     Optional: aws_access_key_id, aws_secret_access_key, etc.
        """
        if "aws_region" not in context:
            raise ValueError("aws_region is required for CloudWatch")
            
        # Extract AWS credentials
        aws_region = context["aws_region"]
        aws_access_key_id = context.get("aws_access_key_id")
        aws_secret_access_key = context.get("aws_secret_access_key")
        
        # Create CloudWatch client
        session_args = {"region_name": aws_region}
        if aws_access_key_id and aws_secret_access_key:
            session_args.update({
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key
            })
            
        session = boto3.Session(**session_args)
        self.clients["cloudwatch"] = session.client('cloudwatch')
        
        # Create EMR client if collecting EMR metrics
        if context.get("collect_emr_metrics", False):
            self.clients["emr"] = session.client('emr')
    
    def _setup_ganglia_client(self, context: Dict[str, Any]) -> None:
        """Set up Ganglia client (no actual client, just configuration).
        
        Args:
            context: Configuration for Ganglia
                     Required: ganglia_host, ganglia_port
        """
        if "ganglia_host" not in context:
            raise ValueError("ganglia_host is required for Ganglia")
            
        if "ganglia_port" not in context:
            context["ganglia_port"] = 8652  # Default Ganglia XML port
    
    def _setup_grafana_client(self, context: Dict[str, Any]) -> None:
        """Set up Grafana client (no actual client, just configuration).
        
        Args:
            context: Configuration for Grafana
                     Required: grafana_url, api_key
        """
        if "grafana_url" not in context:
            raise ValueError("grafana_url is required for Grafana")
            
        if "api_key" not in context:
            raise ValueError("api_key is required for Grafana")
    
    def _setup_prometheus_client(self, context: Dict[str, Any]) -> None:
        """Set up Prometheus client (no actual client, just configuration).
        
        Args:
            context: Configuration for Prometheus
                     Required: prometheus_url
        """
        if "prometheus_url" not in context:
            raise ValueError("prometheus_url is required for Prometheus")
    
    def _collect_cloudwatch_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from AWS CloudWatch.
        
        Args:
            context: Collection context
                     Optional: start_time, end_time, metrics, etc.
                     
        Returns:
            Dictionary of CloudWatch metrics
        """
        metrics = {
            "cloudwatch": {}
        }
        
        if "cloudwatch" not in self.clients:
            metrics["cloudwatch"]["error"] = "CloudWatch client not initialized"
            return metrics
            
        # Determine the time range for metrics
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=1)  # Default: last hour
        
        if "start_time" in context:
            if isinstance(context["start_time"], datetime):
                start_time = context["start_time"]
            else:
                # Try to parse as ISO format
                try:
                    start_time = datetime.fromisoformat(context["start_time"].replace('Z', '+00:00'))
                except (ValueError, AttributeError):
                    # If parsing fails, use default
                    pass
                    
        if "end_time" in context:
            if isinstance(context["end_time"], datetime):
                end_time = context["end_time"]
            else:
                # Try to parse as ISO format
                try:
                    end_time = datetime.fromisoformat(context["end_time"].replace('Z', '+00:00'))
                except (ValueError, AttributeError):
                    # If parsing fails, use default
                    pass
        
        # Get instance metrics if instance IDs are provided
        if "instance_ids" in context:
            metrics["cloudwatch"]["instances"] = self._get_instance_metrics(
                context["instance_ids"], start_time, end_time
            )
        
        # Get EMR metrics if cluster ID is provided
        if "emr_cluster_id" in context and "emr" in self.clients:
            metrics["cloudwatch"]["emr"] = self._get_emr_metrics(
                context["emr_cluster_id"], start_time, end_time
            )
        
        # Get custom metrics
        if "custom_metrics" in context:
            metrics["cloudwatch"]["custom"] = self._get_custom_cloudwatch_metrics(
                context["custom_metrics"], start_time, end_time
            )
            
        return metrics
    
    def _get_instance_metrics(self, instance_ids: List[str], start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """Get CloudWatch metrics for EC2 instances.
        
        Args:
            instance_ids: List of EC2 instance IDs
            start_time: Start time for metrics
            end_time: End time for metrics
            
        Returns:
            Dictionary of instance metrics
        """
        metrics_result = {}
        
        # Define metrics to collect
        metrics_to_collect = [
            {"name": "CPUUtilization", "unit": "Percent", "stat": "Average"},
            {"name": "MemoryUtilization", "unit": "Percent", "stat": "Average"},
            {"name": "DiskReadBytes", "unit": "Bytes", "stat": "Sum"},
            {"name": "DiskWriteBytes", "unit": "Bytes", "stat": "Sum"},
            {"name": "NetworkIn", "unit": "Bytes", "stat": "Sum"},
            {"name": "NetworkOut", "unit": "Bytes", "stat": "Sum"}
        ]
        
        # Collect metrics for each instance
        for instance_id in instance_ids:
            instance_metrics = {}
            
            for metric in metrics_to_collect:
                try:
                    response = self.clients["cloudwatch"].get_metric_statistics(
                        Namespace="AWS/EC2",
                        MetricName=metric["name"],
                        Dimensions=[
                            {
                                "Name": "InstanceId",
                                "Value": instance_id
                            }
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5-minute periods
                        Statistics=[metric["stat"]],
                        Unit=metric["unit"]
                    )
                    
                    # Extract datapoints
                    datapoints = response.get("Datapoints", [])
                    if datapoints:
                        # Sort by timestamp
                        datapoints = sorted(datapoints, key=lambda x: x["Timestamp"])
                        
                        # Extract metric values
                        instance_metrics[metric["name"]] = {
                            "values": [point[metric["stat"]] for point in datapoints],
                            "timestamps": [point["Timestamp"].isoformat() for point in datapoints],
                            "unit": metric["unit"],
                            "stat": metric["stat"],
                            "latest": datapoints[-1][metric["stat"]] if datapoints else None,
                            "average": sum(point[metric["stat"]] for point in datapoints) / len(datapoints) if datapoints else None
                        }
                    else:
                        instance_metrics[metric["name"]] = {
                            "values": [],
                            "timestamps": [],
                            "unit": metric["unit"],
                            "stat": metric["stat"],
                            "latest": None,
                            "average": None
                        }
                        
                except Exception as e:
                    self.logger.error(f"Error getting {metric['name']} for instance {instance_id}: {str(e)}")
                    instance_metrics[metric["name"]] = {
                        "error": str(e)
                    }
            
            metrics_result[instance_id] = instance_metrics
            
        return metrics_result
    
    def _get_emr_metrics(self, cluster_id: str, start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """Get CloudWatch metrics for an EMR cluster.
        
        Args:
            cluster_id: EMR cluster ID
            start_time: Start time for metrics
            end_time: End time for metrics
            
        Returns:
            Dictionary of EMR metrics
        """
        metrics_result = {
            "cluster_info": {},
            "metrics": {}
        }
        
        try:
            # Get cluster info
            cluster_info = self.clients["emr"].describe_cluster(
                ClusterId=cluster_id
            )
            
            if "Cluster" in cluster_info:
                cluster = cluster_info["Cluster"]
                metrics_result["cluster_info"] = {
                    "name": cluster.get("Name", ""),
                    "status": cluster.get("Status", {}).get("State", ""),
                    "normalized_status": cluster.get("Status", {}).get("StateChangeReason", {}).get("Code", ""),
                    "creation_time": cluster.get("Status", {}).get("Timeline", {}).get("CreationDateTime", "").isoformat() if cluster.get("Status", {}).get("Timeline", {}).get("CreationDateTime") else None,
                    "end_time": cluster.get("Status", {}).get("Timeline", {}).get("EndDateTime", "").isoformat() if cluster.get("Status", {}).get("Timeline", {}).get("EndDateTime") else None,
                    "instance_count": cluster.get("InstanceCollectionType", ""),
                    "instance_type": cluster.get("InstanceGroups", [{}])[0].get("InstanceType", "") if cluster.get("InstanceGroups", []) else "",
                    "master_public_dns": cluster.get("MasterPublicDnsName", "")
                }
                
                # Get instance groups info
                if "InstanceGroups" in cluster:
                    metrics_result["cluster_info"]["instance_groups"] = []
                    
                    for group in cluster["InstanceGroups"]:
                        metrics_result["cluster_info"]["instance_groups"].append({
                            "id": group.get("Id", ""),
                            "name": group.get("Name", ""),
                            "type": group.get("InstanceGroupType", ""),
                            "instance_type": group.get("InstanceType", ""),
                            "requested_instances": group.get("RequestedInstanceCount", 0),
                            "running_instances": group.get("RunningInstanceCount", 0),
                            "status": group.get("Status", {}).get("State", "")
                        })
                        
            # Define metrics to collect
            metrics_to_collect = [
                {"name": "HDFSUtilization", "unit": "Percent", "stat": "Average"},
                {"name": "YARNMemoryAvailablePercentage", "unit": "Percent", "stat": "Average"},
                {"name": "AppsCompleted", "unit": "Count", "stat": "Sum"},
                {"name": "AppsFailed", "unit": "Count", "stat": "Sum"},
                {"name": "AppsRunning", "unit": "Count", "stat": "Average"},
                {"name": "AppsSubmitted", "unit": "Count", "stat": "Sum"},
                {"name": "ContainerAllocated", "unit": "Count", "stat": "Average"},
                {"name": "ContainerPending", "unit": "Count", "stat": "Average"},
                {"name": "IsIdle", "unit": "Count", "stat": "Average"}
            ]
            
            # Collect metrics
            for metric in metrics_to_collect:
                try:
                    response = self.clients["cloudwatch"].get_metric_statistics(
                        Namespace="AWS/ElasticMapReduce",
                        MetricName=metric["name"],
                        Dimensions=[
                            {
                                "Name": "JobFlowId",
                                "Value": cluster_id
                            }
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5-minute periods
                        Statistics=[metric["stat"]],
                        Unit=metric["unit"]
                    )
                    
                    # Extract datapoints
                    datapoints = response.get("Datapoints", [])
                    if datapoints:
                        # Sort by timestamp
                        datapoints = sorted(datapoints, key=lambda x: x["Timestamp"])
                        
                        # Extract metric values
                        metrics_result["metrics"][metric["name"]] = {
                            "values": [point[metric["stat"]] for point in datapoints],
                            "timestamps": [point["Timestamp"].isoformat() for point in datapoints],
                            "unit": metric["unit"],
                            "stat": metric["stat"],
                            "latest": datapoints[-1][metric["stat"]] if datapoints else None,
                            "average": sum(point[metric["stat"]] for point in datapoints) / len(datapoints) if datapoints else None
                        }
                    else:
                        metrics_result["metrics"][metric["name"]] = {
                            "values": [],
                            "timestamps": [],
                            "unit": metric["unit"],
                            "stat": metric["stat"],
                            "latest": None,
                            "average": None
                        }
                        
                except Exception as e:
                    self.logger.error(f"Error getting {metric['name']} for cluster {cluster_id}: {str(e)}")
                    metrics_result["metrics"][metric["name"]] = {
                        "error": str(e)
                    }
                    
            # Get cluster steps if available
            try:
                steps_response = self.clients["emr"].list_steps(
                    ClusterId=cluster_id
                )
                
                if "Steps" in steps_response:
                    metrics_result["steps"] = []
                    
                    for step in steps_response["Steps"]:
                        metrics_result["steps"].append({
                            "id": step.get("Id", ""),
                            "name": step.get("Name", ""),
                            "status": step.get("Status", {}).get("State", ""),
                            "created_time": step.get("Status", {}).get("Timeline", {}).get("CreationDateTime", "").isoformat() if step.get("Status", {}).get("Timeline", {}).get("CreationDateTime") else None,
                            "start_time": step.get("Status", {}).get("Timeline", {}).get("StartDateTime", "").isoformat() if step.get("Status", {}).get("Timeline", {}).get("StartDateTime") else None,
                            "end_time": step.get("Status", {}).get("Timeline", {}).get("EndDateTime", "").isoformat() if step.get("Status", {}).get("Timeline", {}).get("EndDateTime") else None,
                            "action_on_failure": step.get("ActionOnFailure", "")
                        })
                        
            except Exception as e:
                self.logger.error(f"Error getting steps for cluster {cluster_id}: {str(e)}")
                metrics_result["steps_error"] = str(e)
                
        except Exception as e:
            self.logger.error(f"Error getting EMR metrics for cluster {cluster_id}: {str(e)}")
            metrics_result["error"] = str(e)
            
        return metrics_result
    
    def _get_custom_cloudwatch_metrics(self, metric_configs: List[Dict[str, Any]], start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """Get custom CloudWatch metrics.
        
        Args:
            metric_configs: List of custom metric configurations
                           Each config should have: namespace, name, dimensions, unit, stat
            start_time: Start time for metrics
            end_time: End time for metrics
            
        Returns:
            Dictionary of custom metrics
        """
        metrics_result = {}
        
        for config in metric_configs:
            if not all(k in config for k in ["namespace", "name"]):
                continue
                
            metric_name = config["name"]
            namespace = config["namespace"]
            dimensions = config.get("dimensions", [])
            unit = config.get("unit", "None")
            stat = config.get("stat", "Average")
            
            try:
                response = self.clients["cloudwatch"].get_metric_statistics(
                    Namespace=namespace,
                    MetricName=metric_name,
                    Dimensions=dimensions,
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=300,  # 5-minute periods
                    Statistics=[stat],
                    Unit=unit
                )
                
                # Extract datapoints
                datapoints = response.get("Datapoints", [])
                if datapoints:
                    # Sort by timestamp
                    datapoints = sorted(datapoints, key=lambda x: x["Timestamp"])
                    
                    # Extract metric values
                    metrics_result[metric_name] = {
                        "values": [point[stat] for point in datapoints],
                        "timestamps": [point["Timestamp"].isoformat() for point in datapoints],
                        "unit": unit,
                        "stat": stat,
                        "namespace": namespace,
                        "dimensions": dimensions,
                        "latest": datapoints[-1][stat] if datapoints else None,
                        "average": sum(point[stat] for point in datapoints) / len(datapoints) if datapoints else None
                    }
                else:
                    metrics_result[metric_name] = {
                        "values": [],
                        "timestamps": [],
                        "unit": unit,
                        "stat": stat,
                        "namespace": namespace,
                        "dimensions": dimensions,
                        "latest": None,
                        "average": None
                    }
                    
            except Exception as e:
                self.logger.error(f"Error getting custom metric {metric_name}: {str(e)}")
                metrics_result[metric_name] = {
                    "error": str(e)
                }
                
        return metrics_result
    
    def _collect_ganglia_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from Ganglia.
        
        Args:
            context: Collection context
                     Optional: metrics, cluster_name, etc.
                     
        Returns:
            Dictionary of Ganglia metrics
        """
        metrics = {
            "ganglia": {}
        }
        
        if "ganglia_host" not in self.configs:
            metrics["ganglia"]["error"] = "Ganglia host not configured"
            return metrics
            
        ganglia_host = self.configs["ganglia_host"]
        ganglia_port = self.configs.get("ganglia_port", 8652)
        
        # Construct Ganglia XML URL
        ganglia_url = f"http://{ganglia_host}:{ganglia_port}/ganglia/xml"
        
        try:
            # Fetch Ganglia XML
            response = requests.get(ganglia_url, timeout=10)
            
            if response.status_code != 200:
                metrics["ganglia"]["error"] = f"Failed to fetch Ganglia XML: {response.status_code} {response.text}"
                return metrics
                
            # Parse XML
            root = ET.fromstring(response.text)
            
            # Extract cluster info
            clusters = {}
            for cluster in root.findall("./CLUSTER"):
                cluster_name = cluster.get("NAME", "unknown")
                clusters[cluster_name] = {
                    "name": cluster_name,
                    "localtime": cluster.get("LOCALTIME", ""),
                    "owner": cluster.get("OWNER", ""),
                    "hosts": {}
                }
                
                # Extract host info
                for host in cluster.findall("./HOST"):
                    host_name = host.get("NAME", "unknown")
                    host_ip = host.get("IP", "")
                    
                    clusters[cluster_name]["hosts"][host_name] = {
                        "name": host_name,
                        "ip": host_ip,
                        "reported": host.get("REPORTED", ""),
                        "metrics": {}
                    }
                    
                    # Extract metrics for this host
                    for metric in host.findall("./METRIC"):
                        metric_name = metric.get("NAME", "unknown")
                        
                        clusters[cluster_name]["hosts"][host_name]["metrics"][metric_name] = {
                            "name": metric_name,
                            "value": metric.get("VAL", ""),
                            "type": metric.get("TYPE", ""),
                            "units": metric.get("UNITS", ""),
                            "slope": metric.get("SLOPE", ""),
                            "tn": int(metric.get("TN", 0)),
                            "tmax": int(metric.get("TMAX", 0)),
                            "dmax": int(metric.get("DMAX", 0))
                        }
            
            # Filter by specific cluster if provided
            cluster_filter = context.get("cluster_name")
            if cluster_filter and cluster_filter in clusters:
                metrics["ganglia"]["clusters"] = {cluster_filter: clusters[cluster_filter]}
            else:
                metrics["ganglia"]["clusters"] = clusters
                
            # Extract Spark-related metrics if available
            spark_metrics = self._extract_spark_metrics_from_ganglia(clusters)
            if spark_metrics:
                metrics["ganglia"]["spark"] = spark_metrics
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error fetching Ganglia metrics: {str(e)}")
            metrics["ganglia"]["error"] = f"Failed to connect to Ganglia: {str(e)}"
        except ET.ParseError as e:
            self.logger.error(f"Error parsing Ganglia XML: {str(e)}")
            metrics["ganglia"]["error"] = f"Failed to parse Ganglia XML: {str(e)}"
        except Exception as e:
            self.logger.error(f"Error processing Ganglia data: {str(e)}")
            metrics["ganglia"]["error"] = str(e)
            
        return metrics
    
    def _extract_spark_metrics_from_ganglia(self, clusters: Dict[str, Any]) -> Dict[str, Any]:
        """Extract Spark-related metrics from Ganglia data.
        
        Args:
            clusters: Dictionary of Ganglia clusters and hosts
            
        Returns:
            Dictionary of Spark metrics
        """
        spark_metrics = {}
        
        # Check for Spark JVM metrics
        spark_metric_patterns = [
            r"^spark\..*",
            r".*\.driver\..*",
            r".*\.executor\..*",
            r"^jvm\..*"
        ]
        
        for cluster_name, cluster in clusters.items():
            for host_name, host in cluster.get("hosts", {}).items():
                for metric_name, metric in host.get("metrics", {}).items():
                    # Check if metric is related to Spark
                    is_spark_metric = any(re.match(pattern, metric_name) for pattern in spark_metric_patterns)
                    
                    if is_spark_metric:
                        # Organize by host and metric name
                        if host_name not in spark_metrics:
                            spark_metrics[host_name] = {}
                            
                        spark_metrics[host_name][metric_name] = metric
                        
        return spark_metrics
    
    def _collect_grafana_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from Grafana.
        
        Args:
            context: Collection context
                     Optional: dashboard_uid, panel_id, etc.
                     
        Returns:
            Dictionary of Grafana metrics
        """
        metrics = {
            "grafana": {}
        }
        
        if "grafana_url" not in self.configs or "api_key" not in self.configs:
            metrics["grafana"]["error"] = "Grafana URL or API key not configured"
            return metrics
            
        grafana_url = self.configs["grafana_url"]
        api_key = self.configs["api_key"]
        
        # Set up headers for Grafana API
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            # Get dashboard info if dashboard_uid is provided
            if "dashboard_uid" in context:
                dashboard_uid = context["dashboard_uid"]
                dashboard_url = f"{grafana_url}/api/dashboards/uid/{dashboard_uid}"
                
                dashboard_response = requests.get(dashboard_url, headers=headers)
                
                if dashboard_response.status_code == 200:
                    dashboard_data = dashboard_response.json()
                    
                    if "dashboard" in dashboard_data:
                        metrics["grafana"]["dashboard"] = {
                            "uid": dashboard_data["dashboard"].get("uid", ""),
                            "title": dashboard_data["dashboard"].get("title", ""),
                            "description": dashboard_data["dashboard"].get("description", ""),
                            "panels": []
                        }
                        
                        # Extract panel info
                        for panel in dashboard_data["dashboard"].get("panels", []):
                            panel_info = {
                                "id": panel.get("id", ""),
                                "title": panel.get("title", ""),
                                "type": panel.get("type", ""),
                                "description": panel.get("description", "")
                            }
                            
                            # Extract targets (queries)
                            if "targets" in panel:
                                panel_info["targets"] = []
                                
                                for target in panel["targets"]:
                                    panel_info["targets"].append({
                                        "refId": target.get("refId", ""),
                                        "expr": target.get("expr", ""),
                                        "datasource": target.get("datasource", {}).get("uid", "") if isinstance(target.get("datasource"), dict) else target.get("datasource", "")
                                    })
                                    
                            metrics["grafana"]["dashboard"]["panels"].append(panel_info)
                            
                    else:
                        metrics["grafana"]["dashboard"] = {"error": "Dashboard data not found in response"}
                else:
                    metrics["grafana"]["dashboard"] = {
                        "error": f"Failed to fetch dashboard: {dashboard_response.status_code} {dashboard_response.text}"
                    }
                    
            # Get panel data if panel_id is provided
            if "dashboard_uid" in context and "panel_id" in context:
                dashboard_uid = context["dashboard_uid"]
                panel_id = context["panel_id"]
                
                # Get time range
                from_time = context.get("from_time", "now-1h")
                to_time = context.get("to_time", "now")
                
                panel_url = f"{grafana_url}/api/dashboards/uid/{dashboard_uid}/panels/{panel_id}/query"
                panel_data = {
                    "from": from_time,
                    "to": to_time,
                    "queries": [
                        {
                            "refId": "A",
                            "datasourceId": 1,
                            "datasourceName": "Prometheus",
                            "expr": context.get("query", ""),
                            "intervalMs": 1000
                        }
                    ]
                }
                
                panel_response = requests.post(panel_url, headers=headers, json=panel_data)
                
                if panel_response.status_code == 200:
                    panel_result = panel_response.json()
                    metrics["grafana"]["panel_data"] = panel_result
                else:
                    metrics["grafana"]["panel_data"] = {
                        "error": f"Failed to fetch panel data: {panel_response.status_code} {panel_response.text}"
                    }
                    
            # Get datasources
            datasources_url = f"{grafana_url}/api/datasources"
            datasources_response = requests.get(datasources_url, headers=headers)
            
            if datasources_response.status_code == 200:
                metrics["grafana"]["datasources"] = datasources_response.json()
            else:
                metrics["grafana"]["datasources"] = {
                    "error": f"Failed to fetch datasources: {datasources_response.status_code} {datasources_response.text}"
                }
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error fetching Grafana metrics: {str(e)}")
            metrics["grafana"]["error"] = f"Failed to connect to Grafana: {str(e)}"
        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing Grafana response: {str(e)}")
            metrics["grafana"]["error"] = f"Failed to parse Grafana response: {str(e)}"
        except Exception as e:
            self.logger.error(f"Error processing Grafana data: {str(e)}")
            metrics["grafana"]["error"] = str(e)
            
        return metrics
    
    def _collect_prometheus_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from Prometheus.
        
        Args:
            context: Collection context
                     Required: queries (list of Prometheus queries)
                     
        Returns:
            Dictionary of Prometheus metrics
        """
        metrics = {
            "prometheus": {}
        }
        
        if "prometheus_url" not in self.configs:
            metrics["prometheus"]["error"] = "Prometheus URL not configured"
            return metrics
            
        prometheus_url = self.configs["prometheus_url"]
        
        # Get queries from context
        queries = context.get("queries", [])
        if not queries:
            metrics["prometheus"]["error"] = "No queries provided"
            return metrics
            
        # Get time range
        end_time = int(time.time())
        start_time = end_time - 3600  # Default: last hour
        step = context.get("step", "15s")
        
        if "start_time" in context:
            if isinstance(context["start_time"], (int, float)):
                start_time = int(context["start_time"])
            elif isinstance(context["start_time"], str):
                try:
                    # Try to parse as ISO format
                    start_time = int(datetime.fromisoformat(context["start_time"].replace('Z', '+00:00')).timestamp())
                except ValueError:
                    # If parsing fails, use default
                    pass
                    
        if "end_time" in context:
            if isinstance(context["end_time"], (int, float)):
                end_time = int(context["end_time"])
            elif isinstance(context["end_time"], str):
                try:
                    # Try to parse as ISO format
                    end_time = int(datetime.fromisoformat(context["end_time"].replace('Z', '+00:00')).timestamp())
                except ValueError:
                    # If parsing fails, use default
                    pass
                    
        # Execute queries
        try:
            metrics["prometheus"]["results"] = {}
            
            for query in queries:
                query_name = query.get("name", "")
                query_expr = query.get("expr", "")
                
                if not query_expr:
                    continue
                    
                # Execute query
                query_url = f"{prometheus_url}/api/v1/query_range"
                params = {
                    "query": query_expr,
                    "start": start_time,
                    "end": end_time,
                    "step": step
                }
                
                response = requests.get(query_url, params=params)
                
                if response.status_code == 200:
                    query_result = response.json()
                    
                    if query_result["status"] == "success":
                        metrics["prometheus"]["results"][query_name or query_expr] = query_result["data"]
                    else:
                        metrics["prometheus"]["results"][query_name or query_expr] = {
                            "error": query_result.get("error", "Unknown error")
                        }
                else:
                    metrics["prometheus"]["results"][query_name or query_expr] = {
                        "error": f"Failed to execute query: {response.status_code} {response.text}"
                    }
                    
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error fetching Prometheus metrics: {str(e)}")
            metrics["prometheus"]["error"] = f"Failed to connect to Prometheus: {str(e)}"
        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing Prometheus response: {str(e)}")
            metrics["prometheus"]["error"] = f"Failed to parse Prometheus response: {str(e)}"
        except Exception as e:
            self.logger.error(f"Error processing Prometheus data: {str(e)}")
            metrics["prometheus"]["error"] = str(e)
            
        return metrics

================================================
File: /base_collector.py
================================================
from abc import ABC, abstractmethod
import time
import logging
from typing import Dict, Any, Optional, List

class BaseCollector(ABC):
    """Abstract base class for all metric collectors in the Spark Pipeline Debugger."""
    
    def __init__(self, name: str):
        """Initialize the collector with a name.
        
        Args:
            name: Unique name for this collector
        """
        self.name = name
        self.logger = logging.getLogger(f"spark_debugger.collectors.{name}")
        self._metrics_cache = {}
        self._last_collection_time = 0
    
    @abstractmethod
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from the source.
        
        Args:
            context: Dictionary containing relevant context for collection
                     (e.g., spark_session, application_id, etc.)
                     
        Returns:
            Dictionary of collected metrics
        """
        pass
    
    @abstractmethod
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        pass
    
    def collect_with_cache(self, context: Dict[str, Any], max_cache_age_seconds: int = 5) -> Dict[str, Any]:
        """Collect metrics with caching to avoid repeated calls in a short time period.
        
        Args:
            context: Collection context
            max_cache_age_seconds: Maximum age of cached metrics in seconds
            
        Returns:
            Dictionary of collected metrics (from cache or fresh collection)
        """
        current_time = time.time()
        if current_time - self._last_collection_time > max_cache_age_seconds:
            try:
                self._metrics_cache = self.collect(context)
                self._last_collection_time = current_time
                self.logger.debug(f"Collected fresh metrics for {self.name}")
            except Exception as e:
                self.logger.error(f"Error collecting metrics for {self.name}: {str(e)}")
                # If we have no cached metrics, raise the exception
                if not self._metrics_cache:
                    raise
        else:
            self.logger.debug(f"Using cached metrics for {self.name} (age: {current_time - self._last_collection_time:.2f}s)")
            
        return self._metrics_cache
    
    def get_metric(self, metric_name: str, context: Dict[str, Any] = None) -> Optional[Any]:
        """Get a specific metric value.
        
        Args:
            metric_name: Name of the metric to retrieve
            context: Optional context for collection if not using cache
            
        Returns:
            The metric value or None if not available
        """
        metrics = self._metrics_cache
        if context is not None:
            metrics = self.collect_with_cache(context)
            
        # Handle nested metric names with dot notation (e.g., "memory.heap.used")
        parts = metric_name.split('.')
        value = metrics
        for part in parts:
            if isinstance(value, dict) and part in value:
                value = value[part]
            else:
                return None
                
        return value


================================================
File: /analyze_failure.py
================================================
from github_utils import get_repo_contents, get_file_contents
from google.generativeai import GenerativeAI
from dotenv import load_dotenv

load_dotenv()

genai = GenerativeAI()

def sys_prompt(logs, codebase_structure):

    return f"""You are a helpful assistant that analyzes logs from failed Apache Spark jobs and provides a report on the failure.

    Your final message should be in the following format:
    {Report.format()}

    You should analyze the following logs:
    {logs}

    Additionally, you have access to the Spark codebase and the data that was ingested to the pipeline via tool calls. Here's the codebase structure:
    {codebase_structure}
    You may spend some time thinking, but your final message should be in the report format above.
    """

# Logs structure is tbd, ingestion_data is tbd
def analyze_failure(logs, github_repo_owner, github_repo_url, discord_channel_id, ingestion_data) -> Report:

    codebase_structure = get_repo_contents(github_repo_owner, github_repo_url)

    sys_prompt = sys_prompt(logs, codebase_structure)

    # Call Gemini API with system prompt
    response = genai.generate_text(
        model="gemini-2.5-pro",
        prompt=sys_prompt,
        temperature=0.3,
        max_output_tokens=2048
    )
    final_message = response.text

    report = Report(final_message)

    return report



class Report:
    def __init__(self, final_message: str):
        self.final_message = final_message
        self.logs, self.code, self.ingestion_data, self.hypothesis, self.suggested_fix = self.parse_final_message(final_message)
    
    def parse_final_message(self, final_message: str):
        pass

    def format():
        return f"""
        """

================================================
File: /httpendpoints_collector.py
================================================
# spark_debugger/collectors/http_endpoints.py
import json
import time
import urllib.parse
import requests
from typing import Dict, Any, List, Optional, Union

from base_collector import BaseCollector

class SparkHttpCollector(BaseCollector):
    """Collects metrics from Spark UI HTTP endpoints.
    
    This collector provides information about applications, jobs, stages, and
    executors by querying the Spark Web UI's REST API endpoints.
    """
    
    def __init__(self, base_url: Optional[str] = None):
        """Initialize the HTTP endpoints collector.
        
        Args:
            base_url: Base URL for Spark UI (e.g., http://localhost:4040)
                     If None, will attempt to auto-discover
        """
        super().__init__(name="spark_http")
        self.base_url = base_url
        self.app_id = None
        self.timeout = 10  # seconds for HTTP requests
        
    def setup(self, context: Dict[str, Any]) -> None:
        """Set up the collector with context information.
        
        Args:
            context: Dictionary that may contain:
                    - spark_session: Active Spark session
                    - base_url: Override for Spark UI URL
                    - app_id: Specific application ID to monitor
                    - timeout: Timeout for HTTP requests
        """
        # Override base_url if provided
        if "base_url" in context:
            self.base_url = context["base_url"]
            
        # Try to get the application ID
        if "app_id" in context:
            self.app_id = context["app_id"]
        elif "spark_session" in context:
            spark = context["spark_session"]
            self.app_id = spark.sparkContext.applicationId
            
        # Set timeout if provided
        if "timeout" in context:
            self.timeout = context["timeout"]
            
        # If we don't have a base_url yet, try to discover it
        if self.base_url is None:
            if "spark_session" in context:
                spark = context["spark_session"]
                ui_web_url = spark.sparkContext.uiWebUrl
                if ui_web_url:
                    self.base_url = ui_web_url
                    self.logger.info(f"Discovered Spark UI URL: {self.base_url}")
            
            # Fallback to default
            if self.base_url is None:
                self.base_url = "http://localhost:4040"
                self.logger.warning(f"Using default Spark UI URL: {self.base_url}")
                
        self.logger.info(f"HTTP Endpoints collector initialized with base URL: {self.base_url}")
    
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        return [
            # Application metrics
            "app.id", "app.name", "app.status", "app.startTime", "app.endTime", 
            "app.duration", "app.user",
            
            # Job metrics
            "jobs.count", "jobs.active", "jobs.completed", "jobs.failed",
            "jobs.details", "jobs.duration",
            
            # Stage metrics
            "stages.count", "stages.active", "stages.completed", "stages.failed",
            "stages.skipped", "stages.details",
            
            # Executor metrics
            "executors.count", "executors.active", "executors.dead",
            "executors.metrics", "executors.memory", "executors.disk",
            
            # Storage metrics
            "storage.memoryUsed", "storage.diskUsed", "storage.totalCached",
            
            # Environment
            "environment.spark", "environment.jvm", "environment.system"
        ]
    
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect metrics from Spark UI HTTP endpoints.
        
        Args:
            context: Dictionary containing collection context
                     May include app_id, base_url to override defaults
                     
        Returns:
            Dictionary of collected metrics
        """
        # Update setup if context contains relevant info
        if any(key in context for key in ["base_url", "app_id", "spark_session", "timeout"]):
            self.setup(context)
            
        metrics = {
            "timestamp": time.time(),
            "base_url": self.base_url
        }
        
        # First, get application information
        app_info = self._get_application_info()
        if app_info:
            metrics["app"] = app_info
            
            # Get application ID if we don't have it yet
            if self.app_id is None and "id" in app_info:
                self.app_id = app_info["id"]
        
        if self.app_id:
            # Get jobs information
            metrics["jobs"] = self._get_jobs_info()
            
            # Get stages information
            metrics["stages"] = self._get_stages_info()
            
            # Get executors information
            metrics["executors"] = self._get_executors_info()
            
            # Get storage information
            metrics["storage"] = self._get_storage_info()
            
            # Get environment information
            metrics["environment"] = self._get_environment_info()
            
        return metrics
    
    def get_app_id(self, force_refresh: bool = False) -> Optional[str]:
        """Get the application ID from the Spark UI.
        
        Args:
            force_refresh: If True, query the API even if we already have an ID
            
        Returns:
            Application ID or None if not found
        """
        if self.app_id is not None and not force_refresh:
            return self.app_id
            
        try:
            response = self._make_request("/api/v1/applications")
            if response and isinstance(response, list) and len(response) > 0:
                self.app_id = response[0].get("id")
                return self.app_id
        except Exception as e:
            self.logger.error(f"Error getting application ID: {str(e)}")
            
        return None
    
    def _get_application_info(self) -> Dict[str, Any]:
        """Get information about the Spark application.
        
        Returns:
            Dictionary with application details
        """
        app_info = {}
        
        # First, get the list of applications
        try:
            response = self._make_request("/api/v1/applications")
            
            if response and isinstance(response, list):
                if len(response) == 0:
                    self.logger.warning("No Spark applications found")
                    return app_info
                    
                # If we have a specific app_id, find that application
                app = None
                if self.app_id:
                    for a in response:
                        if a.get("id") == self.app_id:
                            app = a
                            break
                
                # If no specific app found, use the first one
                if app is None:
                    app = response[0]
                    
                # Extract basic info
                app_info = {
                    "id": app.get("id"),
                    "name": app.get("name"),
                    "attempts": app.get("attempts", [])
                }
                
                # If we have an ID, get more detailed info
                if "id" in app_info:
                    self.app_id = app_info["id"]
                    app_detail = self._make_request(f"/api/v1/applications/{self.app_id}")
                    
                    if app_detail:
                        # Update with more detailed information
                        app_info.update({
                            "startTime": app_detail.get("startTime"),
                            "endTime": app_detail.get("endTime"),
                            "duration": app_detail.get("duration"),
                            "sparkUser": app_detail.get("sparkUser"),
                            "completed": app_detail.get("completed", False),
                            "lastUpdated": app_detail.get("lastUpdated")
                        })
                        
        except Exception as e:
            self.logger.error(f"Error getting application info: {str(e)}")
            
        return app_info
    
    def _get_jobs_info(self) -> Dict[str, Any]:
        """Get information about Spark jobs.
        
        Returns:
            Dictionary with jobs information
        """
        jobs_info = {
            "count": 0,
            "active": 0,
            "completed": 0,
            "failed": 0,
            "details": []
        }
        
        if not self.app_id:
            self.logger.warning("No application ID available")
            return jobs_info
            
        try:
            response = self._make_request(f"/api/v1/applications/{self.app_id}/jobs")
            
            if response and isinstance(response, list):
                jobs_info["count"] = len(response)
                
                # Count different job statuses
                for job in response:
                    status = job.get("status")
                    if status == "RUNNING":
                        jobs_info["active"] += 1
                    elif status == "SUCCEEDED":
                        jobs_info["completed"] += 1
                    elif status == "FAILED":
                        jobs_info["failed"] += 1
                
                # Add detailed job information (limited to latest 10 jobs)
                latest_jobs = sorted(
                    response, 
                    key=lambda j: j.get("submissionTime", ""), 
                    reverse=True
                )[:10]
                
                jobs_info["details"] = []
                for job in latest_jobs:
                    jobs_info["details"].append({
                        "jobId": job.get("jobId"),
                        "name": job.get("name"),
                        "status": job.get("status"),
                        "stageIds": job.get("stageIds", []),
                        "submissionTime": job.get("submissionTime"),
                        "completionTime": job.get("completionTime"),
                        "numTasks": job.get("numTasks"),
                        "numActiveTasks": job.get("numActiveTasks"),
                        "numCompletedTasks": job.get("numCompletedTasks"),
                        "numSkippedTasks": job.get("numSkippedTasks"),
                        "numFailedTasks": job.get("numFailedTasks"),
                        "numKilledTasks": job.get("numKilledTasks")
                    })
                    
        except Exception as e:
            self.logger.error(f"Error getting jobs info: {str(e)}")
            
        return jobs_info
    
    def _get_stages_info(self) -> Dict[str, Any]:
        """Get information about Spark stages.
        
        Returns:
            Dictionary with stages information
        """
        stages_info = {
            "count": 0,
            "active": 0,
            "completed": 0,
            "failed": 0,
            "skipped": 0,
            "details": []
        }
        
        if not self.app_id:
            self.logger.warning("No application ID available")
            return stages_info
            
        try:
            response = self._make_request(f"/api/v1/applications/{self.app_id}/stages")
            
            if response and isinstance(response, list):
                stages_info["count"] = len(response)
                
                # Count different stage statuses
                for stage in response:
                    status = stage.get("status")
                    if status == "ACTIVE":
                        stages_info["active"] += 1
                    elif status == "COMPLETE":
                        stages_info["completed"] += 1
                    elif status == "FAILED":
                        stages_info["failed"] += 1
                    elif status == "SKIPPED":
                        stages_info["skipped"] += 1
                
                # Add detailed stage information for problematic stages
                failed_stages = [s for s in response if s.get("status") == "FAILED"]
                active_stages = [s for s in response if s.get("status") == "ACTIVE"]
                
                # Combine failed and active stages, limited to 10
                important_stages = (failed_stages + active_stages)[:10]
                
                stages_info["details"] = []
                for stage in important_stages:
                    # Get more detailed stage info
                    stage_id = stage.get("stageId")
                    attempt_id = stage.get("attemptId")
                    
                    stage_detail = self._make_request(
                        f"/api/v1/applications/{self.app_id}/stages/{stage_id}/{attempt_id}"
                    )
                    
                    if stage_detail:
                        stages_info["details"].append({
                            "stageId": stage_id,
                            "attemptId": attempt_id,
                            "name": stage.get("name"),
                            "status": stage.get("status"),
                            "numTasks": stage.get("numTasks"),
                            "numActiveTasks": stage.get("numActiveTasks"),
                            "numCompleteTasks": stage.get("numCompleteTasks"),
                            "numFailedTasks": stage.get("numFailedTasks"),
                            "executorRunTime": stage.get("executorRunTime"),
                            "inputBytes": stage.get("inputBytes"),
                            "outputBytes": stage.get("outputBytes"),
                            "shuffleReadBytes": stage.get("shuffleReadBytes"),
                            "shuffleWriteBytes": stage.get("shuffleWriteBytes"),
                            "memoryBytesSpilled": stage.get("memoryBytesSpilled"),
                            "diskBytesSpilled": stage.get("diskBytesSpilled"),
                            "failureReason": stage_detail.get("failureReason")
                        })
                    
        except Exception as e:
            self.logger.error(f"Error getting stages info: {str(e)}")
            
        return stages_info
    
    def _get_executors_info(self) -> Dict[str, Any]:
        """Get information about Spark executors.
        
        Returns:
            Dictionary with executors information
        """
        executors_info = {
            "count": 0,
            "active": 0,
            "dead": 0,
            "metrics": {},
            "details": []
        }
        
        if not self.app_id:
            self.logger.warning("No application ID available")
            return executors_info
            
        try:
            response = self._make_request(f"/api/v1/applications/{self.app_id}/executors")
            
            if response and isinstance(response, list):
                executors_info["count"] = len(response)
                
                # Count active vs. dead executors
                for executor in response:
                    if executor.get("isActive", True):
                        executors_info["active"] += 1
                    else:
                        executors_info["dead"] += 1
                
                # Calculate aggregate metrics
                if executors_info["count"] > 0:
                    total_memory = sum(e.get("maxMemory", 0) for e in response)
                    used_memory = sum(e.get("memoryUsed", 0) for e in response)
                    total_disk = sum(e.get("totalDiskUsed", 0) for e in response if "totalDiskUsed" in e)
                    total_cores = sum(e.get("totalCores", 0) for e in response)
                    
                    executors_info["metrics"] = {
                        "totalMemory": total_memory,
                        "usedMemory": used_memory,
                        "memoryUtilization": used_memory / total_memory if total_memory > 0 else 0,
                        "totalDisk": total_disk,
                        "totalCores": total_cores,
                        "totalTasks": sum(e.get("totalTasks", 0) for e in response),
                        "activeTasks": sum(e.get("activeTasks", 0) for e in response),
                        "failedTasks": sum(e.get("failedTasks", 0) for e in response),
                        "completedTasks": sum(e.get("completedTasks", 0) for e in response),
                        "totalDuration": sum(e.get("totalDuration", 0) for e in response),
                        "totalGCTime": sum(e.get("totalGCTime", 0) for e in response),
                        "totalInputBytes": sum(e.get("totalInputBytes", 0) for e in response),
                        "totalShuffleRead": sum(e.get("totalShuffleRead", 0) for e in response),
                        "totalShuffleWrite": sum(e.get("totalShuffleWrite", 0) for e in response)
                    }
                    
                    # Calculate GC pressure
                    total_duration = executors_info["metrics"]["totalDuration"]
                    total_gc_time = executors_info["metrics"]["totalGCTime"]
                    if total_duration > 0:
                        executors_info["metrics"]["gcPressure"] = total_gc_time / total_duration
                    
                # Add detailed executor information
                executors_info["details"] = []
                for executor in response:
                    executors_info["details"].append({
                        "id": executor.get("id"),
                        "hostPort": executor.get("hostPort"),
                        "isActive": executor.get("isActive", True),
                        "rddBlocks": executor.get("rddBlocks"),
                        "memoryUsed": executor.get("memoryUsed"),
                        "diskUsed": executor.get("diskUsed"),
                        "totalCores": executor.get("totalCores"),
                        "maxTasks": executor.get("maxTasks"),
                        "activeTasks": executor.get("activeTasks"),
                        "failedTasks": executor.get("failedTasks"),
                        "completedTasks": executor.get("completedTasks"),
                        "totalTasks": executor.get("totalTasks"),
                        "totalDuration": executor.get("totalDuration"),
                        "totalGCTime": executor.get("totalGCTime"),
                        "totalInputBytes": executor.get("totalInputBytes"),
                        "totalShuffleRead": executor.get("totalShuffleRead"),
                        "totalShuffleWrite": executor.get("totalShuffleWrite")
                    })
                    
        except Exception as e:
            self.logger.error(f"Error getting executors info: {str(e)}")
            
        return executors_info
    
    def _get_storage_info(self) -> Dict[str, Any]:
        """Get information about Spark storage (cached RDDs, DataFrames).
        
        Returns:
            Dictionary with storage information
        """
        storage_info = {
            "memoryUsed": 0,
            "diskUsed": 0,
            "totalCached": 0,
            "details": []
        }
        
        if not self.app_id:
            self.logger.warning("No application ID available")
            return storage_info
            
        try:
            response = self._make_request(f"/api/v1/applications/{self.app_id}/storage/rdd")
            
            if response and isinstance(response, list):
                storage_info["totalCached"] = len(response)
                
                # Aggregate storage metrics
                for rdd in response:
                    storage_info["memoryUsed"] += rdd.get("memoryUsed", 0)
                    storage_info["diskUsed"] += rdd.get("diskUsed", 0)
                
                # Add detailed RDD information
                storage_info["details"] = []
                for rdd in response:
                    storage_info["details"].append({
                        "rddId": rdd.get("id"),
                        "name": rdd.get("name"),
                        "memoryUsed": rdd.get("memoryUsed"),
                        "diskUsed": rdd.get("diskUsed"),
                        "numPartitions": rdd.get("numPartitions"),
                        "numCachedPartitions": rdd.get("numCachedPartitions"),
                        "storageLevel": rdd.get("storageLevel")
                    })
                    
        except Exception as e:
            self.logger.error(f"Error getting storage info: {str(e)}")
            
        return storage_info
    
    def _get_environment_info(self) -> Dict[str, Any]:
        """Get information about the Spark environment.
        
        Returns:
            Dictionary with environment information
        """
        env_info = {
            "spark": {},
            "jvm": {},
            "system": {}
        }
        
        if not self.app_id:
            self.logger.warning("No application ID available")
            return env_info
            
        try:
            response = self._make_request(f"/api/v1/applications/{self.app_id}/environment")
            
            if response and isinstance(response, dict):
                # Extract Spark properties
                if "sparkProperties" in response:
                    props = response["sparkProperties"]
                    env_info["spark"] = {
                        "version": props.get("spark.version", ""),
                        "driver.memory": props.get("spark.driver.memory", ""),
                        "executor.memory": props.get("spark.executor.memory", ""),
                        "executor.cores": props.get("spark.executor.cores", ""),
                        "executor.instances": props.get("spark.executor.instances", ""),
                        "master": props.get("spark.master", ""),
                        "app.name": props.get("spark.app.name", ""),
                        "network.timeout": props.get("spark.network.timeout", ""),
                        "shuffle.partitions": props.get("spark.sql.shuffle.partitions", ""),
                        "serializer": props.get("spark.serializer", "")
                    }
                
                # Extract JVM information
                if "systemProperties" in response:
                    sys_props = response["systemProperties"]
                    env_info["jvm"] = {
                        "version": sys_props.get("java.version", ""),
                        "vendor": sys_props.get("java.vendor", ""),
                        "home": sys_props.get("java.home", ""),
                        "runtime_name": sys_props.get("java.runtime.name", ""),
                        "vm_name": sys_props.get("java.vm.name", ""),
                        "vm_version": sys_props.get("java.vm.version", "")
                    }
                
                # Extract system information
                if "systemProperties" in response:
                    sys_props = response["systemProperties"]
                    env_info["system"] = {
                        "os_name": sys_props.get("os.name", ""),
                        "os_version": sys_props.get("os.version", ""),
                        "os_arch": sys_props.get("os.arch", ""),
                        "user_name": sys_props.get("user.name", ""),
                        "user_dir": sys_props.get("user.dir", "")
                    }
                    
        except Exception as e:
            self.logger.error(f"Error getting environment info: {str(e)}")
            
        return env_info
    
    def _make_request(self, endpoint: str) -> Optional[Union[Dict, List]]:
        """Make an HTTP request to the Spark UI API.
        
        Args:
            endpoint: API endpoint (e.g., /api/v1/applications)
            
        Returns:
            JSON response data or None if error
        """
        url = urllib.parse.urljoin(self.base_url, endpoint)
        
        try:
            response = requests.get(url, timeout=self.timeout)
            
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.warning(f"HTTP {response.status_code} from {url}: {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Request error for {url}: {str(e)}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON decode error for {url}: {str(e)}")
            return None


================================================
File: /collectors.md
================================================
# Spark Pipeline Debuggers Collectors Guide

This document provides an overview of the collectors in the Spark Pipeline Debugger framework and explains the available API endpoints.

## Collectors Overview

The Spark Pipeline Debugger framework includes several collectors, each responsible for gathering different types of metrics to help diagnose and monitor Spark applications.

### 1. BaseCollector

The `BaseCollector` is an abstract base class that all other collectors inherit from. It provides:

- A unified interface for collecting metrics
- Caching functionality to avoid repeated calls in a short time period
- Methods for retrieving specific metrics

### 2. CustomLoggingCollector

The `CustomLoggingCollector` parses Spark logs to identify issues and extract structured information:

- **Purpose**: Scan log files for patterns indicating problems in Spark jobs
- **Key metrics collected**:
  - Exception counts, types, and details
  - Data quality issues (null values, schema mismatches, type conversions)
  - GC overhead alerts
  - Shuffle failures
  - Serialization errors
  - Streaming metrics (processing time, input/output rates, batch sizes)
- **Features**:
  - Automatically discovers log directories
  - Tracks specific job and stage logs
  - Parses timestamps and extracts structured information from logs
  - Maintains pattern library for common Spark issues

### 3. CustomAppMetricsCollector

The `CustomAppMetricsCollector` enables domain-specific metrics for your application:

- **Purpose**: Collect application-specific and domain-specific metrics
- **Key metrics collected**:
  - Data quality metrics (null percentages, duplicate percentages)
  - Business metrics (success rates, latency, throughput)
  - Custom metrics defined by users
- **Features**:
  - Extensible with user-defined metric functions
  - Supports quality rules for data validation
  - Automatically extracts schema information
  - Provides built-in metrics for common data quality checks

### 4. SparkMeasureCollector

The `SparkMeasureCollector` provides detailed performance metrics by leveraging the SparkMeasure library:

- **Purpose**: Collect fine-grained performance metrics for stages and tasks
- **Key metrics collected**:
  - Execution times
  - CPU times
  - Memory usage
  - Shuffle metrics
  - I/O metrics
- **Derived metrics**:
  - Data skew detection
  - GC pressure analysis
  - Shuffle intensity calculation
  - Memory spill ratio
- **Modes**: Supports both stage-level (lightweight) and task-level (detailed) collection

### 5. SparkHttpCollector

The `SparkHttpCollector` interfaces with Spark's REST API to provide application-level metrics:

- **Purpose**: Collect metrics from Spark UI HTTP endpoints
- **Key metrics collected**:
  - Application information
  - Job statistics and details
  - Stage information
  - Executor metrics
  - Storage metrics (cached RDDs, DataFrames)
  - Environment information
- **Features**:
  - Auto-discovery of Spark UI URL
  - Detailed executor statistics
  - Job and stage failure analysis

### 6. ExternalMetricsCollector

The `ExternalMetricsCollector` integrates with external monitoring systems:

- **Purpose**: Interface with third-party monitoring tools
- **Supported systems**:
  - CloudWatch (AWS)
  - Ganglia
  - Grafana
  - Prometheus
- **Key metrics collected**:
  - Instance metrics (CPU, memory, disk, network)
  - EMR cluster metrics
  - Custom metrics from these systems
- **Features**:
  - Automatic configuration based on available systems
  - Flexible query capabilities
  - Authentication with external systems

### 7. OSMetricsCollector

The `OSMetricsCollector` provides system-level metrics:

- **Purpose**: Collect OS-level metrics from nodes running Spark
- **Key metrics collected**:
  - CPU usage and load
  - Memory usage
  - Disk I/O and utilization
  - Network traffic
  - Process counts
- **Collection methods**:
  - Prometheus/node_exporter integration
  - Direct system calls using psutil
- **Features**:
  - Calculates rates for cumulative metrics
  - Works on single-node or cluster environments
  - Supports both local and remote node metrics

## API Endpoints

The Spark Pipeline Debugger provides a comprehensive REST API for accessing metrics and controlling collection:

### Root Endpoint

- **GET /** - Returns basic information about the API and the status of enabled collectors

### Settings Endpoints

- **GET /api/v1/settings** - Get current settings for collectors and monitoring
- **POST /api/v1/settings** - Update settings for collectors and monitoring

### Metrics Collection Endpoints

- **POST /api/v1/metrics/collect** - Trigger metrics collection from all enabled collectors
- **GET /api/v1/metrics** - Get all metrics from all collectors

#### Individual Collector Endpoints

- **GET /api/v1/metrics/sparkmeasure** - Get performance metrics from SparkMeasure
- **GET /api/v1/metrics/http** - Get metrics from Spark UI HTTP endpoints
- **GET /api/v1/metrics/logging** - Get custom logging metrics
- **GET /api/v1/metrics/os** - Get OS-level metrics
- **POST /api/v1/metrics/os/collect** - Collect OS metrics with custom configuration
- **GET /api/v1/metrics/app** - Get application-specific metrics
- **POST /api/v1/metrics/app/data-quality** - Analyze data quality with custom configuration
- **GET /api/v1/metrics/external** - Get external metrics
- **POST /api/v1/metrics/external/collect** - Collect external metrics with custom configuration

### Application Endpoints

- **GET /api/v1/applications** - Get all applications
- **GET /api/v1/applications/{app_id}** - Get details for a specific application
- **GET /api/v1/applications/{app_id}/jobs** - Get all jobs for an application
- **GET /api/v1/applications/{app_id}/jobs/{job_id}** - Get details for a specific job
- **GET /api/v1/applications/{app_id}/stages** - Get all stages for an application
- **GET /api/v1/applications/{app_id}/stages/{stage_id}** - Get details for a specific stage
- **GET /api/v1/applications/{app_id}/executors** - Get all executors for an application
- **GET /api/v1/applications/{app_id}/exceptions** - Get all exceptions for an application

### Analysis Endpoints

- **GET /api/v1/applications/{app_id}/jobs/{job_id}/diagnosis** - Get diagnosis for a specific job
- **GET /api/v1/applications/{app_id}/performance** - Get performance metrics for an application
- **GET /api/v1/dashboard** - Get combined metrics for dashboard display

## Usage Examples

### Starting the API Server

```bash
python collectors_api.py --host 0.0.0.0 --port 8000 --spark-ui-url http://spark-master:4040 --log-dir /path/to/spark/logs --prometheus-url http://prometheus:9090
```

### Collecting All Metrics

```bash
curl -X POST http://localhost:8000/api/v1/metrics/collect
```

### Getting a Job Diagnosis

```bash
curl http://localhost:8000/api/v1/applications/app-20250509123456/jobs/1/diagnosis
```

### Analyzing Data Quality

```bash
curl -X POST http://localhost:8000/api/v1/metrics/app/data-quality -H "Content-Type: application/json" -d '{"table_name": "my_table", "columns": ["id", "name", "value"]}'
```

## Configuration Options

The API server and collectors can be configured through environment variables or command-line arguments:

- **SPARK_UI_URL**: URL for the Spark UI (default: http://localhost:4040)
- **SPARK_LOG_DIR**: Directory containing Spark logs (default: /tmp/spark-logs)
- **CACHE_TTL**: Time-to-live for metrics cache in seconds (default: 60)
- **COLLECTION_INTERVAL**: Interval for automatic metrics collection in seconds (default: 30)
- **PROMETHEUS_URL**: URL for Prometheus server (for OS metrics)
- **CUSTOM_METRICS_PATH**: Path to custom metrics definition file
- **AWS_REGION**: AWS region for CloudWatch metrics
- **GANGLIA_HOST**: Hostname for Ganglia metrics

## Extending the Framework

The Spark Pipeline Debugger framework is designed to be extensible. You can:

1. Create custom metrics using the `@metric` and `@quality_rule` decorators
2. Add new collectors by inheriting from `BaseCollector`
3. Integrate with additional external systems

## Troubleshooting

Common issues and solutions:

- **No metrics displayed**: Ensure Spark application is running and accessible
- **No logs collected**: Verify log directory path is correct
- **High latency**: Adjust collection interval and cache TTL
- **Missing OS metrics**: Install node_exporter or provide Prometheus URL

For more detailed diagnostics, check the API server logs.

================================================
File: /test_collectors.py
================================================
#!/usr/bin/env python3
# test_collectors.py - Test script for the Spark collectors

from pyspark.sql import SparkSession
import logging
import json
import time
import os

# Import the collectors
from base_collector import BaseCollector
from sparkmeasure_collector import SparkMeasureCollector
from httpendpoints_collector import SparkHttpCollector
from custom_collector import CustomLoggingCollector

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("test_collectors")

def create_spark_session():
    """Create a Spark session for testing"""
    logger.info("Creating Spark session...")
    spark = (SparkSession.builder
             .appName("SparkDebuggerTest")
             .master("local[*]")
             .config("spark.ui.port", "4040")  # Ensure UI is accessible
             .config("spark.eventLog.enabled", "true")
             .config("spark.eventLog.dir", "/tmp/spark-events")
             .getOrCreate())
    return spark

def run_sample_job(spark):
    """Run a sample Spark job that will generate metrics and logs"""
    logger.info("Running sample Spark job...")
    
    # Create a sample dataframe
    data = [(i, f"value_{i % 10}") for i in range(100000)]
    df = spark.createDataFrame(data, ["id", "value"])
    
    # Force some calculations to generate metrics
    logger.info("Performing transformations and actions...")
    
    # Cache the dataframe
    df.cache()
    
    # Count to materialize the cache
    count = df.count()
    logger.info(f"DataFrame count: {count}")
    
    # Group by to force shuffle
    grouped = df.groupBy("value").count()
    grouped_result = grouped.collect()
    logger.info(f"Grouped result count: {len(grouped_result)}")
    
    # Join operation to generate more complex metrics
    df2 = spark.createDataFrame([(i % 10, f"data_{i}") for i in range(20)], ["key", "data"])
    joined = df.withColumnRenamed("value", "key").join(df2, "key")
    joined_count = joined.count()
    logger.info(f"Joined result count: {joined_count}")
    
    # Generate a deliberate error (uncomment to test error handling)
    # try:
    #     spark.sql("SELECT * FROM nonexistent_table").show()
    # except Exception as e:
    #     logger.error(f"Expected error: {str(e)}")
    
    return {
        "df": df,
        "grouped": grouped,
        "joined": joined
    }

def test_sparkmeasure_collector(spark, dfs):
    """Test the SparkMeasure collector"""
    logger.info("Testing SparkMeasure collector...")
    
    try:
        # Initialize the collector
        collector = SparkMeasureCollector(mode="stage")
        
        # Set up with Spark session
        collector.setup({"spark_session": spark})
        
        # Begin collection
        collector.begin_collection(job_id="test_job_1")
        
        # Run some operations
        result = dfs["df"].groupBy("value").agg({"id": "sum"}).collect()
        logger.info(f"Aggregation result count: {len(result)}")
        
        # End collection and get metrics
        metrics = collector.end_collection()
        
        # Test getting current metrics
        current_metrics = collector.collect({"spark_session": spark})
        
        # Print the metrics
        logger.info(f"SparkMeasure metrics keys: {list(metrics.keys())}")
        logger.info(f"StageMetrics sample: {json.dumps(metrics.get('stage', {}).get('duration_stats', {}), indent=2)}")
        
        return {
            "status": "success",
            "metrics": metrics
        }
    except Exception as e:
        logger.error(f"Error testing SparkMeasure collector: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

def test_http_collector(spark):
    """Test the HTTP endpoints collector"""
    logger.info("Testing HTTP endpoints collector...")
    
    try:
        # Initialize the collector
        collector = SparkHttpCollector(base_url="http://localhost:4040")
        
        # Set up with Spark session
        collector.setup({"spark_session": spark})
        
        # Wait for UI to be fully initialized
        time.sleep(2)
        
        # Collect metrics
        metrics = collector.collect({})
        
        # Print the metrics
        logger.info(f"HTTP metrics keys: {list(metrics.keys())}")
        logger.info(f"Application info: {json.dumps(metrics.get('app', {}), indent=2)}")
        
        return {
            "status": "success",
            "metrics": metrics
        }
    except Exception as e:
        logger.error(f"Error testing HTTP collector: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

def test_custom_logging_collector(spark):
    """Test the custom logging collector"""
    logger.info("Testing custom logging collector...")
    
    try:
        # Create a temporary log file for testing
        log_dir = "/tmp/spark-debugger-test-logs"
        os.makedirs(log_dir, exist_ok=True)
        
        # Create a sample log file with test entries
        log_file = os.path.join(log_dir, "test-application.log")
        with open(log_file, "w") as f:
            f.write("2023-05-09 14:32:05 INFO  SparkContext: Running Spark version 3.3.0\n")
            f.write("2023-05-09 14:32:10 WARN  TaskSetManager: Stage 0 contains a task of very large size (1024 KB)\n")
            f.write("2023-05-09 14:32:15 ERROR SparkContext: Exception in thread main: java.lang.OutOfMemoryError: GC overhead limit exceeded\n")
            f.write("2023-05-09 14:32:16 ERROR TaskSetManager: Task 2 in stage 0.0 failed 4 times\n")
            f.write("2023-05-09 14:32:20 INFO  Streaming query made progress: batchDuration=100ms inputRowsPerSecond=1000 processedRowsPerSecond=900\n")
        
        # Initialize the collector
        collector = CustomLoggingCollector(log_dir=log_dir)
        
        # Set up with Spark session
        collector.setup({"log_dir": log_dir})
        
        # Collect metrics
        metrics = collector.collect({})
        
        # Print the metrics
        logger.info(f"Custom logging metrics keys: {list(metrics.keys())}")
        logger.info(f"Log counts: errors={metrics['logs']['error_count']}, warnings={metrics['logs']['warning_count']}")
        logger.info(f"Exceptions: {json.dumps(metrics['exceptions'], indent=2)}")
        
        return {
            "status": "success",
            "metrics": metrics
        }
    except Exception as e:
        logger.error(f"Error testing custom logging collector: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

def main():
    """Main test function"""
    logger.info("Starting Spark Debugger collector tests")
    
    # Create Spark session
    spark = create_spark_session()
    
    try:
        # Run a sample job to generate metrics
        dfs = run_sample_job(spark)
        
        # Test each collector
        sparkmeasure_results = test_sparkmeasure_collector(spark, dfs)
        http_results = test_http_collector(spark)
        logging_results = test_custom_logging_collector(spark)
        
        # Print overall results
        logger.info("Test results:")
        logger.info(f"SparkMeasure collector: {sparkmeasure_results['status']}")
        logger.info(f"HTTP endpoints collector: {http_results['status']}")
        logger.info(f"Custom logging collector: {logging_results['status']}")
        
        # Save metrics to file for inspection
        with open("/tmp/spark-debugger-test-results.json", "w") as f:
            json.dump({
                "sparkmeasure": sparkmeasure_results,
                "http": http_results,
                "logging": logging_results
            }, f, indent=2)
        
        logger.info("Test results saved to /tmp/spark-debugger-test-results.json")
        
    finally:
        # Stop Spark session
        spark.stop()
        logger.info("Spark session stopped")

if __name__ == "__main__":
    main()

================================================
File: /collectors_api.py
================================================
#!/usr/bin/env python3
# comprehensive_api.py - Comprehensive REST API for Spark Pipeline Debugger

from fastapi import FastAPI, HTTPException, Query, BackgroundTasks, Depends, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Optional, Union
import os
import json
import logging
import time
import asyncio
from datetime import datetime
import uvicorn

# Import the collectors
from base_collector import BaseCollector
from sparkmeasure_collector import SparkMeasureCollector
from httpendpoints_collector import SparkHttpCollector
from custom_collector import CustomLoggingCollector
from os_metrics_collector import OSMetricsCollector
from custom_app_metrics_collector import CustomAppMetricsCollector
from external_metrics_collector import ExternalMetricsCollector

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("spark_debugger_api")

# Create FastAPI app
app = FastAPI(
    title="Comprehensive Spark Pipeline Debugger API",
    description="REST API for monitoring and debugging Spark applications",
    version="1.0.0"
)

# Add CORS middleware to allow cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For production, specify actual origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global cache for metrics
metrics_cache = {
    "sparkmeasure": {},
    "http": {},
    "logging": {},
    "os_metrics": {},
    "app_metrics": {},
    "external_metrics": {},
    "last_updated": 0
}

# Global settings
settings = {
    "spark_ui_url": os.environ.get("SPARK_UI_URL", "http://localhost:4040"),
    "log_dir": os.environ.get("SPARK_LOG_DIR", "/tmp/spark-logs"),
    "cache_ttl": int(os.environ.get("CACHE_TTL", "60")),  # seconds
    "collection_interval": int(os.environ.get("COLLECTION_INTERVAL", "30")),  # seconds
    "monitoring_enabled": True,
    "os_metrics_enabled": True,
    "app_metrics_enabled": True,
    "external_metrics_enabled": True,
    "prometheus_url": os.environ.get("PROMETHEUS_URL", "http://localhost:9090"),
    "custom_metrics_path": os.environ.get("CUSTOM_METRICS_PATH", None),
    "cloudwatch_region": os.environ.get("AWS_REGION", None),
    "ganglia_host": os.environ.get("GANGLIA_HOST", None)
}

# Create collector instances
sparkmeasure_collector = None
http_collector = None
logging_collector = None
os_metrics_collector = None
app_metrics_collector = None
external_metrics_collector = None

# Pydantic models for API requests and responses
class ApplicationInfo(BaseModel):
    id: str
    name: str
    status: str
    startTime: Optional[int] = None
    endTime: Optional[int] = None
    duration: Optional[int] = None
    user: Optional[str] = None

class JobInfo(BaseModel):
    id: str
    name: str
    status: str
    startTime: Optional[int] = None
    endTime: Optional[int] = None
    duration: Optional[int] = None
    numTasks: Optional[int] = None
    numActiveTasks: Optional[int] = None
    numCompletedTasks: Optional[int] = None
    numFailedTasks: Optional[int] = None

class StageInfo(BaseModel):
    id: str
    attemptId: int
    name: str
    status: str
    numTasks: Optional[int] = None
    executorRunTime: Optional[int] = None
    inputBytes: Optional[int] = None
    outputBytes: Optional[int] = None
    shuffleReadBytes: Optional[int] = None
    shuffleWriteBytes: Optional[int] = None
    memoryBytesSpilled: Optional[int] = None
    diskBytesSpilled: Optional[int] = None
    failureReason: Optional[str] = None

class ExceptionInfo(BaseModel):
    type: str
    message: str
    timestamp: int
    stackTrace: Optional[List[str]] = None

class SettingsUpdate(BaseModel):
    spark_ui_url: Optional[str] = None
    log_dir: Optional[str] = None
    cache_ttl: Optional[int] = None
    collection_interval: Optional[int] = None
    monitoring_enabled: Optional[bool] = None
    os_metrics_enabled: Optional[bool] = None
    app_metrics_enabled: Optional[bool] = None
    external_metrics_enabled: Optional[bool] = None
    prometheus_url: Optional[str] = None
    custom_metrics_path: Optional[str] = None
    cloudwatch_region: Optional[str] = None
    ganglia_host: Optional[str] = None
    external_system_type: Optional[str] = None

class PerformanceMetrics(BaseModel):
    dataSkew: Optional[float] = None
    gcPressure: Optional[float] = None
    shuffleIntensity: Optional[float] = None
    memorySpill: Optional[Dict[str, Any]] = None
    skewLevel: Optional[str] = None
    gcLevel: Optional[str] = None
    shuffleLevel: Optional[str] = None
    spillLevel: Optional[str] = None

class DataQualityConfig(BaseModel):
    table_name: Optional[str] = None
    columns: Optional[List[str]] = None
    rules: Optional[List[Dict[str, Any]]] = None

class ExternalMetricsConfig(BaseModel):
    system_type: str = Field(..., description="Type of external system (cloudwatch, ganglia, grafana, prometheus)")
    config: Dict[str, Any] = Field(..., description="Configuration for the external system")
    queries: Optional[List[Dict[str, Any]]] = None

class OSMetricsConfig(BaseModel):
    prometheus_url: Optional[str] = None
    node_hostnames: Optional[List[str]] = None

# Initialization and monitoring functions
def initialize_collectors():
    """Initialize the collector instances"""
    global sparkmeasure_collector, http_collector, logging_collector
    global os_metrics_collector, app_metrics_collector, external_metrics_collector
    
    try:
        # Initialize SparkMeasure collector
        sparkmeasure_collector = SparkMeasureCollector(mode="stage")
        
        # Initialize HTTP collector
        http_collector = SparkHttpCollector(base_url=settings["spark_ui_url"])
        
        # Initialize custom logging collector
        logging_collector = CustomLoggingCollector(log_dir=settings["log_dir"])
        
        # Initialize OS metrics collector
        if settings["os_metrics_enabled"]:
            os_metrics_collector = OSMetricsCollector(prometheus_url=settings.get("prometheus_url"))
        
        # Initialize application metrics collector
        if settings["app_metrics_enabled"]:
            app_metrics_collector = CustomAppMetricsCollector(custom_metrics_path=settings.get("custom_metrics_path"))
        
        # Initialize external metrics collector
        if settings["external_metrics_enabled"]:
            external_system_type = "cloudwatch"
            if settings.get("ganglia_host"):
                external_system_type = "ganglia"
                
            external_metrics_collector = ExternalMetricsCollector(system_type=external_system_type)
            
            # Set up with available config
            if external_system_type == "cloudwatch" and settings.get("cloudwatch_region"):
                external_metrics_collector.setup({
                    "aws_region": settings["cloudwatch_region"]
                })
            elif external_system_type == "ganglia" and settings.get("ganglia_host"):
                external_metrics_collector.setup({
                    "ganglia_host": settings["ganglia_host"]
                })
        
        logger.info("Collectors initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing collectors: {str(e)}")
        raise

def setup_spark_session():
    """Set up a Spark session for the collectors"""
    try:
        from pyspark.sql import SparkSession
        
        # Try to get existing session or create a new one
        spark = SparkSession.getActiveSession()
        if not spark:
            # Try creating a new session, but handle issues with event logging
            try:
                spark = (SparkSession.builder
                        .appName("SparkDebuggerAPI")
                        .master("local[*]")
                        .config("spark.eventLog.enabled", "false")
                        .getOrCreate())
            except Exception as e:
                logger.warning(f"Error creating Spark session with default config: {str(e)}")
                # Try with minimal configuration
                spark = (SparkSession.builder
                        .appName("SparkDebuggerAPI")
                        .master("local[*]")
                        .config("spark.eventLog.enabled", "false")
                        .config("spark.ui.enabled", "true")
                        .getOrCreate())
        
        # Set up collectors with Spark session
        if sparkmeasure_collector:
            try:
                sparkmeasure_collector.setup({"spark_session": spark})
            except Exception as e:
                logger.warning(f"Error setting up SparkMeasure collector: {str(e)}")
        
        if http_collector:
            try:
                http_collector.setup({"spark_session": spark})
            except Exception as e:
                logger.warning(f"Error setting up HTTP collector: {str(e)}")
                
        if app_metrics_collector:
            try:
                app_metrics_collector.setup({"spark_session": spark})
            except Exception as e:
                logger.warning(f"Error setting up app metrics collector: {str(e)}")
        
        logger.info("Spark session set up for collectors")
        return spark
    except Exception as e:
        logger.error(f"Error setting up Spark session: {str(e)}")
        return None

def collect_metrics():
    """Collect metrics from all collectors"""
    global metrics_cache
    
    try:
        # Prepare context
        context = {}
        
        # Try to get a Spark session
        try:
            from pyspark.sql import SparkSession
            spark = SparkSession.getActiveSession()
            if spark:
                context["spark_session"] = spark
        except:
            pass
            
        # Collect metrics from each collector
        metrics = {
            "timestamp": time.time(),
            "sparkmeasure": {},
            "http": {},
            "logging": {},
            "os_metrics": {},
            "app_metrics": {},
            "external_metrics": {}
        }
        
        # Collect from SparkMeasure
        if sparkmeasure_collector:
            try:
                metrics["sparkmeasure"] = sparkmeasure_collector.collect(context)
                logger.debug("Collected SparkMeasure metrics")
            except Exception as e:
                logger.error(f"Error collecting SparkMeasure metrics: {str(e)}")
        
        # Collect from HTTP endpoints
        if http_collector:
            try:
                metrics["http"] = http_collector.collect(context)
                logger.debug("Collected HTTP metrics")
            except Exception as e:
                logger.error(f"Error collecting HTTP metrics: {str(e)}")
        
        # Collect from custom logging
        if logging_collector:
            try:
                metrics["logging"] = logging_collector.collect(context)
                logger.debug("Collected custom logging metrics")
            except Exception as e:
                logger.error(f"Error collecting custom logging metrics: {str(e)}")
                
        # Collect from OS metrics
        if os_metrics_collector and settings["os_metrics_enabled"]:
            try:
                metrics["os_metrics"] = os_metrics_collector.collect(context)
                logger.debug("Collected OS metrics")
            except Exception as e:
                logger.error(f"Error collecting OS metrics: {str(e)}")
                
        # Collect from app metrics
        if app_metrics_collector and settings["app_metrics_enabled"]:
            try:
                # We need a DataFrame for app metrics, but we'll skip if not available
                if "spark_session" in context:
                    # Try to get an active table or DataFrame
                    spark = context["spark_session"]
                    try:
                        # Find an available table to analyze
                        tables = spark.catalog.listTables()
                        if tables:
                            table_name = tables[0].name
                            app_context = context.copy()
                            app_context["table_name"] = table_name
                            metrics["app_metrics"] = app_metrics_collector.collect(app_context)
                            logger.debug(f"Collected app metrics for table {table_name}")
                    except Exception as e:
                        logger.debug(f"No tables found for app metrics: {str(e)}")
            except Exception as e:
                logger.error(f"Error collecting app metrics: {str(e)}")
                
        # Collect from external metrics
        if external_metrics_collector and settings["external_metrics_enabled"]:
            try:
                metrics["external_metrics"] = external_metrics_collector.collect(context)
                logger.debug("Collected external metrics")
            except Exception as e:
                logger.error(f"Error collecting external metrics: {str(e)}")
        
        # Update the cache
        metrics_cache.update(metrics)
        metrics_cache["last_updated"] = time.time()
        
        logger.info(f"All metrics collected and cached at {datetime.fromtimestamp(metrics_cache['last_updated']).isoformat()}")
        
        return metrics
    except Exception as e:
        logger.error(f"Error collecting metrics: {str(e)}")
        return {"error": str(e)}

async def periodic_collection():
    """Periodically collect metrics in the background"""
    while settings["monitoring_enabled"]:
        try:
            collect_metrics()
        except Exception as e:
            logger.error(f"Error in periodic collection: {str(e)}")
            
        # Sleep for the collection interval
        await asyncio.sleep(settings["collection_interval"])

# API routes
@app.get("/")
async def root():
    """API root endpoint"""
    return {
        "name": "Comprehensive Spark Pipeline Debugger API",
        "version": "1.0.0",
        "status": "running",
        "metrics_last_updated": datetime.fromtimestamp(metrics_cache["last_updated"]).isoformat() if metrics_cache["last_updated"] > 0 else None,
        "enabled_collectors": {
            "sparkmeasure": sparkmeasure_collector is not None,
            "http": http_collector is not None,
            "logging": logging_collector is not None,
            "os_metrics": os_metrics_collector is not None and settings["os_metrics_enabled"],
            "app_metrics": app_metrics_collector is not None and settings["app_metrics_enabled"],
            "external_metrics": external_metrics_collector is not None and settings["external_metrics_enabled"]
        }
    }

@app.get("/api/v1/settings")
async def get_settings():
    """Get current settings"""
    return settings

@app.post("/api/v1/settings")
async def update_settings(new_settings: SettingsUpdate):
    """Update settings"""
    global settings
    
    # Update settings with new values
    if new_settings.spark_ui_url is not None:
        settings["spark_ui_url"] = new_settings.spark_ui_url
        if http_collector:
            http_collector.base_url = new_settings.spark_ui_url
    
    if new_settings.log_dir is not None:
        settings["log_dir"] = new_settings.log_dir
        if logging_collector:
            logging_collector.log_dir = new_settings.log_dir
    
    if new_settings.cache_ttl is not None:
        settings["cache_ttl"] = new_settings.cache_ttl
    
    if new_settings.collection_interval is not None:
        settings["collection_interval"] = new_settings.collection_interval
    
    if new_settings.monitoring_enabled is not None:
        settings["monitoring_enabled"] = new_settings.monitoring_enabled
        
    if new_settings.os_metrics_enabled is not None:
        settings["os_metrics_enabled"] = new_settings.os_metrics_enabled
        # Initialize collector if not already
        if settings["os_metrics_enabled"] and os_metrics_collector is None:
            os_metrics_collector = OSMetricsCollector(prometheus_url=settings.get("prometheus_url"))
        
    if new_settings.app_metrics_enabled is not None:
        settings["app_metrics_enabled"] = new_settings.app_metrics_enabled
        # Initialize collector if not already
        if settings["app_metrics_enabled"] and app_metrics_collector is None:
            app_metrics_collector = CustomAppMetricsCollector(custom_metrics_path=settings.get("custom_metrics_path"))
    
    if new_settings.external_metrics_enabled is not None:
        settings["external_metrics_enabled"] = new_settings.external_metrics_enabled
        # Initialize collector if not already
        if settings["external_metrics_enabled"] and external_metrics_collector is None:
            external_system_type = new_settings.external_system_type or "cloudwatch"
            external_metrics_collector = ExternalMetricsCollector(system_type=external_system_type)
            
    if new_settings.prometheus_url is not None:
        settings["prometheus_url"] = new_settings.prometheus_url
        if os_metrics_collector:
            os_metrics_collector.prometheus_url = new_settings.prometheus_url
    
    if new_settings.custom_metrics_path is not None:
        settings["custom_metrics_path"] = new_settings.custom_metrics_path
        if app_metrics_collector:
            app_metrics_collector.custom_metrics_path = new_settings.custom_metrics_path
            # Reload custom metrics
            app_metrics_collector._load_custom_metrics(new_settings.custom_metrics_path)
    
    if new_settings.cloudwatch_region is not None:
        settings["cloudwatch_region"] = new_settings.cloudwatch_region
        if external_metrics_collector and external_metrics_collector.system_type == "cloudwatch":
            external_metrics_collector.setup({
                "aws_region": new_settings.cloudwatch_region
            })
    
    if new_settings.ganglia_host is not None:
        settings["ganglia_host"] = new_settings.ganglia_host
        if external_metrics_collector and external_metrics_collector.system_type == "ganglia":
            external_metrics_collector.setup({
                "ganglia_host": new_settings.ganglia_host
            })
            
    if new_settings.external_system_type is not None:
        if external_metrics_collector:
            external_metrics_collector.system_type = new_settings.external_system_type
            
    return settings

@app.post("/api/v1/metrics/collect")
async def trigger_collection(background_tasks: BackgroundTasks):
    """Trigger a metrics collection"""
    background_tasks.add_task(collect_metrics)
    return {"message": "Collection started", "status": "success"}

@app.get("/api/v1/metrics")
async def get_all_metrics(force_refresh: bool = False):
    """Get all metrics"""
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache

# Original metrics endpoints
@app.get("/api/v1/metrics/sparkmeasure")
async def get_sparkmeasure_metrics(force_refresh: bool = False):
    """Get SparkMeasure metrics"""
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache["sparkmeasure"]

@app.get("/api/v1/metrics/http")
async def get_http_metrics(force_refresh: bool = False):
    """Get HTTP metrics"""
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache["http"]

@app.get("/api/v1/metrics/logging")
async def get_logging_metrics(force_refresh: bool = False):
    """Get custom logging metrics"""
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache["logging"]

# New metrics endpoints
@app.get("/api/v1/metrics/os")
async def get_os_metrics(force_refresh: bool = False):
    """Get OS-level metrics"""
    if not settings["os_metrics_enabled"] or not os_metrics_collector:
        raise HTTPException(status_code=400, detail="OS metrics collection is not enabled")
        
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache["os_metrics"]

@app.post("/api/v1/metrics/os/collect")
async def collect_os_metrics(config: OSMetricsConfig):
    """Collect OS metrics with custom configuration"""
    if not settings["os_metrics_enabled"] or not os_metrics_collector:
        raise HTTPException(status_code=400, detail="OS metrics collection is not enabled")
        
    try:
        # Create context from config
        context = {}
        if config.prometheus_url:
            context["prometheus_url"] = config.prometheus_url
        if config.node_hostnames:
            context["node_hostnames"] = config.node_hostnames
            
        # Collect metrics
        metrics = os_metrics_collector.collect(context)
        
        # Update cache
        metrics_cache["os_metrics"] = metrics
        metrics_cache["last_updated"] = time.time()
        
        return metrics
    except Exception as e:
        logger.error(f"Error collecting OS metrics: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/metrics/app")
async def get_app_metrics(force_refresh: bool = False):
    """Get application-specific metrics"""
    if not settings["app_metrics_enabled"] or not app_metrics_collector:
        raise HTTPException(status_code=400, detail="Application metrics collection is not enabled")
        
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache["app_metrics"]

@app.post("/api/v1/metrics/app/data-quality")
async def analyze_data_quality(config: DataQualityConfig):
    """Analyze data quality with custom configuration"""
    if not settings["app_metrics_enabled"] or not app_metrics_collector:
        raise HTTPException(status_code=400, detail="Application metrics collection is not enabled")
        
    try:
        # Get Spark session
        from pyspark.sql import SparkSession
        spark = SparkSession.getActiveSession()
        if not spark:
            raise HTTPException(status_code=400, detail="No active Spark session found")
            
        # Create context
        context = {
            "spark_session": spark
        }
        
        # Add table name if provided
        if config.table_name:
            context["table_name"] = config.table_name
            
        # Add custom_args if needed
        if config.columns or config.rules:
            context["custom_args"] = {}
            if config.columns:
                context["custom_args"]["columns"] = config.columns
            if config.rules:
                context["custom_args"]["rules"] = config.rules
                
        # Collect metrics
        metrics = app_metrics_collector.collect(context)
        
        # Update cache
        metrics_cache["app_metrics"] = metrics
        metrics_cache["last_updated"] = time.time()
        
        return metrics
    except Exception as e:
        logger.error(f"Error analyzing data quality: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/metrics/external")
async def get_external_metrics(force_refresh: bool = False):
    """Get external metrics"""
    if not settings["external_metrics_enabled"] or not external_metrics_collector:
        raise HTTPException(status_code=400, detail="External metrics collection is not enabled")
        
    # Check if cache is expired or force refresh is requested
    cache_age = time.time() - metrics_cache["last_updated"]
    if force_refresh or cache_age > settings["cache_ttl"]:
        logger.info("Collecting fresh metrics")
        collect_metrics()
    
    return metrics_cache["external_metrics"]

@app.post("/api/v1/metrics/external/collect")
async def collect_external_metrics(config: ExternalMetricsConfig):
    """Collect external metrics with custom configuration"""
    if not settings["external_metrics_enabled"]:
        raise HTTPException(status_code=400, detail="External metrics collection is not enabled")
        
    try:
        # Initialize collector if not already
        global external_metrics_collector
        if not external_metrics_collector:
            external_metrics_collector = ExternalMetricsCollector(system_type=config.system_type)
            
        # Update collector if system type changed
        if external_metrics_collector.system_type != config.system_type:
            external_metrics_collector.system_type = config.system_type
            
        # Set up collector with config
        external_metrics_collector.setup(config.config)
        
        # Create context with queries if provided
        context = config.config.copy()
        if config.queries:
            context["queries"] = config.queries
            
        # Collect metrics
        metrics = external_metrics_collector.collect(context)
        
        # Update cache
        metrics_cache["external_metrics"] = metrics
        metrics_cache["last_updated"] = time.time()
        
        return metrics
    except Exception as e:
        logger.error(f"Error collecting external metrics: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Application and Job endpoints
@app.get("/api/v1/applications")
async def get_applications():
    """Get all applications"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    app_info = http_metrics.get("app", {})
    
    # Format as a list for consistency
    if app_info and not isinstance(app_info, list):
        app_info = [app_info]
    
    return app_info

@app.get("/api/v1/applications/{app_id}")
async def get_application(app_id: str):
    """Get details for a specific application"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    app_info = http_metrics.get("app", {})
    
    # Check if we have the requested app
    if not app_info or (isinstance(app_info, dict) and app_info.get("id") != app_id) or (
            isinstance(app_info, list) and not any(app.get("id") == app_id for app in app_info)):
        # Try to refresh with the specific app ID
        if http_collector:
            http_collector.app_id = app_id
            http_metrics = http_collector.collect({})
            metrics_cache["http"] = http_metrics
            app_info = http_metrics.get("app", {})
    
    # Format the result
    if isinstance(app_info, list):
        for app in app_info:
            if app.get("id") == app_id:
                return app
        raise HTTPException(status_code=404, detail=f"Application {app_id} not found")
    elif isinstance(app_info, dict) and app_info.get("id") == app_id:
        return app_info
    else:
        raise HTTPException(status_code=404, detail=f"Application {app_id} not found")

@app.get("/api/v1/applications/{app_id}/jobs")
async def get_jobs(app_id: str):
    """Get all jobs for an application"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    
    # Check if we have the requested app
    if http_collector and http_collector.app_id != app_id:
        http_collector.app_id = app_id
        http_metrics = http_collector.collect({})
        metrics_cache["http"] = http_metrics
    
    jobs_info = http_metrics.get("jobs", {})
    if not jobs_info:
        raise HTTPException(status_code=404, detail=f"No jobs found for application {app_id}")
    
    return jobs_info

@app.get("/api/v1/applications/{app_id}/jobs/{job_id}")
async def get_job(app_id: str, job_id: str):
    """Get details for a specific job"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    
    # Check if we have the requested app
    if http_collector and http_collector.app_id != app_id:
        http_collector.app_id = app_id
        http_metrics = http_collector.collect({})
        metrics_cache["http"] = http_metrics
    
    jobs_info = http_metrics.get("jobs", {})
    if not jobs_info:
        raise HTTPException(status_code=404, detail=f"No jobs found for application {app_id}")
    
    # Find the specific job
    for job in jobs_info.get("details", []):
        if str(job.get("jobId")) == job_id:
            return job
    
    raise HTTPException(status_code=404, detail=f"Job {job_id} not found for application {app_id}")

@app.get("/api/v1/applications/{app_id}/stages")
async def get_stages(app_id: str):
    """Get all stages for an application"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    
    # Check if we have the requested app
    if http_collector and http_collector.app_id != app_id:
        http_collector.app_id = app_id
        http_metrics = http_collector.collect({})
        metrics_cache["http"] = http_metrics
    
    stages_info = http_metrics.get("stages", {})
    if not stages_info:
        raise HTTPException(status_code=404, detail=f"No stages found for application {app_id}")
    
    return stages_info

@app.get("/api/v1/applications/{app_id}/stages/{stage_id}")
async def get_stage(app_id: str, stage_id: str):
    """Get details for a specific stage"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    
    # Check if we have the requested app
    if http_collector and http_collector.app_id != app_id:
        http_collector.app_id = app_id
        http_metrics = http_collector.collect({})
        metrics_cache["http"] = http_metrics
    
    stages_info = http_metrics.get("stages", {})
    if not stages_info:
        raise HTTPException(status_code=404, detail=f"No stages found for application {app_id}")
    
    # Find the specific stage
    for stage in stages_info.get("details", []):
        if str(stage.get("stageId")) == stage_id:
            return stage
    
    raise HTTPException(status_code=404, detail=f"Stage {stage_id} not found for application {app_id}")

@app.get("/api/v1/applications/{app_id}/executors")
async def get_executors(app_id: str):
    """Get all executors for an application"""
    # Make sure we have recent HTTP metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    http_metrics = metrics_cache.get("http", {})
    
    # Check if we have the requested app
    if http_collector and http_collector.app_id != app_id:
        http_collector.app_id = app_id
        http_metrics = http_collector.collect({})
        metrics_cache["http"] = http_metrics
    
    executors_info = http_metrics.get("executors", {})
    if not executors_info:
        raise HTTPException(status_code=404, detail=f"No executors found for application {app_id}")
    
    return executors_info

@app.get("/api/v1/applications/{app_id}/exceptions")
async def get_exceptions(app_id: str):
    """Get all exceptions for an application"""
    # Make sure we have recent logging metrics
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    logging_metrics = metrics_cache.get("logging", {})
    exceptions = logging_metrics.get("exceptions", {})
    
    # Check if we have logging collector and update the app ID
    if logging_collector:
        try:
            logging_collector.setup({"app_id": app_id})
            logging_metrics = logging_collector.collect({})
            metrics_cache["logging"] = logging_metrics
            exceptions = logging_metrics.get("exceptions", {})
        except Exception as e:
            logger.error(f"Error collecting logging metrics for app {app_id}: {str(e)}")
    
    return exceptions

@app.get("/api/v1/applications/{app_id}/jobs/{job_id}/diagnosis")
async def get_job_diagnosis(app_id: str, job_id: str):
    """Get diagnosis for a specific job"""
    # Collect metrics from all collectors
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    # Get metrics from each collector
    sparkmeasure_metrics = metrics_cache.get("sparkmeasure", {})
    http_metrics = metrics_cache.get("http", {})
    logging_metrics = metrics_cache.get("logging", {})
    os_metrics = metrics_cache.get("os_metrics", {})
    app_metrics = metrics_cache.get("app_metrics", {})
    
    # Prepare diagnosis information
    diagnosis = {
        "timestamp": time.time(),
        "application_id": app_id,
        "job_id": job_id,
        "status": "SUCCESS",  # Default status
        "summary": "No issues detected",
        "details": [],
        "recommendations": []
    }
    
    # Check for performance issues from SparkMeasure
    if "derived" in sparkmeasure_metrics:
        derived = sparkmeasure_metrics["derived"]
        
        # Check for data skew
        if derived.get("dataSkew", 0) > 3:
            diagnosis["status"] = "WARNING"
            diagnosis["details"].append(f"Data skew detected (skew ratio: {derived.get('dataSkew', 0):.2f})")
            diagnosis["recommendations"].append(
                "Consider using salting techniques or repartitioning to distribute data more evenly"
            )
        
        # Check for GC pressure
        if derived.get("gcPressure", 0) > 0.1:
            diagnosis["status"] = "WARNING"
            diagnosis["details"].append(f"High GC pressure detected ({derived.get('gcPressure', 0)*100:.2f}% of execution time)")
            diagnosis["recommendations"].append(
                "Consider increasing executor memory or adjusting GC settings"
            )
        
        # Check for memory spill
        if "memorySpill" in derived and derived["memorySpill"].get("total", 0) > 1e8:
            diagnosis["status"] = "WARNING"
            spill_mb = derived["memorySpill"].get("total", 0) / 1024 / 1024
            diagnosis["details"].append(f"Significant memory spill detected ({spill_mb:.2f} MB)")
            diagnosis["recommendations"].append(
                "Consider increasing spark.memory.fraction or increasing executor memory"
            )
    
    # Check for failures in jobs from HTTP metrics
    job_found = False
    for job in http_metrics.get("jobs", {}).get("details", []):
        if str(job.get("jobId")) == job_id:
            job_found = True
            if job.get("status") == "FAILED":
                diagnosis["status"] = "ERROR"
                diagnosis["summary"] = f"Job {job_id} failed"
                diagnosis["details"].append(f"Job {job_id} failed with {job.get('numFailedTasks', 0)} failed tasks")
            
            # Check for task failures
            if job.get("numFailedTasks", 0) > 0:
                diagnosis["status"] = "WARNING" if diagnosis["status"] != "ERROR" else "ERROR"
                diagnosis["details"].append(f"{job.get('numFailedTasks', 0)} tasks failed in this job")
    
    # Check for exceptions in logging metrics
    exceptions = logging_metrics.get("exceptions", {}).get("latest", [])
    for exception in exceptions[:5]:  # Limit to first 5 exceptions
        exception_type = exception.get("type", "Unknown")
        diagnosis["details"].append(f"Exception detected: {exception_type}")
        
        # Update status based on exceptions
        if exception_type in ["OutOfMemoryError", "SparkException"]:
            diagnosis["status"] = "ERROR"
        elif diagnosis["status"] != "ERROR":
            diagnosis["status"] = "WARNING"
        
        # Add specific recommendations based on exception type
        if "OutOfMemoryError" in exception_type:
            diagnosis["recommendations"].append(
                "Increase executor memory or driver memory, check for data skew"
            )
        elif "SparkException" in exception_type and "Task not serializable" in exception.get("message", ""):
            diagnosis["recommendations"].append(
                "Check for non-serializable objects in your Spark transformations"
            )
        elif "FetchFailedException" in exception_type:
            diagnosis["recommendations"].append(
                "Check for executor failures during shuffle, consider enabling external shuffle service"
            )
    
    # Check OS metrics for system-level issues
    if os_metrics and "nodes" in os_metrics:
        for node_name, node_metrics in os_metrics.get("nodes", {}).items():
            # Check CPU usage
            if "cpu" in node_metrics and "usage" in node_metrics["cpu"]:
                cpu_usage = node_metrics["cpu"]["usage"]
                if cpu_usage > 90:
                    diagnosis["status"] = "WARNING" if diagnosis["status"] != "ERROR" else "ERROR"
                    diagnosis["details"].append(f"High CPU usage on node {node_name}: {cpu_usage:.2f}%")
                    diagnosis["recommendations"].append(
                        "Consider increasing executor cores or reducing parallelism"
                    )
            
            # Check memory usage
            if "memory" in node_metrics and "percent" in node_metrics["memory"]:
                memory_usage = node_metrics["memory"]["percent"]
                if memory_usage > 90:
                    diagnosis["status"] = "WARNING" if diagnosis["status"] != "ERROR" else "ERROR"
                    diagnosis["details"].append(f"High memory usage on node {node_name}: {memory_usage:.2f}%")
                    diagnosis["recommendations"].append(
                        "Consider increasing node memory or reducing executor memory"
                    )
            
            # Check disk usage
            if "disk" in node_metrics and "partitions" in node_metrics["disk"]:
                for mount, partition in node_metrics["disk"]["partitions"].items():
                    if partition.get("percent", 0) > 90:
                        diagnosis["status"] = "WARNING" if diagnosis["status"] != "ERROR" else "ERROR"
                        diagnosis["details"].append(f"High disk usage on node {node_name}, mount {mount}: {partition.get('percent', 0):.2f}%")
                        diagnosis["recommendations"].append(
                            "Consider cleaning up disk space or increasing disk size"
                        )
    
    # Check app metrics for data quality issues
    if app_metrics and "data_quality" in app_metrics:
        data_quality = app_metrics["data_quality"]
        
        # Check for high null rate
        if "overall_null_rate" in data_quality and data_quality["overall_null_rate"] > 0.1:
            diagnosis["status"] = "WARNING" if diagnosis["status"] != "ERROR" else "ERROR"
            diagnosis["details"].append(f"High null rate in data: {data_quality['overall_null_rate']*100:.2f}%")
            diagnosis["recommendations"].append(
                "Check data source for missing values and add null handling in processing"
            )
        
        # Check for quality rule violations
        if "quality_rules" in data_quality:
            for rule_name, rule_result in data_quality["quality_rules"].items():
                if isinstance(rule_result, dict) and rule_result.get("pass") is False:
                    diagnosis["status"] = "WARNING" if diagnosis["status"] != "ERROR" else "ERROR"
                    diagnosis["details"].append(f"Data quality rule violation: {rule_name} - {rule_result.get('message', '')}")
                    diagnosis["recommendations"].append(
                        "Add data quality validation at source or implement cleanup in pipeline"
                    )
    
    # Update summary based on status
    if diagnosis["status"] == "ERROR":
        diagnosis["summary"] = "Critical issues detected that caused job failure"
    elif diagnosis["status"] == "WARNING":
        diagnosis["summary"] = "Performance issues detected that may impact job execution"
    
    # If no specific issues found, check for job in HTTP metrics
    if len(diagnosis["details"]) == 0:
        if not job_found:
            diagnosis["summary"] = f"No data available for job {job_id}"
            diagnosis["status"] = "UNKNOWN"
        else:
            diagnosis["summary"] = f"Job {job_id} completed successfully with no issues detected"
    
    return diagnosis

@app.get("/api/v1/applications/{app_id}/performance")
async def get_performance_metrics(app_id: str):
    """Get performance metrics for an application"""
    # Collect metrics from all collectors
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    # Get metrics from each collector
    sparkmeasure_metrics = metrics_cache.get("sparkmeasure", {})
    http_metrics = metrics_cache.get("http", {})
    os_metrics = metrics_cache.get("os_metrics", {})
    
    # Extract performance metrics
    performance = {
        "timestamp": time.time(),
        "application_id": app_id,
    }
    
    # Add derived metrics from SparkMeasure
    if "derived" in sparkmeasure_metrics:
        performance["sparkmeasure"] = sparkmeasure_metrics["derived"]
    
    # Add executor metrics from HTTP
    if "executors" in http_metrics:
        performance["executors"] = http_metrics["executors"].get("metrics", {})
    
    # Add stage metrics if available
    if "stage" in sparkmeasure_metrics:
        stage_metrics = sparkmeasure_metrics["stage"]
        
        # Extract relevant performance metrics
        stage_performance = {}
        for key in ["executorRunTime", "executorCpuTime", "jvmGCTime", 
                    "shuffleFetchWaitTime", "shuffleWriteTime", "resultSize",
                    "diskBytesSpilled", "memoryBytesSpilled", "peakExecutionMemory"]:
            if key in stage_metrics:
                stage_performance[key] = stage_metrics[key]
        
        performance["stage_metrics"] = stage_performance
        
    # Add OS metrics if available
    if os_metrics and "nodes" in os_metrics:
        performance["os"] = {
            "nodes": {}
        }
        
        # Extract key OS metrics for each node
        for node_name, node_metrics in os_metrics["nodes"].items():
            performance["os"]["nodes"][node_name] = {}
            
            # CPU metrics
            if "cpu" in node_metrics:
                performance["os"]["nodes"][node_name]["cpu"] = {
                    "usage": node_metrics["cpu"].get("usage"),
                    "load1": node_metrics["cpu"].get("load1"),
                    "load5": node_metrics["cpu"].get("load5"),
                    "load15": node_metrics["cpu"].get("load15")
                }
                
            # Memory metrics
            if "memory" in node_metrics:
                performance["os"]["nodes"][node_name]["memory"] = {
                    "usage_percent": node_metrics["memory"].get("percent"),
                    "used": node_metrics["memory"].get("used"),
                    "total": node_metrics["memory"].get("total")
                }
                
            # Disk metrics
            if "disk" in node_metrics:
                # Disk I/O rates
                disk_io = {}
                if "read_bytes_per_sec" in node_metrics["disk"]:
                    disk_io["read_rate"] = node_metrics["disk"]["read_bytes_per_sec"]
                if "write_bytes_per_sec" in node_metrics["disk"]:
                    disk_io["write_rate"] = node_metrics["disk"]["write_bytes_per_sec"]
                    
                performance["os"]["nodes"][node_name]["disk"] = disk_io
                
            # Network metrics
            if "network" in node_metrics and "total" in node_metrics["network"]:
                network_io = {}
                if "bytes_sent_per_sec" in node_metrics["network"]["total"]:
                    network_io["send_rate"] = node_metrics["network"]["total"]["bytes_sent_per_sec"]
                if "bytes_recv_per_sec" in node_metrics["network"]["total"]:
                    network_io["receive_rate"] = node_metrics["network"]["total"]["bytes_recv_per_sec"]
                    
                performance["os"]["nodes"][node_name]["network"] = network_io
    
    return performance

@app.get("/api/v1/dashboard")
async def get_dashboard_metrics():
    """Get metrics for dashboard"""
    # Collect metrics from all collectors
    cache_age = time.time() - metrics_cache["last_updated"]
    if cache_age > settings["cache_ttl"]:
        collect_metrics()
    
    # Prepare dashboard data
    dashboard = {
        "timestamp": time.time(),
        "applications": [],
        "jobs": {
            "total": 0,
            "running": 0,
            "completed": 0,
            "failed": 0,
            "recent": []
        },
        "stages": {
            "total": 0,
            "active": 0,
            "completed": 0,
            "failed": 0,
            "skipped": 0
        },
        "tasks": {
            "total": 0,
            "active": 0,
            "completed": 0,
            "failed": 0
        },
        "executors": {
            "total": 0,
            "active": 0,
            "dead": 0,
            "memory_usage": 0,
            "memory_total": 0
        },
        "performance": {
            "dataSkew": None,
            "gcPressure": None,
            "shuffleIntensity": None,
            "memorySpill": None
        },
        "exceptions": {
            "count": 0,
            "latest": []
        },
        "os_metrics": {
            "cpu_usage": None,
            "memory_usage": None,
            "disk_io": None,
            "network_io": None
        },
        "data_quality": {
            "null_rate": None,
            "duplicate_rate": None,
            "quality_rules": []
        }
    }
    
    # Extract application info
    http_metrics = metrics_cache.get("http", {})
    app_info = http_metrics.get("app", {})
    
    if app_info:
        # Format as a list for consistency
        if not isinstance(app_info, list):
            app_info = [app_info]
        
        dashboard["applications"] = app_info
    
    # Extract job info
    jobs_info = http_metrics.get("jobs", {})
    if jobs_info:
        dashboard["jobs"]["total"] = jobs_info.get("count", 0)
        dashboard["jobs"]["running"] = jobs_info.get("active", 0)
        dashboard["jobs"]["completed"] = jobs_info.get("completed", 0)
        dashboard["jobs"]["failed"] = jobs_info.get("failed", 0)
        dashboard["jobs"]["recent"] = jobs_info.get("details", [])
    
    # Extract stage info
    stages_info = http_metrics.get("stages", {})
    if stages_info:
        dashboard["stages"]["total"] = stages_info.get("count", 0)
        dashboard["stages"]["active"] = stages_info.get("active", 0)
        dashboard["stages"]["completed"] = stages_info.get("completed", 0)
        dashboard["stages"]["failed"] = stages_info.get("failed", 0)
        dashboard["stages"]["skipped"] = stages_info.get("skipped", 0)
    
    # Extract executor info
    executors_info = http_metrics.get("executors", {})
    if executors_info:
        dashboard["executors"]["total"] = executors_info.get("count", 0)
        dashboard["executors"]["active"] = executors_info.get("active", 0)
        dashboard["executors"]["dead"] = executors_info.get("dead", 0)
        
        if "metrics" in executors_info:
            dashboard["executors"]["memory_usage"] = executors_info["metrics"].get("usedMemory", 0)
            dashboard["executors"]["memory_total"] = executors_info["metrics"].get("totalMemory", 0)
    
    # Extract performance metrics
    sparkmeasure_metrics = metrics_cache.get("sparkmeasure", {})
    if "derived" in sparkmeasure_metrics:
        dashboard["performance"] = sparkmeasure_metrics["derived"]
    
    # Extract exception info
    logging_metrics = metrics_cache.get("logging", {})
    exceptions = logging_metrics.get("exceptions", {})
    if exceptions:
        dashboard["exceptions"]["count"] = exceptions.get("count", 0)
        dashboard["exceptions"]["latest"] = exceptions.get("latest", [])
    
    # Extract OS metrics
    os_metrics = metrics_cache.get("os_metrics", {})
    if os_metrics and "nodes" in os_metrics:
        # Calculate average across all nodes
        cpu_values = []
        memory_values = []
        disk_read_values = []
        disk_write_values = []
        network_recv_values = []
        network_send_values = []
        
        for node_name, node_metrics in os_metrics["nodes"].items():
            # CPU
            if "cpu" in node_metrics and "usage" in node_metrics["cpu"]:
                cpu_values.append(node_metrics["cpu"]["usage"])
                
            # Memory
            if "memory" in node_metrics and "percent" in node_metrics["memory"]:
                memory_values.append(node_metrics["memory"]["percent"])
                
            # Disk I/O
            if "disk" in node_metrics:
                if "read_bytes_per_sec" in node_metrics["disk"]:
                    disk_read_values.append(node_metrics["disk"]["read_bytes_per_sec"])
                if "write_bytes_per_sec" in node_metrics["disk"]:
                    disk_write_values.append(node_metrics["disk"]["write_bytes_per_sec"])
                    
            # Network I/O
            if "network" in node_metrics and "total" in node_metrics["network"]:
                if "bytes_recv_per_sec" in node_metrics["network"]["total"]:
                    network_recv_values.append(node_metrics["network"]["total"]["bytes_recv_per_sec"])
                if "bytes_sent_per_sec" in node_metrics["network"]["total"]:
                    network_send_values.append(node_metrics["network"]["total"]["bytes_sent_per_sec"])
        
        # Calculate averages
        if cpu_values:
            dashboard["os_metrics"]["cpu_usage"] = sum(cpu_values) / len(cpu_values)
        if memory_values:
            dashboard["os_metrics"]["memory_usage"] = sum(memory_values) / len(memory_values)
        if disk_read_values and disk_write_values:
            dashboard["os_metrics"]["disk_io"] = {
                "read_rate": sum(disk_read_values) / len(disk_read_values),
                "write_rate": sum(disk_write_values) / len(disk_write_values)
            }
        if network_recv_values and network_send_values:
            dashboard["os_metrics"]["network_io"] = {
                "receive_rate": sum(network_recv_values) / len(network_recv_values),
                "send_rate": sum(network_send_values) / len(network_send_values)
            }
    
    # Extract data quality metrics
    app_metrics = metrics_cache.get("app_metrics", {})
    if app_metrics and "data_quality" in app_metrics:
        data_quality = app_metrics["data_quality"]
        
        # Overall null rate
        if "overall_null_rate" in data_quality:
            dashboard["data_quality"]["null_rate"] = data_quality["overall_null_rate"]
            
        # Duplicate rate
        if "duplicate_percentage" in data_quality:
            dashboard["data_quality"]["duplicate_rate"] = data_quality["duplicate_percentage"] / 100.0
            
        # Quality rules
        if "quality_rules" in data_quality:
            quality_rules = []
            for rule_name, rule_result in data_quality["quality_rules"].items():
                if isinstance(rule_result, dict):
                    quality_rules.append({
                        "name": rule_name,
                        "passed": rule_result.get("pass", True),
                        "message": rule_result.get("message", "")
                    })
            
            dashboard["data_quality"]["quality_rules"] = quality_rules
    
    return dashboard

# Startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Initialize collectors and start background tasks on startup"""
    # Initialize collectors
    initialize_collectors()
    
    # Try to set up Spark session
    setup_spark_session()
    
    # Collect initial metrics
    collect_metrics()
    
    # Start background task for periodic collection
    asyncio.create_task(periodic_collection())
    
    logger.info("API server started")

@app.on_event("shutdown")
async def shutdown_event():
    """Clean up resources on shutdown"""
    settings["monitoring_enabled"] = False
    logger.info("API server shutting down")

# Run the API server
if __name__ == "__main__":
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description="Comprehensive Spark Pipeline Debugger API")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind to")
    parser.add_argument("--spark-ui-url", type=str, default="http://localhost:4040", help="Spark UI URL")
    parser.add_argument("--log-dir", type=str, default="/tmp/spark-logs", help="Spark log directory")
    parser.add_argument("--collection-interval", type=int, default=30, help="Metrics collection interval in seconds")
    parser.add_argument("--prometheus-url", type=str, default=None, help="Prometheus URL for OS metrics")
    parser.add_argument("--custom-metrics-path", type=str, default=None, help="Path to custom metrics definition file")
    parser.add_argument("--cloudwatch-region", type=str, default=None, help="AWS region for CloudWatch metrics")
    parser.add_argument("--ganglia-host", type=str, default=None, help="Ganglia host for metrics")
    parser.add_argument("--no-os-metrics", action="store_true", help="Disable OS metrics collection")
    parser.add_argument("--no-app-metrics", action="store_true", help="Disable application metrics collection")
    parser.add_argument("--no-external-metrics", action="store_true", help="Disable external metrics collection")
    args = parser.parse_args()
    
    # Update settings
    settings["spark_ui_url"] = args.spark_ui_url
    settings["log_dir"] = args.log_dir
    settings["collection_interval"] = args.collection_interval
    
    if args.prometheus_url:
        settings["prometheus_url"] = args.prometheus_url
    if args.custom_metrics_path:
        settings["custom_metrics_path"] = args.custom_metrics_path
    if args.cloudwatch_region:
        settings["cloudwatch_region"] = args.cloudwatch_region
    if args.ganglia_host:
        settings["ganglia_host"] = args.ganglia_host
        
    settings["os_metrics_enabled"] = not args.no_os_metrics
    settings["app_metrics_enabled"] = not args.no_app_metrics
    settings["external_metrics_enabled"] = not args.no_external_metrics
    
    # Start the server
    uvicorn.run(app, host=args.host, port=args.port)

================================================
File: /os_metrics_collector.py
================================================
#!/usr/bin/env python3
# os_metrics_collector.py - OS-level metrics collector using Prometheus/node_exporter

import time
import logging
import json
import requests
from typing import Dict, List, Any, Optional, Union
import subprocess
import socket
import platform
import psutil

from base_collector import BaseCollector

class OSMetricsCollector(BaseCollector):
    """Collects OS-level metrics for Spark nodes using Prometheus or direct system calls.
    
    This collector provides information about CPU, memory, disk I/O, and network
    utilization on the nodes where Spark executors are running.
    """
    
    def __init__(self, prometheus_url: Optional[str] = None):
        """Initialize the OS metrics collector.
        
        Args:
            prometheus_url: URL of Prometheus server. If None, will use direct system calls.
        """
        super().__init__(name="os_metrics")
        self.prometheus_url = prometheus_url
        self.use_prometheus = prometheus_url is not None
        self.node_exporter_metrics = [
            # CPU metrics
            "node_cpu_seconds_total",
            "node_load1",
            "node_load5",
            "node_load15",
            # Memory metrics
            "node_memory_MemTotal_bytes",
            "node_memory_MemFree_bytes",
            "node_memory_Cached_bytes",
            "node_memory_Buffers_bytes",
            "node_memory_SwapTotal_bytes",
            "node_memory_SwapFree_bytes",
            # Disk metrics
            "node_disk_io_time_seconds_total",
            "node_disk_read_bytes_total",
            "node_disk_written_bytes_total",
            "node_filesystem_avail_bytes",
            "node_filesystem_size_bytes",
            # Network metrics
            "node_network_receive_bytes_total",
            "node_network_transmit_bytes_total",
            "node_network_receive_packets_total",
            "node_network_transmit_packets_total",
        ]
        # Store previous measurements for rate calculations
        self.previous_metrics = {}
        self.previous_timestamp = 0
        # Get hostname for local metrics
        self.hostname = socket.gethostname()
        
    def get_supported_metrics(self) -> List[str]:
        """Return a list of metrics that this collector can provide."""
        return [
            # CPU metrics
            "os.cpu.usage", "os.cpu.load", "os.cpu.cores",
            # Memory metrics
            "os.memory.total", "os.memory.free", "os.memory.used",
            "os.memory.cached", "os.memory.buffers", 
            "os.memory.swap.total", "os.memory.swap.free", "os.memory.swap.used",
            # Disk metrics
            "os.disk.io.time", "os.disk.read.bytes", "os.disk.write.bytes",
            "os.disk.usage", "os.disk.free",
            # Network metrics
            "os.network.receive.bytes", "os.network.transmit.bytes",
            "os.network.receive.packets", "os.network.transmit.packets",
            "os.network.receive.errors", "os.network.transmit.errors",
            # Process metrics
            "os.process.count", "os.process.spark.count",
            # System metrics
            "os.uptime", "os.users"
        ]
    
    def setup(self, context: Dict[str, Any]) -> None:
        """Set up the collector with context information.
        
        Args:
            context: Dictionary containing collection context.
                     May include prometheus_url to override the default.
        """
        if "prometheus_url" in context:
            self.prometheus_url = context["prometheus_url"]
            self.use_prometheus = self.prometheus_url is not None
            
        self.logger.info(f"OS Metrics collector initialized with Prometheus: {self.use_prometheus}")
    
    def collect(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Collect OS-level metrics.
        
        Args:
            context: Dictionary containing collection context.
                     May include node_hostnames if collecting from remote nodes.
                     
        Returns:
            Dictionary of collected metrics.
        """
        metrics = {
            "timestamp": time.time(),
            "hostname": self.hostname,
            "nodes": {}
        }
        
        # Update context if needed
        if "prometheus_url" in context:
            self.prometheus_url = context["prometheus_url"]
            self.use_prometheus = self.prometheus_url is not None
            
        # Get node hostnames to collect metrics for
        nodes = context.get("node_hostnames", [self.hostname])
        
        if self.use_prometheus:
            # Collect metrics from Prometheus
            metrics["nodes"] = self._collect_from_prometheus(nodes)
        else:
            # Collect metrics from direct system calls
            for node in nodes:
                if node == self.hostname:
                    # Local node
                    metrics["nodes"][node] = self._collect_local_metrics()
                else:
                    # Remote node - would require SSH or agent
                    self.logger.warning(f"Direct metrics collection from remote node {node} not supported. Use Prometheus.")
        
        # Calculate rates for cumulative metrics
        current_timestamp = time.time()
        if self.previous_timestamp > 0:
            time_diff = current_timestamp - self.previous_timestamp
            metrics = self._calculate_rates(metrics, time_diff)
            
        # Store current metrics for next rate calculation
        self.previous_metrics = metrics.copy()
        self.previous_timestamp = current_timestamp
        
        return metrics
    
    def _collect_from_prometheus(self, nodes: List[str]) -> Dict[str, Any]:
        """Collect metrics from Prometheus.
        
        Args:
            nodes: List of node hostnames to collect metrics for.
            
        Returns:
            Dictionary of node metrics.
        """
        node_metrics = {}
        
        try:
            for node in nodes:
                node_metrics[node] = {
                    "cpu": {},
                    "memory": {},
                    "disk": {},
                    "network": {},
                    "system": {}
                }
                
                # Collect metrics for this node
                for metric in self.node_exporter_metrics:
                    query = f'{metric}{{instance=~".*{node}.*"}}'
                    response = requests.get(f"{self.prometheus_url}/api/v1/query", params={"query": query})
                    
                    if response.status_code == 200:
                        result = response.json()
                        if result["status"] == "success" and result["data"]["result"]:
                            # Process results based on metric type
                            self._process_prometheus_metric(node_metrics[node], metric, result["data"]["result"])
                    else:
                        self.logger.warning(f"Failed to query Prometheus: {response.status_code} {response.text}")
                
                # Get process information
                query = 'process_cpu_seconds_total{job="spark"}'
                response = requests.get(f"{self.prometheus_url}/api/v1/query", params={"query": query})
                if response.status_code == 200:
                    result = response.json()
                    if result["status"] == "success":
                        node_metrics[node]["system"]["spark_processes"] = len(result["data"]["result"])
                
        except Exception as e:
            self.logger.error(f"Error collecting from Prometheus: {str(e)}")
            
        return node_metrics
    
    def _process_prometheus_metric(self, node_metrics: Dict[str, Any], metric: str, results: List[Dict[str, Any]]) -> None:
        """Process a Prometheus metric and add it to the node metrics.
        
        Args:
            node_metrics: Dictionary to add metrics to.
            metric: Metric name.
            results: List of metric results from Prometheus.
        """
        # CPU metrics
        if metric == "node_cpu_seconds_total":
            # Group by mode (user, system, idle, etc.)
            cpu_times = {}
            for result in results:
                mode = result["metric"].get("mode", "unknown")
                value = float(result["value"][1])
                if mode in cpu_times:
                    cpu_times[mode] += value
                else:
                    cpu_times[mode] = value
            
            node_metrics["cpu"]["cpu_times"] = cpu_times
            
            # Calculate CPU usage if we have idle time
            if "idle" in cpu_times:
                total_time = sum(cpu_times.values())
                idle_time = cpu_times["idle"]
                node_metrics["cpu"]["usage"] = 100.0 * (1.0 - (idle_time / total_time)) if total_time > 0 else 0
                
        elif metric == "node_load1":
            node_metrics["cpu"]["load1"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_load5":
            node_metrics["cpu"]["load5"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_load15":
            node_metrics["cpu"]["load15"] = float(results[0]["value"][1]) if results else 0
            
        # Memory metrics
        elif metric == "node_memory_MemTotal_bytes":
            node_metrics["memory"]["total"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_memory_MemFree_bytes":
            node_metrics["memory"]["free"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_memory_Cached_bytes":
            node_metrics["memory"]["cached"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_memory_Buffers_bytes":
            node_metrics["memory"]["buffers"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_memory_SwapTotal_bytes":
            node_metrics["memory"]["swap_total"] = float(results[0]["value"][1]) if results else 0
        elif metric == "node_memory_SwapFree_bytes":
            node_metrics["memory"]["swap_free"] = float(results[0]["value"][1]) if results else 0
            
        # Disk metrics
        elif metric == "node_disk_io_time_seconds_total":
            total_io_time = sum(float(r["value"][1]) for r in results)
            node_metrics["disk"]["io_time_total"] = total_io_time
        elif metric == "node_disk_read_bytes_total":
            total_read_bytes = sum(float(r["value"][1]) for r in results)
            node_metrics["disk"]["read_bytes_total"] = total_read_bytes
        elif metric == "node_disk_written_bytes_total":
            total_written_bytes = sum(float(r["value"][1]) for r in results)
            node_metrics["disk"]["written_bytes_total"] = total_written_bytes
        elif metric == "node_filesystem_avail_bytes":
            # Sum available space across all filesystems
            total_avail = sum(float(r["value"][1]) for r in results if r["metric"].get("fstype") not in ["tmpfs", "devtmpfs"])
            node_metrics["disk"]["available_bytes"] = total_avail
        elif metric == "node_filesystem_size_bytes":
            # Sum total size across all filesystems
            total_size = sum(float(r["value"][1]) for r in results if r["metric"].get("fstype") not in ["tmpfs", "devtmpfs"])
            node_metrics["disk"]["total_bytes"] = total_size
            
            # Calculate usage percentage if we have both metrics
            if "available_bytes" in node_metrics["disk"] and total_size > 0:
                used_bytes = total_size - node_metrics["disk"]["available_bytes"]
                node_metrics["disk"]["usage_percent"] = 100.0 * used_bytes / total_size
            
        # Network metrics
        elif metric == "node_network_receive_bytes_total":
            # Sum across all interfaces except lo
            total_rx_bytes = sum(float(r["value"][1]) for r in results if r["metric"].get("device") != "lo")
            node_metrics["network"]["receive_bytes_total"] = total_rx_bytes
        elif metric == "node_network_transmit_bytes_total":
            # Sum across all interfaces except lo
            total_tx_bytes = sum(float(r["value"][1]) for r in results if r["metric"].get("device") != "lo")
            node_metrics["network"]["transmit_bytes_total"] = total_tx_bytes
        elif metric == "node_network_receive_packets_total":
            total_rx_packets = sum(float(r["value"][1]) for r in results if r["metric"].get("device") != "lo")
            node_metrics["network"]["receive_packets_total"] = total_rx_packets
        elif metric == "node_network_transmit_packets_total":
            total_tx_packets = sum(float(r["value"][1]) for r in results if r["metric"].get("device") != "lo")
            node_metrics["network"]["transmit_packets_total"] = total_tx_packets
    
    def _collect_local_metrics(self) -> Dict[str, Any]:
        """Collect metrics from the local system using direct system calls.
        
        Returns:
            Dictionary of OS metrics.
        """
        metrics = {
            "cpu": {},
            "memory": {},
            "disk": {},
            "network": {},
            "system": {}
        }
        
        try:
            # CPU metrics
            cpu_times = psutil.cpu_times()
            cpu_percent = psutil.cpu_percent(interval=None, percpu=False)
            load_avg = psutil.getloadavg()
            
            metrics["cpu"] = {
                "usage": cpu_percent,
                "load1": load_avg[0],
                "load5": load_avg[1],
                "load15": load_avg[2],
                "cores": psutil.cpu_count(logical=True),
                "physical_cores": psutil.cpu_count(logical=False),
                "times": {
                    "user": cpu_times.user,
                    "system": cpu_times.system,
                    "idle": cpu_times.idle,
                    "iowait": getattr(cpu_times, "iowait", 0)
                }
            }
            
            # Memory metrics
            mem = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            metrics["memory"] = {
                "total": mem.total,
                "available": mem.available,
                "used": mem.used,
                "free": mem.free,
                "cached": getattr(mem, "cached", 0),
                "buffers": getattr(mem, "buffers", 0),
                "percent": mem.percent,
                "swap_total": swap.total,
                "swap_used": swap.used,
                "swap_free": swap.free,
                "swap_percent": swap.percent
            }
            
            # Disk metrics
            io_counters = psutil.disk_io_counters()
            
            metrics["disk"] = {
                "read_count": io_counters.read_count,
                "write_count": io_counters.write_count,
                "read_bytes": io_counters.read_bytes,
                "write_bytes": io_counters.write_bytes,
                "read_time": io_counters.read_time,
                "write_time": io_counters.write_time,
                "busy_time": getattr(io_counters, "busy_time", 0),
                "partitions": {}
            }
            
            # Disk usage for each partition
            for partition in psutil.disk_partitions():
                try:
                    usage = psutil.disk_usage(partition.mountpoint)
                    metrics["disk"]["partitions"][partition.mountpoint] = {
                        "total": usage.total,
                        "used": usage.used,
                        "free": usage.free,
                        "percent": usage.percent,
                        "fstype": partition.fstype
                    }
                except (PermissionError, OSError):
                    # Skip partitions that can't be accessed
                    pass
            
            # Network metrics
            net_io = psutil.net_io_counters(pernic=True)
            
            metrics["network"] = {
                "interfaces": {},
                "total": {
                    "bytes_sent": 0,
                    "bytes_recv": 0,
                    "packets_sent": 0,
                    "packets_recv": 0,
                    "errin": 0,
                    "errout": 0,
                    "dropin": 0,
                    "dropout": 0
                }
            }
            
            # Per-interface metrics
            for interface, counters in net_io.items():
                if interface != "lo":  # Skip loopback
                    metrics["network"]["interfaces"][interface] = {
                        "bytes_sent": counters.bytes_sent,
                        "bytes_recv": counters.bytes_recv,
                        "packets_sent": counters.packets_sent,
                        "packets_recv": counters.packets_recv,
                        "errin": counters.errin,
                        "errout": counters.errout,
                        "dropin": counters.dropin,
                        "dropout": counters.dropout
                    }
                    
                    # Update totals
                    for key in metrics["network"]["total"]:
                        metrics["network"]["total"][key] += getattr(counters, key)
            
            # System metrics
            boot_time = psutil.boot_time()
            
            metrics["system"] = {
                "uptime": time.time() - boot_time,
                "boot_time": boot_time,
                "users": len(psutil.users()),
                "process_count": len(psutil.pids())
            }
            
            # Count Spark processes
            spark_processes = [p for p in psutil.process_iter(['name', 'cmdline']) 
                              if any('spark' in cmd.lower() for cmd in p.info['cmdline'] 
                                     if isinstance(cmd, str)) 
                              if p.info['cmdline']]
            
            metrics["system"]["spark_processes"] = len(spark_processes)
            
        except Exception as e:
            self.logger.error(f"Error collecting local metrics: {str(e)}")
            
        return metrics
    
    def _calculate_rates(self, metrics: Dict[str, Any], time_diff: float) -> Dict[str, Any]:
        """Calculate rates for cumulative metrics.
        
        Args:
            metrics: Current metrics.
            time_diff: Time difference in seconds since last collection.
            
        Returns:
            Metrics with rates added.
        """
        if not self.previous_metrics or "nodes" not in self.previous_metrics:
            return metrics
            
        for node, node_metrics in metrics.get("nodes", {}).items():
            if node not in self.previous_metrics.get("nodes", {}):
                continue
                
            prev_node_metrics = self.previous_metrics["nodes"][node]
            
            # Disk I/O rates
            if "disk" in node_metrics and "disk" in prev_node_metrics:
                current_disk = node_metrics["disk"]
                prev_disk = prev_node_metrics["disk"]
                
                # Add rate metrics
                if "read_bytes" in current_disk and "read_bytes" in prev_disk:
                    read_bytes_diff = current_disk["read_bytes"] - prev_disk["read_bytes"]
                    current_disk["read_bytes_per_sec"] = read_bytes_diff / time_diff
                    
                if "write_bytes" in current_disk and "write_bytes" in prev_disk:
                    write_bytes_diff = current_disk["write_bytes"] - prev_disk["write_bytes"]
                    current_disk["write_bytes_per_sec"] = write_bytes_diff / time_diff
                    
                # Prometheus specific metrics
                if "read_bytes_total" in current_disk and "read_bytes_total" in prev_disk:
                    read_bytes_diff = current_disk["read_bytes_total"] - prev_disk["read_bytes_total"]
                    current_disk["read_bytes_per_sec"] = read_bytes_diff / time_diff
                    
                if "written_bytes_total" in current_disk and "written_bytes_total" in prev_disk:
                    write_bytes_diff = current_disk["written_bytes_total"] - prev_disk["written_bytes_total"]
                    current_disk["write_bytes_per_sec"] = write_bytes_diff / time_diff
            
            # Network I/O rates
            if "network" in node_metrics and "network" in prev_node_metrics:
                current_net = node_metrics["network"]
                prev_net = prev_node_metrics["network"]
                
                # Direct metrics
                if "total" in current_net and "total" in prev_net:
                    current_total = current_net["total"]
                    prev_total = prev_net["total"]
                    
                    if "bytes_sent" in current_total and "bytes_sent" in prev_total:
                        bytes_sent_diff = current_total["bytes_sent"] - prev_total["bytes_sent"]
                        current_total["bytes_sent_per_sec"] = bytes_sent_diff / time_diff
                        
                    if "bytes_recv" in current_total and "bytes_recv" in prev_total:
                        bytes_recv_diff = current_total["bytes_recv"] - prev_total["bytes_recv"]
                        current_total["bytes_recv_per_sec"] = bytes_recv_diff / time_diff
                
                # Prometheus metrics
                if "transmit_bytes_total" in current_net and "transmit_bytes_total" in prev_net:
                    tx_bytes_diff = current_net["transmit_bytes_total"] - prev_net["transmit_bytes_total"]
                    current_net["transmit_bytes_per_sec"] = tx_bytes_diff / time_diff
                    
                if "receive_bytes_total" in current_net and "receive_bytes_total" in prev_net:
                    rx_bytes_diff = current_net["receive_bytes_total"] - prev_net["receive_bytes_total"]
                    current_net["receive_bytes_per_sec"] = rx_bytes_diff / time_diff
        
        return metrics

