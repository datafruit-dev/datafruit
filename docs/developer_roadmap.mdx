---
title: "Feature Roadmap"
description: "Covers where we are taking the development of datafruit and features you should be looking out for!"
---

## Jobs

Jobs will be mainly used to perform the **T** in ETL/ELT. Define a job using the `@sql_job` decorator.

```python
# dft.py - Initial version
import datafruit as dft
import os
from sqlmodel import Field, SQLModel
from typing import Optional
from datetime import datetime

class User(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    username: str = Field(unique=True)
    email: str = Field(unique=True)
    full_name: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)

class Post(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    user_id: int
    title: str
    content: str
    created_at: datetime = Field(default_factory=datetime.utcnow)

db = dft.PostgresDB(
    os.getenv("PG_DB_URL") or "",
    [User, Post]
)

@dft.sql_job(db)
def user_summary():
    """Create a summary of user activity"""
    return """
        CREATE TABLE user_summary AS
        SELECT 
            u.id,
            u.username,
            u.email,
            u.full_name,
            COUNT(p.id) as post_count,
            MAX(p.created_at) as last_post_date
        FROM user u
        LEFT JOIN post p ON u.id = p.user_id
        GROUP BY u.id, u.username, u.email, u.full_name
    """

@dft.sql_job(db)
def email_report():
    """Generate email validation report"""
    return """
        CREATE TABLE email_validation AS
        SELECT 
            username,
            email,
            CASE 
                WHEN email LIKE '%@%.%' THEN 'valid'
                ELSE 'invalid'
            END as email_status
        FROM user
        WHERE email IS NOT NULL
    """

dft.export([db])
```

### Running your job 

```bash
# First, apply your schema
dft plan
dft apply

# Then run your job
dft run user_summary
```

at which point `datafruit` will execute your job. What happens if you change the schema which causes the requirements of the job to change?

### Automatic impact analysis

Lets say you modify the database schema

```python
# dft.py - Updated version
class User(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    username: str = Field(unique=True)
    # email: str = Field(unique=True)        # ❌ REMOVED
    # full_name: Optional[str] = None        # ❌ REMOVED
    created_at: datetime = Field(default_factory=datetime.utcnow)

# Jobs stay the same - they still reference removed columns
```

running `dft plan` or `dft run` would show the impact and prevent you from keeping broken jobs.

```bash
dft plan
```

```python
Datafruit will perform the following actions:

~ Table: user
│ - Remove column email (VARCHAR)
│ - Remove column full_name (VARCHAR)

⚠️  The following jobs will be affected:

│ ❌ user_summary
│   └─ References user.email (being removed)
│   └─ References user.full_name (being removed)
│
│ ❌ email_report  
│   └─ References user.email (being removed)

Plan saved to .dft/plan.json
❌ Cannot apply - fix job dependencies first
```

### Ignore jobs

When requirements change frequently, it can be annoying to have to constantly update your jobs. Especially ones being unused. To ignore a job, all you have to do is wrap it with the `@ignore` decorator

```python
@dft.ignore
@dft.sql_job(db)
def backfill_user_scores():
    """One-time backfill - already ran in production"""
    return """
        UPDATE users 
        SET score = calculate_historical_score(id)
        WHERE score IS NULL
    """
```

### Incremental Execution

```python
@dft.sql_job(db, incremental=True, incremental_key="created_at")
def daily_user_metrics():
    """Only process records newer than last run"""
    return """
        INSERT INTO user_daily_metrics
        SELECT 
            DATE(u.created_at) as date,
            COUNT(*) as new_users
        FROM users u
        WHERE u.created_at > {last_run_timestamp}
        GROUP BY DATE(u.created_at)
    """

# When executed, datafruit automatically:
# 1. Looks up last successful run timestamp
# 2. Substitutes {last_run_timestamp} in the SQL
# 3. Only processes new data since last run
```

### Scheduling and Triggers

```python
@dft.sql_job(db, schedule="daily", retry=3)
def daily_report():
    return "SELECT * FROM daily_aggregates WHERE date = CURRENT_DATE"

@dft.sql_job(db, trigger="after_schema_change")  # Runs after dft apply
def refresh_materialized_views():
    return "REFRESH MATERIALIZED VIEW user_summary"

@dft.sql_job(db, trigger="on_dependency_change")
def user_derived_metrics():
    return "SELECT user_id, calculated_score FROM users"
```

### Parameterization

```python
@dft.sql_job(db, parameters={"lookback_days": 30, "min_score": 0.5})
def parameterized_job():
    return """
        SELECT user_id, score
        FROM user_activity 
        WHERE created_at > CURRENT_DATE - INTERVAL '{lookback_days} days'
        AND score > {min_score}
    """

# Override at runtime: dft run parameterized_job --lookback_days=7
```